13117 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 11 ms
13117 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  11 ms
13117 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13117 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13106 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

13118 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 184 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@53ffe82c
13118 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 3 (262144 bytes)
13119 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13119 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 184 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@18fa8337
13119 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 4 (16777216 bytes)
13119 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 184 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2fe13e2a
13119 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 5 (16777216 bytes)
13119 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 184 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@53ffe82c
13119 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 6 (16777216 bytes)
13120 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 4 (16777216 bytes)
13120 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 184 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@18fa8337
13120 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (65536 bytes)
13120 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 184 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@18fa8337
13119 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[1, int, true] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       boolean isNull = i.isNullAt(1);
/* 025 */       int value = isNull ? -1 : (i.getInt(1));
/* 026 */       isNullA = isNull;
/* 027 */       primitiveA = value;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB;
/* 031 */     int primitiveB;
/* 032 */     {
/* 033 */
/* 034 */       boolean isNull = i.isNullAt(1);
/* 035 */       int value = isNull ? -1 : (i.getInt(1));
/* 036 */       isNullB = isNull;
/* 037 */       primitiveB = value;
/* 038 */     }
/* 039 */     if (isNullA && isNullB) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */     return 0;
/* 053 */   }
/* 054 */ }

13120 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13121 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13121 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[1, int, true] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull1 = i.isNullAt(1);
/* 030 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 031 */     long value = -9223372036854775808L;
/* 032 */     boolean isNull = false;
/* 033 */     if (!isNull1) {
/* 034 */       value = (long) value1;
/* 035 */     }
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */     return result;
/* 042 */   }
/* 043 */ }

13121 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 185 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@6579066c
13122 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (65536 bytes)
13120 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 184 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@d426f6b
13122 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (16777216 bytes)
13120 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 2 (262144 bytes)
13122 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 3 (262144 bytes)
13122 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 1, partitions 183-184
13121 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Task 182 releasing lock for broadcast_2
13122 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
13123 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Getting 1 non-empty blocks out of 1 blocks
13122 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 184 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@53ffe82c
13123 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 6 (16777216 bytes)
13122 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 183 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@6e5252fc
13123 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (16777216 bytes)
13123 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 183 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@6e5252fc
13123 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 184 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@53ffe82c
13123 [Executor task launch worker-4] INFO  org.apache.spark.internal.Logging$class  - Finished task 180.0 in stage 2.0 (TID 182). 4221 bytes result sent to driver
13123 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 1 ms
13124 [dispatcher-event-loop-5] DEBUG org.apache.spark.internal.Logging$class  - parentName: , name: TaskSet_2, runningTasks: 3
13124 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  2 ms
13124 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13124 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13124 [dispatcher-event-loop-5] INFO  org.apache.spark.internal.Logging$class  - Starting task 184.0 in stage 2.0 (TID 186, localhost, partition 184, ANY, 5536 bytes)
13125 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13125 [task-result-getter-2] INFO  org.apache.spark.internal.Logging$class  - Finished task 180.0 in stage 2.0 (TID 182) in 86 ms on localhost (181/200)
13125 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 2)
13125 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13125 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13125 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13125 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 2)
13125 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 2)
13125 [Executor task launch worker-4] INFO  org.apache.spark.internal.Logging$class  - Running task 184.0 in stage 2.0 (TID 186)
13125 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[0, int, false] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       int value = i.getInt(0);
/* 025 */       isNullA = false;
/* 026 */       primitiveA = value;
/* 027 */     }
/* 028 */     i = b;
/* 029 */     boolean isNullB;
/* 030 */     int primitiveB;
/* 031 */     {
/* 032 */
/* 033 */       int value = i.getInt(0);
/* 034 */       isNullB = false;
/* 035 */       primitiveB = value;
/* 036 */     }
/* 037 */     if (isNullA && isNullB) {
/* 038 */       // Nothing
/* 039 */     } else if (isNullA) {
/* 040 */       return -1;
/* 041 */     } else if (isNullB) {
/* 042 */       return 1;
/* 043 */     } else {
/* 044 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 045 */       if (comp != 0) {
/* 046 */         return comp;
/* 047 */       }
/* 048 */     }
/* 049 */
/* 050 */     return 0;
/* 051 */   }
/* 052 */ }

13125 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - ShuffleMapTask finished on driver
13126 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13126 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 2)
13126 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13126 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13126 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13126 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13126 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13127 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 2)
13127 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 2)
13126 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Task 186's epoch is 2
13127 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[0, int, false] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     int value1 = i.getInt(0);
/* 030 */     long value = -9223372036854775808L;
/* 031 */     boolean isNull = false;
/* 032 */     if (!false) {
/* 033 */       value = (long) value1;
/* 034 */     }
/* 035 */     if (isNull) {
/* 036 */       rowWriter.setNullAt(0);
/* 037 */     } else {
/* 038 */       rowWriter.write(0, value);
/* 039 */     }
/* 040 */     return result;
/* 041 */   }
/* 042 */ }

13127 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Getting local block broadcast_2
13127 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Task 186 trying to acquire read lock for broadcast_2
13127 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Task 186 acquired read lock for broadcast_2
13127 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Level for block broadcast_2 is StorageLevel(disk, memory, deserialized, 1 replicas)
13127 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 185 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@799c94a9
13128 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 1 (65536 bytes)
13128 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13128 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13129 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for createexternalrow(input[0, int, true], input[1, int, true], input[2, string, true].toString, StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private MutableRow mutableRow;
/* 009 */   private Object[] values;
/* 010 */   private org.apache.spark.sql.types.StructType schema;
/* 011 */
/* 012 */
/* 013 */   public SpecificSafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     mutableRow = (MutableRow) references[references.length - 1];
/* 016 */
/* 017 */     this.schema = (org.apache.spark.sql.types.StructType) references[0];
/* 018 */   }
/* 019 */
/* 020 */   public java.lang.Object apply(java.lang.Object _i) {
/* 021 */     InternalRow i = (InternalRow) _i;
/* 022 */
/* 023 */     values = new Object[3];
/* 024 */
/* 025 */     boolean isNull1 = i.isNullAt(0);
/* 026 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 027 */     if (isNull1) {
/* 028 */       values[0] = null;
/* 029 */     } else {
/* 030 */       values[0] = value1;
/* 031 */     }
/* 032 */
/* 033 */     boolean isNull2 = i.isNullAt(1);
/* 034 */     int value2 = isNull2 ? -1 : (i.getInt(1));
/* 035 */     if (isNull2) {
/* 036 */       values[1] = null;
/* 037 */     } else {
/* 038 */       values[1] = value2;
/* 039 */     }
/* 040 */
/* 041 */     boolean isNull4 = i.isNullAt(2);
/* 042 */     UTF8String value4 = isNull4 ? null : (i.getUTF8String(2));
/* 043 */
/* 044 */     boolean isNull3 = isNull4;
/* 045 */     final java.lang.String value3 = isNull3 ? null : (java.lang.String) value4.toString();
/* 046 */     isNull3 = value3 == null;
/* 047 */     if (isNull3) {
/* 048 */       values[2] = null;
/* 049 */     } else {
/* 050 */       values[2] = value3;
/* 051 */     }
/* 052 */
/* 053 */     final org.apache.spark.sql.Row value = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values, schema);
/* 054 */     if (false) {
/* 055 */       mutableRow.setNullAt(0);
/* 056 */     } else {
/* 057 */
/* 058 */       mutableRow.update(0, value);
/* 059 */     }
/* 060 */
/* 061 */     return mutableRow;
/* 062 */   }
/* 063 */ }

13129 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 0, partitions 184-185
13129 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
13129 [Executor task launch worker-4] INFO  org.apache.spark.internal.Logging$class  - Getting 1 non-empty blocks out of 1 blocks
13129 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - SpecificUnsafeRowJoiner(StructType(StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)), StructType(StructField(value,IntegerType,true))):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeRowJoiner();
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeRowJoiner extends org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowJoiner {
/* 006 */   private byte[] buf = new byte[64];
/* 007 */   private UnsafeRow out = new UnsafeRow(4);
/* 008 */
/* 009 */   public UnsafeRow join(UnsafeRow row1, UnsafeRow row2) {
/* 010 */     // row1: 3 fields, 1 words in bitset
/* 011 */     // row2: 1, 1 words in bitset
/* 012 */     // output: 4 fields, 1 words in bitset
/* 013 */     final int sizeInBytes = row1.getSizeInBytes() + row2.getSizeInBytes() - 8;
/* 014 */     if (sizeInBytes > buf.length) {
/* 015 */       buf = new byte[sizeInBytes];
/* 016 */     }
/* 017 */
/* 018 */     final java.lang.Object obj1 = row1.getBaseObject();
/* 019 */     final long offset1 = row1.getBaseOffset();
/* 020 */     final java.lang.Object obj2 = row2.getBaseObject();
/* 021 */     final long offset2 = row2.getBaseOffset();
/* 022 */
/* 023 */     Platform.putLong(buf, 16, Platform.getLong(obj1, offset1 + 0) | (Platform.getLong(obj2, offset2) << 3));
/* 024 */
/* 025 */     // Copy fixed length data for row1
/* 026 */     Platform.copyMemory(
/* 027 */       obj1, offset1 + 8,
/* 028 */       buf, 24,
/* 029 */       24);
/* 030 */
/* 031 */
/* 032 */     // Copy fixed length data for row2
/* 033 */     Platform.copyMemory(
/* 034 */       obj2, offset2 + 8,
/* 035 */       buf, 48,
/* 036 */       8);
/* 037 */
/* 038 */
/* 039 */     // Copy variable length data for row1
/* 040 */     long numBytesVariableRow1 = row1.getSizeInBytes() - 32;
/* 041 */     Platform.copyMemory(
/* 042 */       obj1, offset1 + 32,
/* 043 */       buf, 56,
/* 044 */       numBytesVariableRow1);
/* 045 */
/* 046 */
/* 047 */     // Copy variable length data for row2
/* 048 */     long numBytesVariableRow2 = row2.getSizeInBytes() - 16;
/* 049 */     Platform.copyMemory(
/* 050 */       obj2, offset2 + 16,
/* 051 */       buf, 56 + numBytesVariableRow1,
/* 052 */       numBytesVariableRow2);
/* 053 */
/* 054 */
/* 055 */
/* 056 */
/* 057 */     Platform.putLong(buf, 40, Platform.getLong(buf, 40) + (8L << 32));
/* 058 */
/* 059 */
/* 060 */
/* 061 */     out.pointTo(buf, sizeInBytes);
/* 062 */
/* 063 */     return out;
/* 064 */   }
/* 065 */ }

13130 [Executor task launch worker-4] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 1 ms
13130 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.expressions.codegen.package$ExpressionCanonicalizer$CleanExpressions ===
!input[0, int, true] AS value#47   input[0, int, true]
!+- input[0, int, true]            
                
13130 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 2 iterations.
13130 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - 
=== Result of Batch CleanExpressions ===
!input[0, int, true] AS value#47   input[0, int, true]
!+- input[0, int, true]            
        
13131 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull = i.isNullAt(0);
/* 030 */     int value = isNull ? -1 : (i.getInt(0));
/* 031 */     if (isNull) {
/* 032 */       rowWriter.setNullAt(0);
/* 033 */     } else {
/* 034 */       rowWriter.write(0, value);
/* 035 */     }
/* 036 */     return result;
/* 037 */   }
/* 038 */ }

13130 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  1 ms
13131 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13131 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13132 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[1, int, true] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       boolean isNull = i.isNullAt(1);
/* 025 */       int value = isNull ? -1 : (i.getInt(1));
/* 026 */       isNullA = isNull;
/* 027 */       primitiveA = value;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB;
/* 031 */     int primitiveB;
/* 032 */     {
/* 033 */
/* 034 */       boolean isNull = i.isNullAt(1);
/* 035 */       int value = isNull ? -1 : (i.getInt(1));
/* 036 */       isNullB = isNull;
/* 037 */       primitiveB = value;
/* 038 */     }
/* 039 */     if (isNullA && isNullB) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */     return 0;
/* 053 */   }
/* 054 */ }

13132 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13132 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13131 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13133 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13133 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[1, int, true] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull1 = i.isNullAt(1);
/* 030 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 031 */     long value = -9223372036854775808L;
/* 032 */     boolean isNull = false;
/* 033 */     if (!isNull1) {
/* 034 */       value = (long) value1;
/* 035 */     }
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */     return result;
/* 042 */   }
/* 043 */ }

13133 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 186 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@7928d25c
13133 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (65536 bytes)
13133 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for pmod(hash(input[3, int, true], 42), 200):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     boolean isNull = false;
/* 027 */
/* 028 */     int value1 = 42;
/* 029 */
/* 030 */     boolean isNull2 = i.isNullAt(3);
/* 031 */     int value2 = isNull2 ? -1 : (i.getInt(3));
/* 032 */     if (!isNull2) {
/* 033 */       value1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value2, value1);
/* 034 */     }
/* 035 */
/* 036 */     int value = -1;
/* 037 */
/* 038 */     int remainder = value1 % 200;
/* 039 */     if (remainder < 0) {
/* 040 */       value = (remainder + 200) % 200;
/* 041 */     } else {
/* 042 */       value = remainder;
/* 043 */     }
/* 044 */     rowWriter.write(0, value);
/* 045 */     return result;
/* 046 */   }
/* 047 */ }

13134 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13134 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13134 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13134 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13135 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13135 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13135 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13134 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 1, partitions 184-185
13135 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
13135 [Executor task launch worker-4] INFO  org.apache.spark.internal.Logging$class  - Getting 1 non-empty blocks out of 1 blocks
13136 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

13136 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 185 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@489bc73d
13136 [Executor task launch worker-4] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 1 ms
13136 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 2 (262144 bytes)
13136 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  1 ms
13152 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13153 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13153 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 2 (262144 bytes)
13154 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 184 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@d426f6b
13152 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13154 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (16777216 bytes)
13154 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 184 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@d426f6b
13154 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13154 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13155 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13155 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13155 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13155 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13155 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13156 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

13156 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 185 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@30cfd07b
13156 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 3 (262144 bytes)
13157 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13157 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 185 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@6579066c
13157 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 4 (16777216 bytes)
13158 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 185 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@799c94a9
13158 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 5 (16777216 bytes)
13158 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 185 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@30cfd07b
13158 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 6 (16777216 bytes)
13158 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 4 (16777216 bytes)
13159 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 185 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@6579066c
13159 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (65536 bytes)
13159 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 185 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@6579066c
13159 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 185 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@489bc73d
13159 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 5 (16777216 bytes)
13159 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 183 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@50a98ae8
13159 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 1 (65536 bytes)
13159 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 183 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@50a98ae8
13159 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Task 183 releasing lock for broadcast_2
13159 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (16777216 bytes)
13160 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 3 (262144 bytes)
13160 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 185 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@30cfd07b
13160 [Executor task launch worker-3] INFO  org.apache.spark.internal.Logging$class  - Finished task 181.0 in stage 2.0 (TID 183). 4294 bytes result sent to driver
13160 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 6 (16777216 bytes)
13160 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 185 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@30cfd07b
13160 [dispatcher-event-loop-2] DEBUG org.apache.spark.internal.Logging$class  - parentName: , name: TaskSet_2, runningTasks: 3
13161 [dispatcher-event-loop-2] INFO  org.apache.spark.internal.Logging$class  - Starting task 185.0 in stage 2.0 (TID 187, localhost, partition 185, ANY, 5536 bytes)
13161 [Executor task launch worker-3] INFO  org.apache.spark.internal.Logging$class  - Running task 185.0 in stage 2.0 (TID 187)
13161 [task-result-getter-3] INFO  org.apache.spark.internal.Logging$class  - Finished task 181.0 in stage 2.0 (TID 183) in 95 ms on localhost (182/200)
13161 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13161 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 2)
13161 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13161 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13162 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13162 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Task 187's epoch is 2
13162 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Getting local block broadcast_2
13162 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Task 187 trying to acquire read lock for broadcast_2
13162 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Task 187 acquired read lock for broadcast_2
13162 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 2)
13162 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Level for block broadcast_2 is StorageLevel(disk, memory, deserialized, 1 replicas)
13162 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 2)
13163 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - ShuffleMapTask finished on driver
13163 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13163 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 2)
13164 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13164 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13164 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13164 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 2)
13164 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 2)
13164 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 0, partitions 185-186
13164 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
13164 [Executor task launch worker-3] INFO  org.apache.spark.internal.Logging$class  - Getting 1 non-empty blocks out of 1 blocks
13164 [Executor task launch worker-3] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 0 ms
13165 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  1 ms
13165 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13165 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13166 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[1, int, true] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       boolean isNull = i.isNullAt(1);
/* 025 */       int value = isNull ? -1 : (i.getInt(1));
/* 026 */       isNullA = isNull;
/* 027 */       primitiveA = value;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB;
/* 031 */     int primitiveB;
/* 032 */     {
/* 033 */
/* 034 */       boolean isNull = i.isNullAt(1);
/* 035 */       int value = isNull ? -1 : (i.getInt(1));
/* 036 */       isNullB = isNull;
/* 037 */       primitiveB = value;
/* 038 */     }
/* 039 */     if (isNullA && isNullB) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */     return 0;
/* 053 */   }
/* 054 */ }

13166 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13166 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13166 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[1, int, true] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull1 = i.isNullAt(1);
/* 030 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 031 */     long value = -9223372036854775808L;
/* 032 */     boolean isNull = false;
/* 033 */     if (!isNull1) {
/* 034 */       value = (long) value1;
/* 035 */     }
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */     return result;
/* 042 */   }
/* 043 */ }

13167 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 187 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@48c304d4
13167 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (65536 bytes)
13167 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 1, partitions 185-186
13167 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
13167 [Executor task launch worker-3] INFO  org.apache.spark.internal.Logging$class  - Getting 1 non-empty blocks out of 1 blocks
13167 [Executor task launch worker-3] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 0 ms
13168 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  1 ms
13168 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13168 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13168 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[0, int, false] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       int value = i.getInt(0);
/* 025 */       isNullA = false;
/* 026 */       primitiveA = value;
/* 027 */     }
/* 028 */     i = b;
/* 029 */     boolean isNullB;
/* 030 */     int primitiveB;
/* 031 */     {
/* 032 */
/* 033 */       int value = i.getInt(0);
/* 034 */       isNullB = false;
/* 035 */       primitiveB = value;
/* 036 */     }
/* 037 */     if (isNullA && isNullB) {
/* 038 */       // Nothing
/* 039 */     } else if (isNullA) {
/* 040 */       return -1;
/* 041 */     } else if (isNullB) {
/* 042 */       return 1;
/* 043 */     } else {
/* 044 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 045 */       if (comp != 0) {
/* 046 */         return comp;
/* 047 */       }
/* 048 */     }
/* 049 */
/* 050 */     return 0;
/* 051 */   }
/* 052 */ }

13169 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13169 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13169 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[0, int, false] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       int value = i.getInt(0);
/* 025 */       isNullA = false;
/* 026 */       primitiveA = value;
/* 027 */     }
/* 028 */     i = b;
/* 029 */     boolean isNullB;
/* 030 */     int primitiveB;
/* 031 */     {
/* 032 */
/* 033 */       int value = i.getInt(0);
/* 034 */       isNullB = false;
/* 035 */       primitiveB = value;
/* 036 */     }
/* 037 */     if (isNullA && isNullB) {
/* 038 */       // Nothing
/* 039 */     } else if (isNullA) {
/* 040 */       return -1;
/* 041 */     } else if (isNullB) {
/* 042 */       return 1;
/* 043 */     } else {
/* 044 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 045 */       if (comp != 0) {
/* 046 */         return comp;
/* 047 */       }
/* 048 */     }
/* 049 */
/* 050 */     return 0;
/* 051 */   }
/* 052 */ }

13169 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13169 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13170 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[0, int, false] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     int value1 = i.getInt(0);
/* 030 */     long value = -9223372036854775808L;
/* 031 */     boolean isNull = false;
/* 032 */     if (!false) {
/* 033 */       value = (long) value1;
/* 034 */     }
/* 035 */     if (isNull) {
/* 036 */       rowWriter.setNullAt(0);
/* 037 */     } else {
/* 038 */       rowWriter.write(0, value);
/* 039 */     }
/* 040 */     return result;
/* 041 */   }
/* 042 */ }

13169 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[0, int, false] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     int value1 = i.getInt(0);
/* 030 */     long value = -9223372036854775808L;
/* 031 */     boolean isNull = false;
/* 032 */     if (!false) {
/* 033 */       value = (long) value1;
/* 034 */     }
/* 035 */     if (isNull) {
/* 036 */       rowWriter.setNullAt(0);
/* 037 */     } else {
/* 038 */       rowWriter.write(0, value);
/* 039 */     }
/* 040 */     return result;
/* 041 */   }
/* 042 */ }

13170 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 2 (262144 bytes)
13171 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 185 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@489bc73d
13171 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (16777216 bytes)
13171 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 185 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@489bc73d
13171 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 5 (16777216 bytes)
13171 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 184 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2fe13e2a
13171 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 1 (65536 bytes)
13171 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 184 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2fe13e2a
13171 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 187 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@58980376
13172 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 1 (65536 bytes)
13171 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Task 184 releasing lock for broadcast_2
13172 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13172 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13172 [Executor task launch worker-0] INFO  org.apache.spark.internal.Logging$class  - Finished task 182.0 in stage 2.0 (TID 184). 4294 bytes result sent to driver
13172 [dispatcher-event-loop-3] DEBUG org.apache.spark.internal.Logging$class  - parentName: , name: TaskSet_2, runningTasks: 3
13173 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for createexternalrow(input[0, int, true], input[1, int, true], input[2, string, true].toString, StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private MutableRow mutableRow;
/* 009 */   private Object[] values;
/* 010 */   private org.apache.spark.sql.types.StructType schema;
/* 011 */
/* 012 */
/* 013 */   public SpecificSafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     mutableRow = (MutableRow) references[references.length - 1];
/* 016 */
/* 017 */     this.schema = (org.apache.spark.sql.types.StructType) references[0];
/* 018 */   }
/* 019 */
/* 020 */   public java.lang.Object apply(java.lang.Object _i) {
/* 021 */     InternalRow i = (InternalRow) _i;
/* 022 */
/* 023 */     values = new Object[3];
/* 024 */
/* 025 */     boolean isNull1 = i.isNullAt(0);
/* 026 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 027 */     if (isNull1) {
/* 028 */       values[0] = null;
/* 029 */     } else {
/* 030 */       values[0] = value1;
/* 031 */     }
/* 032 */
/* 033 */     boolean isNull2 = i.isNullAt(1);
/* 034 */     int value2 = isNull2 ? -1 : (i.getInt(1));
/* 035 */     if (isNull2) {
/* 036 */       values[1] = null;
/* 037 */     } else {
/* 038 */       values[1] = value2;
/* 039 */     }
/* 040 */
/* 041 */     boolean isNull4 = i.isNullAt(2);
/* 042 */     UTF8String value4 = isNull4 ? null : (i.getUTF8String(2));
/* 043 */
/* 044 */     boolean isNull3 = isNull4;
/* 045 */     final java.lang.String value3 = isNull3 ? null : (java.lang.String) value4.toString();
/* 046 */     isNull3 = value3 == null;
/* 047 */     if (isNull3) {
/* 048 */       values[2] = null;
/* 049 */     } else {
/* 050 */       values[2] = value3;
/* 051 */     }
/* 052 */
/* 053 */     final org.apache.spark.sql.Row value = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values, schema);
/* 054 */     if (false) {
/* 055 */       mutableRow.setNullAt(0);
/* 056 */     } else {
/* 057 */
/* 058 */       mutableRow.update(0, value);
/* 059 */     }
/* 060 */
/* 061 */     return mutableRow;
/* 062 */   }
/* 063 */ }

13171 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 186 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@6f360c82
13173 [dispatcher-event-loop-3] INFO  org.apache.spark.internal.Logging$class  - Starting task 186.0 in stage 2.0 (TID 188, localhost, partition 186, ANY, 5536 bytes)
13173 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13173 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 2)
13174 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13174 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13173 [Executor task launch worker-0] INFO  org.apache.spark.internal.Logging$class  - Running task 186.0 in stage 2.0 (TID 188)
13174 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13174 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 2)
13174 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 2)
13174 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - SpecificUnsafeRowJoiner(StructType(StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)), StructType(StructField(value,IntegerType,true))):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeRowJoiner();
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeRowJoiner extends org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowJoiner {
/* 006 */   private byte[] buf = new byte[64];
/* 007 */   private UnsafeRow out = new UnsafeRow(4);
/* 008 */
/* 009 */   public UnsafeRow join(UnsafeRow row1, UnsafeRow row2) {
/* 010 */     // row1: 3 fields, 1 words in bitset
/* 011 */     // row2: 1, 1 words in bitset
/* 012 */     // output: 4 fields, 1 words in bitset
/* 013 */     final int sizeInBytes = row1.getSizeInBytes() + row2.getSizeInBytes() - 8;
/* 014 */     if (sizeInBytes > buf.length) {
/* 015 */       buf = new byte[sizeInBytes];
/* 016 */     }
/* 017 */
/* 018 */     final java.lang.Object obj1 = row1.getBaseObject();
/* 019 */     final long offset1 = row1.getBaseOffset();
/* 020 */     final java.lang.Object obj2 = row2.getBaseObject();
/* 021 */     final long offset2 = row2.getBaseOffset();
/* 022 */
/* 023 */     Platform.putLong(buf, 16, Platform.getLong(obj1, offset1 + 0) | (Platform.getLong(obj2, offset2) << 3));
/* 024 */
/* 025 */     // Copy fixed length data for row1
/* 026 */     Platform.copyMemory(
/* 027 */       obj1, offset1 + 8,
/* 028 */       buf, 24,
/* 029 */       24);
/* 030 */
/* 031 */
/* 032 */     // Copy fixed length data for row2
/* 033 */     Platform.copyMemory(
/* 034 */       obj2, offset2 + 8,
/* 035 */       buf, 48,
/* 036 */       8);
/* 037 */
/* 038 */
/* 039 */     // Copy variable length data for row1
/* 040 */     long numBytesVariableRow1 = row1.getSizeInBytes() - 32;
/* 041 */     Platform.copyMemory(
/* 042 */       obj1, offset1 + 32,
/* 043 */       buf, 56,
/* 044 */       numBytesVariableRow1);
/* 045 */
/* 046 */
/* 047 */     // Copy variable length data for row2
/* 048 */     long numBytesVariableRow2 = row2.getSizeInBytes() - 16;
/* 049 */     Platform.copyMemory(
/* 050 */       obj2, offset2 + 16,
/* 051 */       buf, 56 + numBytesVariableRow1,
/* 052 */       numBytesVariableRow2);
/* 053 */
/* 054 */
/* 055 */
/* 056 */
/* 057 */     Platform.putLong(buf, 40, Platform.getLong(buf, 40) + (8L << 32));
/* 058 */
/* 059 */
/* 060 */
/* 061 */     out.pointTo(buf, sizeInBytes);
/* 062 */
/* 063 */     return out;
/* 064 */   }
/* 065 */ }

13173 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 1 (65536 bytes)
13173 [task-result-getter-0] INFO  org.apache.spark.internal.Logging$class  - Finished task 182.0 in stage 2.0 (TID 184) in 94 ms on localhost (183/200)
13174 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.expressions.codegen.package$ExpressionCanonicalizer$CleanExpressions ===
!input[0, int, true] AS value#47   input[0, int, true]
!+- input[0, int, true]            
                
13175 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 2 iterations.
13174 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - ShuffleMapTask finished on driver
13175 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13175 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 2)
13175 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13175 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13175 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13176 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 2)
13176 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 2)
13174 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Task 188's epoch is 2
13175 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - 
=== Result of Batch CleanExpressions ===
!input[0, int, true] AS value#47   input[0, int, true]
!+- input[0, int, true]            
        
13175 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13176 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13176 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Getting local block broadcast_2
13177 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Task 188 trying to acquire read lock for broadcast_2
13177 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Task 188 acquired read lock for broadcast_2
13177 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Level for block broadcast_2 is StorageLevel(disk, memory, deserialized, 1 replicas)
13176 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull = i.isNullAt(0);
/* 030 */     int value = isNull ? -1 : (i.getInt(0));
/* 031 */     if (isNull) {
/* 032 */       rowWriter.setNullAt(0);
/* 033 */     } else {
/* 034 */       rowWriter.write(0, value);
/* 035 */     }
/* 036 */     return result;
/* 037 */   }
/* 038 */ }

13179 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13179 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13179 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for pmod(hash(input[3, int, true], 42), 200):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     boolean isNull = false;
/* 027 */
/* 028 */     int value1 = 42;
/* 029 */
/* 030 */     boolean isNull2 = i.isNullAt(3);
/* 031 */     int value2 = isNull2 ? -1 : (i.getInt(3));
/* 032 */     if (!isNull2) {
/* 033 */       value1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value2, value1);
/* 034 */     }
/* 035 */
/* 036 */     int value = -1;
/* 037 */
/* 038 */     int remainder = value1 % 200;
/* 039 */     if (remainder < 0) {
/* 040 */       value = (remainder + 200) % 200;
/* 041 */     } else {
/* 042 */       value = remainder;
/* 043 */     }
/* 044 */     rowWriter.write(0, value);
/* 045 */     return result;
/* 046 */   }
/* 047 */ }

13180 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13180 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13180 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13180 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13180 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13180 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13180 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13181 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

13181 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 187 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@103b31a6
13181 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 2 (262144 bytes)
13182 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13182 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13182 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13183 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13183 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13183 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13183 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13183 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 5 (16777216 bytes)
13177 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for createexternalrow(input[0, int, true], input[1, int, true], input[2, string, true].toString, StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private MutableRow mutableRow;
/* 009 */   private Object[] values;
/* 010 */   private org.apache.spark.sql.types.StructType schema;
/* 011 */
/* 012 */
/* 013 */   public SpecificSafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     mutableRow = (MutableRow) references[references.length - 1];
/* 016 */
/* 017 */     this.schema = (org.apache.spark.sql.types.StructType) references[0];
/* 018 */   }
/* 019 */
/* 020 */   public java.lang.Object apply(java.lang.Object _i) {
/* 021 */     InternalRow i = (InternalRow) _i;
/* 022 */
/* 023 */     values = new Object[3];
/* 024 */
/* 025 */     boolean isNull1 = i.isNullAt(0);
/* 026 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 027 */     if (isNull1) {
/* 028 */       values[0] = null;
/* 029 */     } else {
/* 030 */       values[0] = value1;
/* 031 */     }
/* 032 */
/* 033 */     boolean isNull2 = i.isNullAt(1);
/* 034 */     int value2 = isNull2 ? -1 : (i.getInt(1));
/* 035 */     if (isNull2) {
/* 036 */       values[1] = null;
/* 037 */     } else {
/* 038 */       values[1] = value2;
/* 039 */     }
/* 040 */
/* 041 */     boolean isNull4 = i.isNullAt(2);
/* 042 */     UTF8String value4 = isNull4 ? null : (i.getUTF8String(2));
/* 043 */
/* 044 */     boolean isNull3 = isNull4;
/* 045 */     final java.lang.String value3 = isNull3 ? null : (java.lang.String) value4.toString();
/* 046 */     isNull3 = value3 == null;
/* 047 */     if (isNull3) {
/* 048 */       values[2] = null;
/* 049 */     } else {
/* 050 */       values[2] = value3;
/* 051 */     }
/* 052 */
/* 053 */     final org.apache.spark.sql.Row value = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values, schema);
/* 054 */     if (false) {
/* 055 */       mutableRow.setNullAt(0);
/* 056 */     } else {
/* 057 */
/* 058 */       mutableRow.update(0, value);
/* 059 */     }
/* 060 */
/* 061 */     return mutableRow;
/* 062 */   }
/* 063 */ }

13184 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - SpecificUnsafeRowJoiner(StructType(StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)), StructType(StructField(value,IntegerType,true))):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeRowJoiner();
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeRowJoiner extends org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowJoiner {
/* 006 */   private byte[] buf = new byte[64];
/* 007 */   private UnsafeRow out = new UnsafeRow(4);
/* 008 */
/* 009 */   public UnsafeRow join(UnsafeRow row1, UnsafeRow row2) {
/* 010 */     // row1: 3 fields, 1 words in bitset
/* 011 */     // row2: 1, 1 words in bitset
/* 012 */     // output: 4 fields, 1 words in bitset
/* 013 */     final int sizeInBytes = row1.getSizeInBytes() + row2.getSizeInBytes() - 8;
/* 014 */     if (sizeInBytes > buf.length) {
/* 015 */       buf = new byte[sizeInBytes];
/* 016 */     }
/* 017 */
/* 018 */     final java.lang.Object obj1 = row1.getBaseObject();
/* 019 */     final long offset1 = row1.getBaseOffset();
/* 020 */     final java.lang.Object obj2 = row2.getBaseObject();
/* 021 */     final long offset2 = row2.getBaseOffset();
/* 022 */
/* 023 */     Platform.putLong(buf, 16, Platform.getLong(obj1, offset1 + 0) | (Platform.getLong(obj2, offset2) << 3));
/* 024 */
/* 025 */     // Copy fixed length data for row1
/* 026 */     Platform.copyMemory(
/* 027 */       obj1, offset1 + 8,
/* 028 */       buf, 24,
/* 029 */       24);
/* 030 */
/* 031 */
/* 032 */     // Copy fixed length data for row2
/* 033 */     Platform.copyMemory(
/* 034 */       obj2, offset2 + 8,
/* 035 */       buf, 48,
/* 036 */       8);
/* 037 */
/* 038 */
/* 039 */     // Copy variable length data for row1
/* 040 */     long numBytesVariableRow1 = row1.getSizeInBytes() - 32;
/* 041 */     Platform.copyMemory(
/* 042 */       obj1, offset1 + 32,
/* 043 */       buf, 56,
/* 044 */       numBytesVariableRow1);
/* 045 */
/* 046 */
/* 047 */     // Copy variable length data for row2
/* 048 */     long numBytesVariableRow2 = row2.getSizeInBytes() - 16;
/* 049 */     Platform.copyMemory(
/* 050 */       obj2, offset2 + 16,
/* 051 */       buf, 56 + numBytesVariableRow1,
/* 052 */       numBytesVariableRow2);
/* 053 */
/* 054 */
/* 055 */
/* 056 */
/* 057 */     Platform.putLong(buf, 40, Platform.getLong(buf, 40) + (8L << 32));
/* 058 */
/* 059 */
/* 060 */
/* 061 */     out.pointTo(buf, sizeInBytes);
/* 062 */
/* 063 */     return out;
/* 064 */   }
/* 065 */ }

13183 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 185 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@799c94a9
13184 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 1 (65536 bytes)
13184 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 185 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@799c94a9
13184 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Task 185 releasing lock for broadcast_2
13183 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13179 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 0, partitions 186-187
13185 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Finished task 183.0 in stage 2.0 (TID 185). 4294 bytes result sent to driver
13184 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.expressions.codegen.package$ExpressionCanonicalizer$CleanExpressions ===
!input[0, int, true] AS value#47   input[0, int, true]
!+- input[0, int, true]            
                
13185 [dispatcher-event-loop-1] DEBUG org.apache.spark.internal.Logging$class  - parentName: , name: TaskSet_2, runningTasks: 3
13185 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

13186 [dispatcher-event-loop-1] INFO  org.apache.spark.internal.Logging$class  - Starting task 187.0 in stage 2.0 (TID 189, localhost, partition 187, ANY, 5536 bytes)
13186 [task-result-getter-1] INFO  org.apache.spark.internal.Logging$class  - Finished task 183.0 in stage 2.0 (TID 185) in 85 ms on localhost (184/200)
13188 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13188 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 2)
13188 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13185 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
13188 [Executor task launch worker-0] INFO  org.apache.spark.internal.Logging$class  - Getting 1 non-empty blocks out of 1 blocks
13189 [Executor task launch worker-0] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 4 ms
13188 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13188 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Running task 187.0 in stage 2.0 (TID 189)
13186 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 187 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@4eecc70
13189 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 3 (262144 bytes)
13189 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Task 189's epoch is 2
13189 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Getting local block broadcast_2
13190 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13185 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 2 iterations.
13190 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - 
=== Result of Batch CleanExpressions ===
!input[0, int, true] AS value#47   input[0, int, true]
!+- input[0, int, true]            
        
13190 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull = i.isNullAt(0);
/* 030 */     int value = isNull ? -1 : (i.getInt(0));
/* 031 */     if (isNull) {
/* 032 */       rowWriter.setNullAt(0);
/* 033 */     } else {
/* 034 */       rowWriter.write(0, value);
/* 035 */     }
/* 036 */     return result;
/* 037 */   }
/* 038 */ }

13191 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13191 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13190 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 187 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@48c304d4
13191 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 4 (16777216 bytes)
13192 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 187 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@58980376
13192 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 5 (16777216 bytes)
13190 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Task 189 trying to acquire read lock for broadcast_2
13192 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Task 189 acquired read lock for broadcast_2
13192 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Level for block broadcast_2 is StorageLevel(disk, memory, deserialized, 1 replicas)
13189 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13194 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 2)
13194 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 2)
13194 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 0, partitions 187-188
13189 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  4 ms
13195 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13195 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13195 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[1, int, true] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       boolean isNull = i.isNullAt(1);
/* 025 */       int value = isNull ? -1 : (i.getInt(1));
/* 026 */       isNullA = isNull;
/* 027 */       primitiveA = value;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB;
/* 031 */     int primitiveB;
/* 032 */     {
/* 033 */
/* 034 */       boolean isNull = i.isNullAt(1);
/* 035 */       int value = isNull ? -1 : (i.getInt(1));
/* 036 */       isNullB = isNull;
/* 037 */       primitiveB = value;
/* 038 */     }
/* 039 */     if (isNullA && isNullB) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */     return 0;
/* 053 */   }
/* 054 */ }

13195 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13196 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13194 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
13196 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Getting 1 non-empty blocks out of 1 blocks
13196 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 2 ms
13196 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  2 ms
13197 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13197 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13197 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[1, int, true] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       boolean isNull = i.isNullAt(1);
/* 025 */       int value = isNull ? -1 : (i.getInt(1));
/* 026 */       isNullA = isNull;
/* 027 */       primitiveA = value;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB;
/* 031 */     int primitiveB;
/* 032 */     {
/* 033 */
/* 034 */       boolean isNull = i.isNullAt(1);
/* 035 */       int value = isNull ? -1 : (i.getInt(1));
/* 036 */       isNullB = isNull;
/* 037 */       primitiveB = value;
/* 038 */     }
/* 039 */     if (isNullA && isNullB) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */     return 0;
/* 053 */   }
/* 054 */ }

13197 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13198 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13198 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[1, int, true] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull1 = i.isNullAt(1);
/* 030 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 031 */     long value = -9223372036854775808L;
/* 032 */     boolean isNull = false;
/* 033 */     if (!isNull1) {
/* 034 */       value = (long) value1;
/* 035 */     }
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */     return result;
/* 042 */   }
/* 043 */ }

13198 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 189 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@691af726
13198 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (65536 bytes)
13199 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 1, partitions 187-188
13194 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - ShuffleMapTask finished on driver
13199 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13199 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 2)
13199 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13199 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13199 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13199 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 2)
13199 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 2)
13192 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 187 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@4eecc70
13200 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 6 (16777216 bytes)
13200 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 4 (16777216 bytes)
13200 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 187 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@48c304d4
13200 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (65536 bytes)
13200 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 187 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@48c304d4
13200 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 187 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@103b31a6
13200 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (16777216 bytes)
13201 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 3 (262144 bytes)
13201 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 187 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@4eecc70
13201 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 6 (16777216 bytes)
13201 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 187 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@4eecc70
13191 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for pmod(hash(input[3, int, true], 42), 200):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     boolean isNull = false;
/* 027 */
/* 028 */     int value1 = 42;
/* 029 */
/* 030 */     boolean isNull2 = i.isNullAt(3);
/* 031 */     int value2 = isNull2 ? -1 : (i.getInt(3));
/* 032 */     if (!isNull2) {
/* 033 */       value1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value2, value1);
/* 034 */     }
/* 035 */
/* 036 */     int value = -1;
/* 037 */
/* 038 */     int remainder = value1 % 200;
/* 039 */     if (remainder < 0) {
/* 040 */       value = (remainder + 200) % 200;
/* 041 */     } else {
/* 042 */       value = remainder;
/* 043 */     }
/* 044 */     rowWriter.write(0, value);
/* 045 */     return result;
/* 046 */   }
/* 047 */ }

13202 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13202 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13202 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13203 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13203 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13203 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13203 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13204 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

13204 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 186 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@415b2751
13204 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 2 (262144 bytes)
13204 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13205 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13205 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13205 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13205 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13205 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13205 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13206 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13206 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

13206 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 186 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@52ecdeb8
13206 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 3 (262144 bytes)
13207 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13207 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 186 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@7928d25c
13207 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 4 (16777216 bytes)
13199 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
13208 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Getting 1 non-empty blocks out of 1 blocks
13208 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 9 ms
13208 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  9 ms
13208 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13208 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13196 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[1, int, true] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull1 = i.isNullAt(1);
/* 030 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 031 */     long value = -9223372036854775808L;
/* 032 */     boolean isNull = false;
/* 033 */     if (!isNull1) {
/* 034 */       value = (long) value1;
/* 035 */     }
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */     return result;
/* 042 */   }
/* 043 */ }

13209 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 188 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@43765010
13209 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (65536 bytes)
13209 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 1, partitions 186-187
13209 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
13208 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 186 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@6f360c82
13209 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 5 (16777216 bytes)
13210 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 186 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@52ecdeb8
13210 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 6 (16777216 bytes)
13209 [Executor task launch worker-0] INFO  org.apache.spark.internal.Logging$class  - Getting 1 non-empty blocks out of 1 blocks
13210 [Executor task launch worker-0] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 1 ms
13210 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  1 ms
13210 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13211 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13211 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[0, int, false] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       int value = i.getInt(0);
/* 025 */       isNullA = false;
/* 026 */       primitiveA = value;
/* 027 */     }
/* 028 */     i = b;
/* 029 */     boolean isNullB;
/* 030 */     int primitiveB;
/* 031 */     {
/* 032 */
/* 033 */       int value = i.getInt(0);
/* 034 */       isNullB = false;
/* 035 */       primitiveB = value;
/* 036 */     }
/* 037 */     if (isNullA && isNullB) {
/* 038 */       // Nothing
/* 039 */     } else if (isNullA) {
/* 040 */       return -1;
/* 041 */     } else if (isNullB) {
/* 042 */       return 1;
/* 043 */     } else {
/* 044 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 045 */       if (comp != 0) {
/* 046 */         return comp;
/* 047 */       }
/* 048 */     }
/* 049 */
/* 050 */     return 0;
/* 051 */   }
/* 052 */ }

13211 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 2 (262144 bytes)
13211 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 187 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@103b31a6
13211 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (16777216 bytes)
13212 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 187 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@103b31a6
13209 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[0, int, false] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       int value = i.getInt(0);
/* 025 */       isNullA = false;
/* 026 */       primitiveA = value;
/* 027 */     }
/* 028 */     i = b;
/* 029 */     boolean isNullB;
/* 030 */     int primitiveB;
/* 031 */     {
/* 032 */
/* 033 */       int value = i.getInt(0);
/* 034 */       isNullB = false;
/* 035 */       primitiveB = value;
/* 036 */     }
/* 037 */     if (isNullA && isNullB) {
/* 038 */       // Nothing
/* 039 */     } else if (isNullA) {
/* 040 */       return -1;
/* 041 */     } else if (isNullB) {
/* 042 */       return 1;
/* 043 */     } else {
/* 044 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 045 */       if (comp != 0) {
/* 046 */         return comp;
/* 047 */       }
/* 048 */     }
/* 049 */
/* 050 */     return 0;
/* 051 */   }
/* 052 */ }

13212 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13212 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13211 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13213 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13213 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[0, int, false] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     int value1 = i.getInt(0);
/* 030 */     long value = -9223372036854775808L;
/* 031 */     boolean isNull = false;
/* 032 */     if (!false) {
/* 033 */       value = (long) value1;
/* 034 */     }
/* 035 */     if (isNull) {
/* 036 */       rowWriter.setNullAt(0);
/* 037 */     } else {
/* 038 */       rowWriter.write(0, value);
/* 039 */     }
/* 040 */     return result;
/* 041 */   }
/* 042 */ }

13213 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 189 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@453e173f
13213 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[0, int, false] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     int value1 = i.getInt(0);
/* 030 */     long value = -9223372036854775808L;
/* 031 */     boolean isNull = false;
/* 032 */     if (!false) {
/* 033 */       value = (long) value1;
/* 034 */     }
/* 035 */     if (isNull) {
/* 036 */       rowWriter.setNullAt(0);
/* 037 */     } else {
/* 038 */       rowWriter.write(0, value);
/* 039 */     }
/* 040 */     return result;
/* 041 */   }
/* 042 */ }

13210 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 5 (16777216 bytes)
13214 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 186 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@6f360c82
13214 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 1 (65536 bytes)
13214 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 186 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@6f360c82
13214 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 4 (16777216 bytes)
13214 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 186 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@7928d25c
13214 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (65536 bytes)
13214 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 186 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@7928d25c
13214 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 188 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@25923682
13215 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 1 (65536 bytes)
13213 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 1 (65536 bytes)
13215 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13215 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13215 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 186 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@415b2751
13216 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (16777216 bytes)
13216 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 3 (262144 bytes)
13215 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13216 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13216 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 186 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@52ecdeb8
13216 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 6 (16777216 bytes)
13216 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for createexternalrow(input[0, int, true], input[1, int, true], input[2, string, true].toString, StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private MutableRow mutableRow;
/* 009 */   private Object[] values;
/* 010 */   private org.apache.spark.sql.types.StructType schema;
/* 011 */
/* 012 */
/* 013 */   public SpecificSafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     mutableRow = (MutableRow) references[references.length - 1];
/* 016 */
/* 017 */     this.schema = (org.apache.spark.sql.types.StructType) references[0];
/* 018 */   }
/* 019 */
/* 020 */   public java.lang.Object apply(java.lang.Object _i) {
/* 021 */     InternalRow i = (InternalRow) _i;
/* 022 */
/* 023 */     values = new Object[3];
/* 024 */
/* 025 */     boolean isNull1 = i.isNullAt(0);
/* 026 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 027 */     if (isNull1) {
/* 028 */       values[0] = null;
/* 029 */     } else {
/* 030 */       values[0] = value1;
/* 031 */     }
/* 032 */
/* 033 */     boolean isNull2 = i.isNullAt(1);
/* 034 */     int value2 = isNull2 ? -1 : (i.getInt(1));
/* 035 */     if (isNull2) {
/* 036 */       values[1] = null;
/* 037 */     } else {
/* 038 */       values[1] = value2;
/* 039 */     }
/* 040 */
/* 041 */     boolean isNull4 = i.isNullAt(2);
/* 042 */     UTF8String value4 = isNull4 ? null : (i.getUTF8String(2));
/* 043 */
/* 044 */     boolean isNull3 = isNull4;
/* 045 */     final java.lang.String value3 = isNull3 ? null : (java.lang.String) value4.toString();
/* 046 */     isNull3 = value3 == null;
/* 047 */     if (isNull3) {
/* 048 */       values[2] = null;
/* 049 */     } else {
/* 050 */       values[2] = value3;
/* 051 */     }
/* 052 */
/* 053 */     final org.apache.spark.sql.Row value = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values, schema);
/* 054 */     if (false) {
/* 055 */       mutableRow.setNullAt(0);
/* 056 */     } else {
/* 057 */
/* 058 */       mutableRow.update(0, value);
/* 059 */     }
/* 060 */
/* 061 */     return mutableRow;
/* 062 */   }
/* 063 */ }

13216 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 186 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@52ecdeb8
13217 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for createexternalrow(input[0, int, true], input[1, int, true], input[2, string, true].toString, StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private MutableRow mutableRow;
/* 009 */   private Object[] values;
/* 010 */   private org.apache.spark.sql.types.StructType schema;
/* 011 */
/* 012 */
/* 013 */   public SpecificSafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     mutableRow = (MutableRow) references[references.length - 1];
/* 016 */
/* 017 */     this.schema = (org.apache.spark.sql.types.StructType) references[0];
/* 018 */   }
/* 019 */
/* 020 */   public java.lang.Object apply(java.lang.Object _i) {
/* 021 */     InternalRow i = (InternalRow) _i;
/* 022 */
/* 023 */     values = new Object[3];
/* 024 */
/* 025 */     boolean isNull1 = i.isNullAt(0);
/* 026 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 027 */     if (isNull1) {
/* 028 */       values[0] = null;
/* 029 */     } else {
/* 030 */       values[0] = value1;
/* 031 */     }
/* 032 */
/* 033 */     boolean isNull2 = i.isNullAt(1);
/* 034 */     int value2 = isNull2 ? -1 : (i.getInt(1));
/* 035 */     if (isNull2) {
/* 036 */       values[1] = null;
/* 037 */     } else {
/* 038 */       values[1] = value2;
/* 039 */     }
/* 040 */
/* 041 */     boolean isNull4 = i.isNullAt(2);
/* 042 */     UTF8String value4 = isNull4 ? null : (i.getUTF8String(2));
/* 043 */
/* 044 */     boolean isNull3 = isNull4;
/* 045 */     final java.lang.String value3 = isNull3 ? null : (java.lang.String) value4.toString();
/* 046 */     isNull3 = value3 == null;
/* 047 */     if (isNull3) {
/* 048 */       values[2] = null;
/* 049 */     } else {
/* 050 */       values[2] = value3;
/* 051 */     }
/* 052 */
/* 053 */     final org.apache.spark.sql.Row value = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values, schema);
/* 054 */     if (false) {
/* 055 */       mutableRow.setNullAt(0);
/* 056 */     } else {
/* 057 */
/* 058 */       mutableRow.update(0, value);
/* 059 */     }
/* 060 */
/* 061 */     return mutableRow;
/* 062 */   }
/* 063 */ }

13217 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - SpecificUnsafeRowJoiner(StructType(StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)), StructType(StructField(value,IntegerType,true))):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeRowJoiner();
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeRowJoiner extends org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowJoiner {
/* 006 */   private byte[] buf = new byte[64];
/* 007 */   private UnsafeRow out = new UnsafeRow(4);
/* 008 */
/* 009 */   public UnsafeRow join(UnsafeRow row1, UnsafeRow row2) {
/* 010 */     // row1: 3 fields, 1 words in bitset
/* 011 */     // row2: 1, 1 words in bitset
/* 012 */     // output: 4 fields, 1 words in bitset
/* 013 */     final int sizeInBytes = row1.getSizeInBytes() + row2.getSizeInBytes() - 8;
/* 014 */     if (sizeInBytes > buf.length) {
/* 015 */       buf = new byte[sizeInBytes];
/* 016 */     }
/* 017 */
/* 018 */     final java.lang.Object obj1 = row1.getBaseObject();
/* 019 */     final long offset1 = row1.getBaseOffset();
/* 020 */     final java.lang.Object obj2 = row2.getBaseObject();
/* 021 */     final long offset2 = row2.getBaseOffset();
/* 022 */
/* 023 */     Platform.putLong(buf, 16, Platform.getLong(obj1, offset1 + 0) | (Platform.getLong(obj2, offset2) << 3));
/* 024 */
/* 025 */     // Copy fixed length data for row1
/* 026 */     Platform.copyMemory(
/* 027 */       obj1, offset1 + 8,
/* 028 */       buf, 24,
/* 029 */       24);
/* 030 */
/* 031 */
/* 032 */     // Copy fixed length data for row2
/* 033 */     Platform.copyMemory(
/* 034 */       obj2, offset2 + 8,
/* 035 */       buf, 48,
/* 036 */       8);
/* 037 */
/* 038 */
/* 039 */     // Copy variable length data for row1
/* 040 */     long numBytesVariableRow1 = row1.getSizeInBytes() - 32;
/* 041 */     Platform.copyMemory(
/* 042 */       obj1, offset1 + 32,
/* 043 */       buf, 56,
/* 044 */       numBytesVariableRow1);
/* 045 */
/* 046 */
/* 047 */     // Copy variable length data for row2
/* 048 */     long numBytesVariableRow2 = row2.getSizeInBytes() - 16;
/* 049 */     Platform.copyMemory(
/* 050 */       obj2, offset2 + 16,
/* 051 */       buf, 56 + numBytesVariableRow1,
/* 052 */       numBytesVariableRow2);
/* 053 */
/* 054 */
/* 055 */
/* 056 */
/* 057 */     Platform.putLong(buf, 40, Platform.getLong(buf, 40) + (8L << 32));
/* 058 */
/* 059 */
/* 060 */
/* 061 */     out.pointTo(buf, sizeInBytes);
/* 062 */
/* 063 */     return out;
/* 064 */   }
/* 065 */ }

13217 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.expressions.codegen.package$ExpressionCanonicalizer$CleanExpressions ===
!input[0, int, true] AS value#47   input[0, int, true]
!+- input[0, int, true]            
                
13217 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 2 iterations.
13217 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - SpecificUnsafeRowJoiner(StructType(StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)), StructType(StructField(value,IntegerType,true))):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeRowJoiner();
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeRowJoiner extends org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowJoiner {
/* 006 */   private byte[] buf = new byte[64];
/* 007 */   private UnsafeRow out = new UnsafeRow(4);
/* 008 */
/* 009 */   public UnsafeRow join(UnsafeRow row1, UnsafeRow row2) {
/* 010 */     // row1: 3 fields, 1 words in bitset
/* 011 */     // row2: 1, 1 words in bitset
/* 012 */     // output: 4 fields, 1 words in bitset
/* 013 */     final int sizeInBytes = row1.getSizeInBytes() + row2.getSizeInBytes() - 8;
/* 014 */     if (sizeInBytes > buf.length) {
/* 015 */       buf = new byte[sizeInBytes];
/* 016 */     }
/* 017 */
/* 018 */     final java.lang.Object obj1 = row1.getBaseObject();
/* 019 */     final long offset1 = row1.getBaseOffset();
/* 020 */     final java.lang.Object obj2 = row2.getBaseObject();
/* 021 */     final long offset2 = row2.getBaseOffset();
/* 022 */
/* 023 */     Platform.putLong(buf, 16, Platform.getLong(obj1, offset1 + 0) | (Platform.getLong(obj2, offset2) << 3));
/* 024 */
/* 025 */     // Copy fixed length data for row1
/* 026 */     Platform.copyMemory(
/* 027 */       obj1, offset1 + 8,
/* 028 */       buf, 24,
/* 029 */       24);
/* 030 */
/* 031 */
/* 032 */     // Copy fixed length data for row2
/* 033 */     Platform.copyMemory(
/* 034 */       obj2, offset2 + 8,
/* 035 */       buf, 48,
/* 036 */       8);
/* 037 */
/* 038 */
/* 039 */     // Copy variable length data for row1
/* 040 */     long numBytesVariableRow1 = row1.getSizeInBytes() - 32;
/* 041 */     Platform.copyMemory(
/* 042 */       obj1, offset1 + 32,
/* 043 */       buf, 56,
/* 044 */       numBytesVariableRow1);
/* 045 */
/* 046 */
/* 047 */     // Copy variable length data for row2
/* 048 */     long numBytesVariableRow2 = row2.getSizeInBytes() - 16;
/* 049 */     Platform.copyMemory(
/* 050 */       obj2, offset2 + 16,
/* 051 */       buf, 56 + numBytesVariableRow1,
/* 052 */       numBytesVariableRow2);
/* 053 */
/* 054 */
/* 055 */
/* 056 */
/* 057 */     Platform.putLong(buf, 40, Platform.getLong(buf, 40) + (8L << 32));
/* 058 */
/* 059 */
/* 060 */
/* 061 */     out.pointTo(buf, sizeInBytes);
/* 062 */
/* 063 */     return out;
/* 064 */   }
/* 065 */ }

13217 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - 
=== Result of Batch CleanExpressions ===
!input[0, int, true] AS value#47   input[0, int, true]
!+- input[0, int, true]            
        
13218 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.expressions.codegen.package$ExpressionCanonicalizer$CleanExpressions ===
!input[0, int, true] AS value#47   input[0, int, true]
!+- input[0, int, true]            
                
13218 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 2 iterations.
13218 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - 
=== Result of Batch CleanExpressions ===
!input[0, int, true] AS value#47   input[0, int, true]
!+- input[0, int, true]            
        
13219 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull = i.isNullAt(0);
/* 030 */     int value = isNull ? -1 : (i.getInt(0));
/* 031 */     if (isNull) {
/* 032 */       rowWriter.setNullAt(0);
/* 033 */     } else {
/* 034 */       rowWriter.write(0, value);
/* 035 */     }
/* 036 */     return result;
/* 037 */   }
/* 038 */ }

13219 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13219 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13218 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull = i.isNullAt(0);
/* 030 */     int value = isNull ? -1 : (i.getInt(0));
/* 031 */     if (isNull) {
/* 032 */       rowWriter.setNullAt(0);
/* 033 */     } else {
/* 034 */       rowWriter.write(0, value);
/* 035 */     }
/* 036 */     return result;
/* 037 */   }
/* 038 */ }

13220 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for pmod(hash(input[3, int, true], 42), 200):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     boolean isNull = false;
/* 027 */
/* 028 */     int value1 = 42;
/* 029 */
/* 030 */     boolean isNull2 = i.isNullAt(3);
/* 031 */     int value2 = isNull2 ? -1 : (i.getInt(3));
/* 032 */     if (!isNull2) {
/* 033 */       value1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value2, value1);
/* 034 */     }
/* 035 */
/* 036 */     int value = -1;
/* 037 */
/* 038 */     int remainder = value1 % 200;
/* 039 */     if (remainder < 0) {
/* 040 */       value = (remainder + 200) % 200;
/* 041 */     } else {
/* 042 */       value = remainder;
/* 043 */     }
/* 044 */     rowWriter.write(0, value);
/* 045 */     return result;
/* 046 */   }
/* 047 */ }

13220 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13220 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13220 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13220 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13221 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13221 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13221 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13220 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13221 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13221 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

13222 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 189 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@506f65b1
13222 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for pmod(hash(input[3, int, true], 42), 200):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     boolean isNull = false;
/* 027 */
/* 028 */     int value1 = 42;
/* 029 */
/* 030 */     boolean isNull2 = i.isNullAt(3);
/* 031 */     int value2 = isNull2 ? -1 : (i.getInt(3));
/* 032 */     if (!isNull2) {
/* 033 */       value1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value2, value1);
/* 034 */     }
/* 035 */
/* 036 */     int value = -1;
/* 037 */
/* 038 */     int remainder = value1 % 200;
/* 039 */     if (remainder < 0) {
/* 040 */       value = (remainder + 200) % 200;
/* 041 */     } else {
/* 042 */       value = remainder;
/* 043 */     }
/* 044 */     rowWriter.write(0, value);
/* 045 */     return result;
/* 046 */   }
/* 047 */ }

13222 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 2 (262144 bytes)
13222 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13222 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13222 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13222 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13223 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13223 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13223 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13223 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

13224 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 188 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@58979567
13222 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13224 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13224 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13224 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13225 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13225 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13225 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13225 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13224 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 2 (262144 bytes)
13226 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13226 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

13226 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13226 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13227 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13227 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13227 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13227 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13227 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13226 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 189 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@5a540203
13228 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 3 (262144 bytes)
13228 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

13228 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13228 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 188 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@26d84836
13229 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 189 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@691af726
13229 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 4 (16777216 bytes)
13229 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 3 (262144 bytes)
13229 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 189 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@453e173f
13229 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 5 (16777216 bytes)
13230 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 2 (262144 bytes)
13230 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 189 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@5a540203
13230 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 6 (16777216 bytes)
13229 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13230 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 186 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@415b2751
13230 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 188 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@43765010
13230 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 5 (16777216 bytes)
13231 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 187 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@58980376
13231 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 1 (65536 bytes)
13231 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 187 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@58980376
13231 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Task 187 releasing lock for broadcast_2
13230 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 4 (16777216 bytes)
13231 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 189 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@691af726
13232 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (65536 bytes)
13231 [Executor task launch worker-3] INFO  org.apache.spark.internal.Logging$class  - Finished task 185.0 in stage 2.0 (TID 187). 4221 bytes result sent to driver
13230 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 4 (16777216 bytes)
13230 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (16777216 bytes)
13232 [dispatcher-event-loop-5] DEBUG org.apache.spark.internal.Logging$class  - parentName: , name: TaskSet_2, runningTasks: 3
13232 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 188 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@25923682
13233 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 5 (16777216 bytes)
13233 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 188 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@26d84836
13233 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 6 (16777216 bytes)
13233 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 4 (16777216 bytes)
13233 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 188 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@43765010
13234 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (65536 bytes)
13234 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 188 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@43765010
13234 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 188 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@58979567
13234 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (16777216 bytes)
13234 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 3 (262144 bytes)
13234 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 188 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@26d84836
13234 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 6 (16777216 bytes)
13234 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 188 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@26d84836
13232 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 189 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@691af726
13233 [dispatcher-event-loop-5] INFO  org.apache.spark.internal.Logging$class  - Starting task 188.0 in stage 2.0 (TID 190, localhost, partition 188, ANY, 5536 bytes)
13232 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 186 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@415b2751
13235 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13235 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 189 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@506f65b1
13235 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (16777216 bytes)
13235 [Executor task launch worker-3] INFO  org.apache.spark.internal.Logging$class  - Running task 188.0 in stage 2.0 (TID 190)
13235 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 2)
13235 [task-result-getter-2] INFO  org.apache.spark.internal.Logging$class  - Finished task 185.0 in stage 2.0 (TID 187) in 75 ms on localhost (185/200)
13236 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13236 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13235 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 3 (262144 bytes)
13236 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 189 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@5a540203
13236 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Task 190's epoch is 2
13236 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13236 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 2)
13236 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 2)
13236 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Getting local block broadcast_2
13237 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Task 190 trying to acquire read lock for broadcast_2
13236 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 6 (16777216 bytes)
13237 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Task 190 acquired read lock for broadcast_2
13237 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Level for block broadcast_2 is StorageLevel(disk, memory, deserialized, 1 replicas)
13237 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - ShuffleMapTask finished on driver
13237 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13237 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 2)
13237 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13237 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13238 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13238 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 2)
13237 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 189 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@5a540203
13238 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 2)
13239 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 0, partitions 188-189
13239 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
13239 [Executor task launch worker-3] INFO  org.apache.spark.internal.Logging$class  - Getting 1 non-empty blocks out of 1 blocks
13239 [Executor task launch worker-3] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 0 ms
13239 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  0 ms
13240 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13240 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13240 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[1, int, true] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       boolean isNull = i.isNullAt(1);
/* 025 */       int value = isNull ? -1 : (i.getInt(1));
/* 026 */       isNullA = isNull;
/* 027 */       primitiveA = value;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB;
/* 031 */     int primitiveB;
/* 032 */     {
/* 033 */
/* 034 */       boolean isNull = i.isNullAt(1);
/* 035 */       int value = isNull ? -1 : (i.getInt(1));
/* 036 */       isNullB = isNull;
/* 037 */       primitiveB = value;
/* 038 */     }
/* 039 */     if (isNullA && isNullB) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */     return 0;
/* 053 */   }
/* 054 */ }

13241 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13241 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13241 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[1, int, true] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull1 = i.isNullAt(1);
/* 030 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 031 */     long value = -9223372036854775808L;
/* 032 */     boolean isNull = false;
/* 033 */     if (!isNull1) {
/* 034 */       value = (long) value1;
/* 035 */     }
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */     return result;
/* 042 */   }
/* 043 */ }

13243 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 190 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2244bdb3
13243 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (65536 bytes)
13244 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 1, partitions 188-189
13244 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
13244 [Executor task launch worker-3] INFO  org.apache.spark.internal.Logging$class  - Getting 1 non-empty blocks out of 1 blocks
13244 [Executor task launch worker-3] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 0 ms
13244 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  0 ms
13245 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13245 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13245 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[0, int, false] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       int value = i.getInt(0);
/* 025 */       isNullA = false;
/* 026 */       primitiveA = value;
/* 027 */     }
/* 028 */     i = b;
/* 029 */     boolean isNullB;
/* 030 */     int primitiveB;
/* 031 */     {
/* 032 */
/* 033 */       int value = i.getInt(0);
/* 034 */       isNullB = false;
/* 035 */       primitiveB = value;
/* 036 */     }
/* 037 */     if (isNullA && isNullB) {
/* 038 */       // Nothing
/* 039 */     } else if (isNullA) {
/* 040 */       return -1;
/* 041 */     } else if (isNullB) {
/* 042 */       return 1;
/* 043 */     } else {
/* 044 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 045 */       if (comp != 0) {
/* 046 */         return comp;
/* 047 */       }
/* 048 */     }
/* 049 */
/* 050 */     return 0;
/* 051 */   }
/* 052 */ }

13246 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13246 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13246 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[0, int, false] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     int value1 = i.getInt(0);
/* 030 */     long value = -9223372036854775808L;
/* 031 */     boolean isNull = false;
/* 032 */     if (!false) {
/* 033 */       value = (long) value1;
/* 034 */     }
/* 035 */     if (isNull) {
/* 036 */       rowWriter.setNullAt(0);
/* 037 */     } else {
/* 038 */       rowWriter.write(0, value);
/* 039 */     }
/* 040 */     return result;
/* 041 */   }
/* 042 */ }

13247 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 190 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@343b805
13247 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 1 (65536 bytes)
13247 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13247 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13247 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 2 (262144 bytes)
13248 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 188 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@58979567
13248 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (16777216 bytes)
13248 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 188 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@58979567
13248 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for createexternalrow(input[0, int, true], input[1, int, true], input[2, string, true].toString, StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private MutableRow mutableRow;
/* 009 */   private Object[] values;
/* 010 */   private org.apache.spark.sql.types.StructType schema;
/* 011 */
/* 012 */
/* 013 */   public SpecificSafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     mutableRow = (MutableRow) references[references.length - 1];
/* 016 */
/* 017 */     this.schema = (org.apache.spark.sql.types.StructType) references[0];
/* 018 */   }
/* 019 */
/* 020 */   public java.lang.Object apply(java.lang.Object _i) {
/* 021 */     InternalRow i = (InternalRow) _i;
/* 022 */
/* 023 */     values = new Object[3];
/* 024 */
/* 025 */     boolean isNull1 = i.isNullAt(0);
/* 026 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 027 */     if (isNull1) {
/* 028 */       values[0] = null;
/* 029 */     } else {
/* 030 */       values[0] = value1;
/* 031 */     }
/* 032 */
/* 033 */     boolean isNull2 = i.isNullAt(1);
/* 034 */     int value2 = isNull2 ? -1 : (i.getInt(1));
/* 035 */     if (isNull2) {
/* 036 */       values[1] = null;
/* 037 */     } else {
/* 038 */       values[1] = value2;
/* 039 */     }
/* 040 */
/* 041 */     boolean isNull4 = i.isNullAt(2);
/* 042 */     UTF8String value4 = isNull4 ? null : (i.getUTF8String(2));
/* 043 */
/* 044 */     boolean isNull3 = isNull4;
/* 045 */     final java.lang.String value3 = isNull3 ? null : (java.lang.String) value4.toString();
/* 046 */     isNull3 = value3 == null;
/* 047 */     if (isNull3) {
/* 048 */       values[2] = null;
/* 049 */     } else {
/* 050 */       values[2] = value3;
/* 051 */     }
/* 052 */
/* 053 */     final org.apache.spark.sql.Row value = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values, schema);
/* 054 */     if (false) {
/* 055 */       mutableRow.setNullAt(0);
/* 056 */     } else {
/* 057 */
/* 058 */       mutableRow.update(0, value);
/* 059 */     }
/* 060 */
/* 061 */     return mutableRow;
/* 062 */   }
/* 063 */ }

13249 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - SpecificUnsafeRowJoiner(StructType(StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)), StructType(StructField(value,IntegerType,true))):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeRowJoiner();
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeRowJoiner extends org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowJoiner {
/* 006 */   private byte[] buf = new byte[64];
/* 007 */   private UnsafeRow out = new UnsafeRow(4);
/* 008 */
/* 009 */   public UnsafeRow join(UnsafeRow row1, UnsafeRow row2) {
/* 010 */     // row1: 3 fields, 1 words in bitset
/* 011 */     // row2: 1, 1 words in bitset
/* 012 */     // output: 4 fields, 1 words in bitset
/* 013 */     final int sizeInBytes = row1.getSizeInBytes() + row2.getSizeInBytes() - 8;
/* 014 */     if (sizeInBytes > buf.length) {
/* 015 */       buf = new byte[sizeInBytes];
/* 016 */     }
/* 017 */
/* 018 */     final java.lang.Object obj1 = row1.getBaseObject();
/* 019 */     final long offset1 = row1.getBaseOffset();
/* 020 */     final java.lang.Object obj2 = row2.getBaseObject();
/* 021 */     final long offset2 = row2.getBaseOffset();
/* 022 */
/* 023 */     Platform.putLong(buf, 16, Platform.getLong(obj1, offset1 + 0) | (Platform.getLong(obj2, offset2) << 3));
/* 024 */
/* 025 */     // Copy fixed length data for row1
/* 026 */     Platform.copyMemory(
/* 027 */       obj1, offset1 + 8,
/* 028 */       buf, 24,
/* 029 */       24);
/* 030 */
/* 031 */
/* 032 */     // Copy fixed length data for row2
/* 033 */     Platform.copyMemory(
/* 034 */       obj2, offset2 + 8,
/* 035 */       buf, 48,
/* 036 */       8);
/* 037 */
/* 038 */
/* 039 */     // Copy variable length data for row1
/* 040 */     long numBytesVariableRow1 = row1.getSizeInBytes() - 32;
/* 041 */     Platform.copyMemory(
/* 042 */       obj1, offset1 + 32,
/* 043 */       buf, 56,
/* 044 */       numBytesVariableRow1);
/* 045 */
/* 046 */
/* 047 */     // Copy variable length data for row2
/* 048 */     long numBytesVariableRow2 = row2.getSizeInBytes() - 16;
/* 049 */     Platform.copyMemory(
/* 050 */       obj2, offset2 + 16,
/* 051 */       buf, 56 + numBytesVariableRow1,
/* 052 */       numBytesVariableRow2);
/* 053 */
/* 054 */
/* 055 */
/* 056 */
/* 057 */     Platform.putLong(buf, 40, Platform.getLong(buf, 40) + (8L << 32));
/* 058 */
/* 059 */
/* 060 */
/* 061 */     out.pointTo(buf, sizeInBytes);
/* 062 */
/* 063 */     return out;
/* 064 */   }
/* 065 */ }

13249 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.expressions.codegen.package$ExpressionCanonicalizer$CleanExpressions ===
!input[0, int, true] AS value#47   input[0, int, true]
!+- input[0, int, true]            
                
13249 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 2 iterations.
13249 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - 
=== Result of Batch CleanExpressions ===
!input[0, int, true] AS value#47   input[0, int, true]
!+- input[0, int, true]            
        
13250 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull = i.isNullAt(0);
/* 030 */     int value = isNull ? -1 : (i.getInt(0));
/* 031 */     if (isNull) {
/* 032 */       rowWriter.setNullAt(0);
/* 033 */     } else {
/* 034 */       rowWriter.write(0, value);
/* 035 */     }
/* 036 */     return result;
/* 037 */   }
/* 038 */ }

13250 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13250 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13251 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for pmod(hash(input[3, int, true], 42), 200):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     boolean isNull = false;
/* 027 */
/* 028 */     int value1 = 42;
/* 029 */
/* 030 */     boolean isNull2 = i.isNullAt(3);
/* 031 */     int value2 = isNull2 ? -1 : (i.getInt(3));
/* 032 */     if (!isNull2) {
/* 033 */       value1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value2, value1);
/* 034 */     }
/* 035 */
/* 036 */     int value = -1;
/* 037 */
/* 038 */     int remainder = value1 % 200;
/* 039 */     if (remainder < 0) {
/* 040 */       value = (remainder + 200) % 200;
/* 041 */     } else {
/* 042 */       value = remainder;
/* 043 */     }
/* 044 */     rowWriter.write(0, value);
/* 045 */     return result;
/* 046 */   }
/* 047 */ }

13251 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13251 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13252 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13252 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13252 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13252 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13253 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13253 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

13254 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 190 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@2bba50a0
13254 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 2 (262144 bytes)
13254 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 189 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@506f65b1
13254 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (16777216 bytes)
13254 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 189 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@506f65b1
13254 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 2 (262144 bytes)
13255 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13255 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13256 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13256 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13256 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13256 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13256 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13257 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13258 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

13258 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 190 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@625c8367
13258 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 3 (262144 bytes)
13259 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13259 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 190 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2244bdb3
13259 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 4 (16777216 bytes)
13260 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 190 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@343b805
13260 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 5 (16777216 bytes)
13261 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 190 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@625c8367
13261 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 6 (16777216 bytes)
13261 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 4 (16777216 bytes)
13261 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 190 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2244bdb3
13262 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (65536 bytes)
13262 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 190 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2244bdb3
13262 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 190 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@2bba50a0
13262 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (16777216 bytes)
13262 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 3 (262144 bytes)
13262 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 190 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@625c8367
13262 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 6 (16777216 bytes)
13262 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Task 186 releasing lock for broadcast_2
13262 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 190 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@625c8367
13263 [Executor task launch worker-4] INFO  org.apache.spark.internal.Logging$class  - Finished task 184.0 in stage 2.0 (TID 186). 4294 bytes result sent to driver
13263 [dispatcher-event-loop-2] DEBUG org.apache.spark.internal.Logging$class  - parentName: , name: TaskSet_2, runningTasks: 3
13264 [dispatcher-event-loop-2] INFO  org.apache.spark.internal.Logging$class  - Starting task 189.0 in stage 2.0 (TID 191, localhost, partition 189, ANY, 5536 bytes)
13264 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13264 [task-result-getter-3] INFO  org.apache.spark.internal.Logging$class  - Finished task 184.0 in stage 2.0 (TID 186) in 140 ms on localhost (186/200)
13264 [Executor task launch worker-4] INFO  org.apache.spark.internal.Logging$class  - Running task 189.0 in stage 2.0 (TID 191)
13264 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 2)
13264 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13264 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13265 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13265 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 2)
13265 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 2)
13265 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - ShuffleMapTask finished on driver
13265 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13265 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 2)
13265 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13265 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13265 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13265 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 2)
13265 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Task 191's epoch is 2
13266 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 2)
13266 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Getting local block broadcast_2
13266 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Task 191 trying to acquire read lock for broadcast_2
13266 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Task 191 acquired read lock for broadcast_2
13266 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Level for block broadcast_2 is StorageLevel(disk, memory, deserialized, 1 replicas)
13267 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 5 (16777216 bytes)
13268 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 188 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@25923682
13268 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 1 (65536 bytes)
13268 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 188 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@25923682
13268 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Task 188 releasing lock for broadcast_2
13269 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 0, partitions 189-190
13269 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
13269 [Executor task launch worker-4] INFO  org.apache.spark.internal.Logging$class  - Getting 1 non-empty blocks out of 1 blocks
13269 [Executor task launch worker-4] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 0 ms
13269 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  0 ms
13269 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13270 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13269 [Executor task launch worker-0] INFO  org.apache.spark.internal.Logging$class  - Finished task 186.0 in stage 2.0 (TID 188). 4221 bytes result sent to driver
13270 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[1, int, true] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       boolean isNull = i.isNullAt(1);
/* 025 */       int value = isNull ? -1 : (i.getInt(1));
/* 026 */       isNullA = isNull;
/* 027 */       primitiveA = value;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB;
/* 031 */     int primitiveB;
/* 032 */     {
/* 033 */
/* 034 */       boolean isNull = i.isNullAt(1);
/* 035 */       int value = isNull ? -1 : (i.getInt(1));
/* 036 */       isNullB = isNull;
/* 037 */       primitiveB = value;
/* 038 */     }
/* 039 */     if (isNullA && isNullB) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */     return 0;
/* 053 */   }
/* 054 */ }

13270 [dispatcher-event-loop-3] DEBUG org.apache.spark.internal.Logging$class  - parentName: , name: TaskSet_2, runningTasks: 3
13270 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13271 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13271 [dispatcher-event-loop-3] INFO  org.apache.spark.internal.Logging$class  - Starting task 190.0 in stage 2.0 (TID 192, localhost, partition 190, ANY, 5536 bytes)
13271 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[1, int, true] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull1 = i.isNullAt(1);
/* 030 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 031 */     long value = -9223372036854775808L;
/* 032 */     boolean isNull = false;
/* 033 */     if (!isNull1) {
/* 034 */       value = (long) value1;
/* 035 */     }
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */     return result;
/* 042 */   }
/* 043 */ }

13271 [task-result-getter-0] INFO  org.apache.spark.internal.Logging$class  - Finished task 186.0 in stage 2.0 (TID 188) in 98 ms on localhost (187/200)
13271 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13272 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 2)
13272 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13272 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13272 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13272 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 2)
13272 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 2)
13272 [Executor task launch worker-0] INFO  org.apache.spark.internal.Logging$class  - Running task 190.0 in stage 2.0 (TID 192)
13272 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - ShuffleMapTask finished on driver
13272 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13272 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 191 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@3928b47f
13273 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (65536 bytes)
13273 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 1, partitions 189-190
13273 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
13273 [Executor task launch worker-4] INFO  org.apache.spark.internal.Logging$class  - Getting 1 non-empty blocks out of 1 blocks
13273 [Executor task launch worker-4] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 0 ms
13273 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 2)
13274 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13274 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13274 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13274 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 2)
13274 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 2)
13273 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  0 ms
13273 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Task 192's epoch is 2
13274 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Getting local block broadcast_2
13274 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Task 192 trying to acquire read lock for broadcast_2
13275 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Task 192 acquired read lock for broadcast_2
13275 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Level for block broadcast_2 is StorageLevel(disk, memory, deserialized, 1 replicas)
13275 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 5 (16777216 bytes)
13275 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 189 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@453e173f
13275 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 1 (65536 bytes)
13275 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 189 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@453e173f
13276 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Task 189 releasing lock for broadcast_2
13276 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Finished task 187.0 in stage 2.0 (TID 189). 4221 bytes result sent to driver
13276 [dispatcher-event-loop-1] DEBUG org.apache.spark.internal.Logging$class  - parentName: , name: TaskSet_2, runningTasks: 3
13277 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 2 (262144 bytes)
13277 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 190 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@2bba50a0
13277 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (16777216 bytes)
13274 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13277 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13277 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[0, int, false] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       int value = i.getInt(0);
/* 025 */       isNullA = false;
/* 026 */       primitiveA = value;
/* 027 */     }
/* 028 */     i = b;
/* 029 */     boolean isNullB;
/* 030 */     int primitiveB;
/* 031 */     {
/* 032 */
/* 033 */       int value = i.getInt(0);
/* 034 */       isNullB = false;
/* 035 */       primitiveB = value;
/* 036 */     }
/* 037 */     if (isNullA && isNullB) {
/* 038 */       // Nothing
/* 039 */     } else if (isNullA) {
/* 040 */       return -1;
/* 041 */     } else if (isNullB) {
/* 042 */       return 1;
/* 043 */     } else {
/* 044 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 045 */       if (comp != 0) {
/* 046 */         return comp;
/* 047 */       }
/* 048 */     }
/* 049 */
/* 050 */     return 0;
/* 051 */   }
/* 052 */ }

13278 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13278 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13277 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 190 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@2bba50a0
13277 [dispatcher-event-loop-1] INFO  org.apache.spark.internal.Logging$class  - Starting task 191.0 in stage 2.0 (TID 193, localhost, partition 191, ANY, 5536 bytes)
13279 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Running task 191.0 in stage 2.0 (TID 193)
13278 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[0, int, false] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     int value1 = i.getInt(0);
/* 030 */     long value = -9223372036854775808L;
/* 031 */     boolean isNull = false;
/* 032 */     if (!false) {
/* 033 */       value = (long) value1;
/* 034 */     }
/* 035 */     if (isNull) {
/* 036 */       rowWriter.setNullAt(0);
/* 037 */     } else {
/* 038 */       rowWriter.write(0, value);
/* 039 */     }
/* 040 */     return result;
/* 041 */   }
/* 042 */ }

13278 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 0, partitions 190-191
13279 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 191 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@7e61f615
13279 [task-result-getter-1] INFO  org.apache.spark.internal.Logging$class  - Finished task 187.0 in stage 2.0 (TID 189) in 94 ms on localhost (188/200)
13279 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13280 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
13280 [Executor task launch worker-0] INFO  org.apache.spark.internal.Logging$class  - Getting 1 non-empty blocks out of 1 blocks
13279 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 1 (65536 bytes)
13279 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Task 193's epoch is 2
13280 [Executor task launch worker-0] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 1 ms
13280 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 2)
13280 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13281 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13281 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13280 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  1 ms
13281 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13281 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13280 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13281 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13282 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[1, int, true] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       boolean isNull = i.isNullAt(1);
/* 025 */       int value = isNull ? -1 : (i.getInt(1));
/* 026 */       isNullA = isNull;
/* 027 */       primitiveA = value;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB;
/* 031 */     int primitiveB;
/* 032 */     {
/* 033 */
/* 034 */       boolean isNull = i.isNullAt(1);
/* 035 */       int value = isNull ? -1 : (i.getInt(1));
/* 036 */       isNullB = isNull;
/* 037 */       primitiveB = value;
/* 038 */     }
/* 039 */     if (isNullA && isNullB) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */     return 0;
/* 053 */   }
/* 054 */ }

13280 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Getting local block broadcast_2
13282 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Task 193 trying to acquire read lock for broadcast_2
13282 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Task 193 acquired read lock for broadcast_2
13282 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Level for block broadcast_2 is StorageLevel(disk, memory, deserialized, 1 replicas)
13282 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13282 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13281 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 2)
13282 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for createexternalrow(input[0, int, true], input[1, int, true], input[2, string, true].toString, StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private MutableRow mutableRow;
/* 009 */   private Object[] values;
/* 010 */   private org.apache.spark.sql.types.StructType schema;
/* 011 */
/* 012 */
/* 013 */   public SpecificSafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     mutableRow = (MutableRow) references[references.length - 1];
/* 016 */
/* 017 */     this.schema = (org.apache.spark.sql.types.StructType) references[0];
/* 018 */   }
/* 019 */
/* 020 */   public java.lang.Object apply(java.lang.Object _i) {
/* 021 */     InternalRow i = (InternalRow) _i;
/* 022 */
/* 023 */     values = new Object[3];
/* 024 */
/* 025 */     boolean isNull1 = i.isNullAt(0);
/* 026 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 027 */     if (isNull1) {
/* 028 */       values[0] = null;
/* 029 */     } else {
/* 030 */       values[0] = value1;
/* 031 */     }
/* 032 */
/* 033 */     boolean isNull2 = i.isNullAt(1);
/* 034 */     int value2 = isNull2 ? -1 : (i.getInt(1));
/* 035 */     if (isNull2) {
/* 036 */       values[1] = null;
/* 037 */     } else {
/* 038 */       values[1] = value2;
/* 039 */     }
/* 040 */
/* 041 */     boolean isNull4 = i.isNullAt(2);
/* 042 */     UTF8String value4 = isNull4 ? null : (i.getUTF8String(2));
/* 043 */
/* 044 */     boolean isNull3 = isNull4;
/* 045 */     final java.lang.String value3 = isNull3 ? null : (java.lang.String) value4.toString();
/* 046 */     isNull3 = value3 == null;
/* 047 */     if (isNull3) {
/* 048 */       values[2] = null;
/* 049 */     } else {
/* 050 */       values[2] = value3;
/* 051 */     }
/* 052 */
/* 053 */     final org.apache.spark.sql.Row value = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values, schema);
/* 054 */     if (false) {
/* 055 */       mutableRow.setNullAt(0);
/* 056 */     } else {
/* 057 */
/* 058 */       mutableRow.update(0, value);
/* 059 */     }
/* 060 */
/* 061 */     return mutableRow;
/* 062 */   }
/* 063 */ }

13283 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 2)
13283 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - ShuffleMapTask finished on driver
13283 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13284 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 2)
13284 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13284 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13284 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[1, int, true] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull1 = i.isNullAt(1);
/* 030 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 031 */     long value = -9223372036854775808L;
/* 032 */     boolean isNull = false;
/* 033 */     if (!isNull1) {
/* 034 */       value = (long) value1;
/* 035 */     }
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */     return result;
/* 042 */   }
/* 043 */ }

13284 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - SpecificUnsafeRowJoiner(StructType(StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)), StructType(StructField(value,IntegerType,true))):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeRowJoiner();
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeRowJoiner extends org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowJoiner {
/* 006 */   private byte[] buf = new byte[64];
/* 007 */   private UnsafeRow out = new UnsafeRow(4);
/* 008 */
/* 009 */   public UnsafeRow join(UnsafeRow row1, UnsafeRow row2) {
/* 010 */     // row1: 3 fields, 1 words in bitset
/* 011 */     // row2: 1, 1 words in bitset
/* 012 */     // output: 4 fields, 1 words in bitset
/* 013 */     final int sizeInBytes = row1.getSizeInBytes() + row2.getSizeInBytes() - 8;
/* 014 */     if (sizeInBytes > buf.length) {
/* 015 */       buf = new byte[sizeInBytes];
/* 016 */     }
/* 017 */
/* 018 */     final java.lang.Object obj1 = row1.getBaseObject();
/* 019 */     final long offset1 = row1.getBaseOffset();
/* 020 */     final java.lang.Object obj2 = row2.getBaseObject();
/* 021 */     final long offset2 = row2.getBaseOffset();
/* 022 */
/* 023 */     Platform.putLong(buf, 16, Platform.getLong(obj1, offset1 + 0) | (Platform.getLong(obj2, offset2) << 3));
/* 024 */
/* 025 */     // Copy fixed length data for row1
/* 026 */     Platform.copyMemory(
/* 027 */       obj1, offset1 + 8,
/* 028 */       buf, 24,
/* 029 */       24);
/* 030 */
/* 031 */
/* 032 */     // Copy fixed length data for row2
/* 033 */     Platform.copyMemory(
/* 034 */       obj2, offset2 + 8,
/* 035 */       buf, 48,
/* 036 */       8);
/* 037 */
/* 038 */
/* 039 */     // Copy variable length data for row1
/* 040 */     long numBytesVariableRow1 = row1.getSizeInBytes() - 32;
/* 041 */     Platform.copyMemory(
/* 042 */       obj1, offset1 + 32,
/* 043 */       buf, 56,
/* 044 */       numBytesVariableRow1);
/* 045 */
/* 046 */
/* 047 */     // Copy variable length data for row2
/* 048 */     long numBytesVariableRow2 = row2.getSizeInBytes() - 16;
/* 049 */     Platform.copyMemory(
/* 050 */       obj2, offset2 + 16,
/* 051 */       buf, 56 + numBytesVariableRow1,
/* 052 */       numBytesVariableRow2);
/* 053 */
/* 054 */
/* 055 */
/* 056 */
/* 057 */     Platform.putLong(buf, 40, Platform.getLong(buf, 40) + (8L << 32));
/* 058 */
/* 059 */
/* 060 */
/* 061 */     out.pointTo(buf, sizeInBytes);
/* 062 */
/* 063 */     return out;
/* 064 */   }
/* 065 */ }

13284 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.expressions.codegen.package$ExpressionCanonicalizer$CleanExpressions ===
!input[0, int, true] AS value#47   input[0, int, true]
!+- input[0, int, true]            
                
13284 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 2 iterations.
13284 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - 
=== Result of Batch CleanExpressions ===
!input[0, int, true] AS value#47   input[0, int, true]
!+- input[0, int, true]            
        
13284 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 192 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@45c85738
13285 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (65536 bytes)
13286 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull = i.isNullAt(0);
/* 030 */     int value = isNull ? -1 : (i.getInt(0));
/* 031 */     if (isNull) {
/* 032 */       rowWriter.setNullAt(0);
/* 033 */     } else {
/* 034 */       rowWriter.write(0, value);
/* 035 */     }
/* 036 */     return result;
/* 037 */   }
/* 038 */ }

13286 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13286 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13286 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for pmod(hash(input[3, int, true], 42), 200):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     boolean isNull = false;
/* 027 */
/* 028 */     int value1 = 42;
/* 029 */
/* 030 */     boolean isNull2 = i.isNullAt(3);
/* 031 */     int value2 = isNull2 ? -1 : (i.getInt(3));
/* 032 */     if (!isNull2) {
/* 033 */       value1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value2, value1);
/* 034 */     }
/* 035 */
/* 036 */     int value = -1;
/* 037 */
/* 038 */     int remainder = value1 % 200;
/* 039 */     if (remainder < 0) {
/* 040 */       value = (remainder + 200) % 200;
/* 041 */     } else {
/* 042 */       value = remainder;
/* 043 */     }
/* 044 */     rowWriter.write(0, value);
/* 045 */     return result;
/* 046 */   }
/* 047 */ }

13287 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13287 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13287 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13287 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13287 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13288 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13288 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13284 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13288 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 2)
13288 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 2)
13284 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 0, partitions 191-192
13288 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

13289 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 191 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@5f1341c0
13289 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 2 (262144 bytes)
13286 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 1, partitions 190-191
13289 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
13289 [Executor task launch worker-0] INFO  org.apache.spark.internal.Logging$class  - Getting 1 non-empty blocks out of 1 blocks
13289 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13290 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13290 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13290 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13290 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13291 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13291 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13291 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13289 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
13291 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Getting 1 non-empty blocks out of 1 blocks
13291 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 3 ms
13292 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  4 ms
13290 [Executor task launch worker-0] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 1 ms
13292 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  3 ms
13292 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13292 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13293 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[1, int, true] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       boolean isNull = i.isNullAt(1);
/* 025 */       int value = isNull ? -1 : (i.getInt(1));
/* 026 */       isNullA = isNull;
/* 027 */       primitiveA = value;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB;
/* 031 */     int primitiveB;
/* 032 */     {
/* 033 */
/* 034 */       boolean isNull = i.isNullAt(1);
/* 035 */       int value = isNull ? -1 : (i.getInt(1));
/* 036 */       isNullB = isNull;
/* 037 */       primitiveB = value;
/* 038 */     }
/* 039 */     if (isNullA && isNullB) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */     return 0;
/* 053 */   }
/* 054 */ }

13293 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13293 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13291 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

13294 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 191 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@427a1582
13294 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 3 (262144 bytes)
13294 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13295 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 191 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@3928b47f
13294 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[1, int, true] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull1 = i.isNullAt(1);
/* 030 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 031 */     long value = -9223372036854775808L;
/* 032 */     boolean isNull = false;
/* 033 */     if (!isNull1) {
/* 034 */       value = (long) value1;
/* 035 */     }
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */     return result;
/* 042 */   }
/* 043 */ }

13292 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13295 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13296 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[0, int, false] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       int value = i.getInt(0);
/* 025 */       isNullA = false;
/* 026 */       primitiveA = value;
/* 027 */     }
/* 028 */     i = b;
/* 029 */     boolean isNullB;
/* 030 */     int primitiveB;
/* 031 */     {
/* 032 */
/* 033 */       int value = i.getInt(0);
/* 034 */       isNullB = false;
/* 035 */       primitiveB = value;
/* 036 */     }
/* 037 */     if (isNullA && isNullB) {
/* 038 */       // Nothing
/* 039 */     } else if (isNullA) {
/* 040 */       return -1;
/* 041 */     } else if (isNullB) {
/* 042 */       return 1;
/* 043 */     } else {
/* 044 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 045 */       if (comp != 0) {
/* 046 */         return comp;
/* 047 */       }
/* 048 */     }
/* 049 */
/* 050 */     return 0;
/* 051 */   }
/* 052 */ }

13296 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13296 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13297 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[0, int, false] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     int value1 = i.getInt(0);
/* 030 */     long value = -9223372036854775808L;
/* 031 */     boolean isNull = false;
/* 032 */     if (!false) {
/* 033 */       value = (long) value1;
/* 034 */     }
/* 035 */     if (isNull) {
/* 036 */       rowWriter.setNullAt(0);
/* 037 */     } else {
/* 038 */       rowWriter.write(0, value);
/* 039 */     }
/* 040 */     return result;
/* 041 */   }
/* 042 */ }

13297 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 192 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@78d85c36
13297 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 1 (65536 bytes)
13295 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 193 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2263e820
13297 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13298 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13298 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for createexternalrow(input[0, int, true], input[1, int, true], input[2, string, true].toString, StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private MutableRow mutableRow;
/* 009 */   private Object[] values;
/* 010 */   private org.apache.spark.sql.types.StructType schema;
/* 011 */
/* 012 */
/* 013 */   public SpecificSafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     mutableRow = (MutableRow) references[references.length - 1];
/* 016 */
/* 017 */     this.schema = (org.apache.spark.sql.types.StructType) references[0];
/* 018 */   }
/* 019 */
/* 020 */   public java.lang.Object apply(java.lang.Object _i) {
/* 021 */     InternalRow i = (InternalRow) _i;
/* 022 */
/* 023 */     values = new Object[3];
/* 024 */
/* 025 */     boolean isNull1 = i.isNullAt(0);
/* 026 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 027 */     if (isNull1) {
/* 028 */       values[0] = null;
/* 029 */     } else {
/* 030 */       values[0] = value1;
/* 031 */     }
/* 032 */
/* 033 */     boolean isNull2 = i.isNullAt(1);
/* 034 */     int value2 = isNull2 ? -1 : (i.getInt(1));
/* 035 */     if (isNull2) {
/* 036 */       values[1] = null;
/* 037 */     } else {
/* 038 */       values[1] = value2;
/* 039 */     }
/* 040 */
/* 041 */     boolean isNull4 = i.isNullAt(2);
/* 042 */     UTF8String value4 = isNull4 ? null : (i.getUTF8String(2));
/* 043 */
/* 044 */     boolean isNull3 = isNull4;
/* 045 */     final java.lang.String value3 = isNull3 ? null : (java.lang.String) value4.toString();
/* 046 */     isNull3 = value3 == null;
/* 047 */     if (isNull3) {
/* 048 */       values[2] = null;
/* 049 */     } else {
/* 050 */       values[2] = value3;
/* 051 */     }
/* 052 */
/* 053 */     final org.apache.spark.sql.Row value = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values, schema);
/* 054 */     if (false) {
/* 055 */       mutableRow.setNullAt(0);
/* 056 */     } else {
/* 057 */
/* 058 */       mutableRow.update(0, value);
/* 059 */     }
/* 060 */
/* 061 */     return mutableRow;
/* 062 */   }
/* 063 */ }

13299 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - SpecificUnsafeRowJoiner(StructType(StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)), StructType(StructField(value,IntegerType,true))):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeRowJoiner();
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeRowJoiner extends org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowJoiner {
/* 006 */   private byte[] buf = new byte[64];
/* 007 */   private UnsafeRow out = new UnsafeRow(4);
/* 008 */
/* 009 */   public UnsafeRow join(UnsafeRow row1, UnsafeRow row2) {
/* 010 */     // row1: 3 fields, 1 words in bitset
/* 011 */     // row2: 1, 1 words in bitset
/* 012 */     // output: 4 fields, 1 words in bitset
/* 013 */     final int sizeInBytes = row1.getSizeInBytes() + row2.getSizeInBytes() - 8;
/* 014 */     if (sizeInBytes > buf.length) {
/* 015 */       buf = new byte[sizeInBytes];
/* 016 */     }
/* 017 */
/* 018 */     final java.lang.Object obj1 = row1.getBaseObject();
/* 019 */     final long offset1 = row1.getBaseOffset();
/* 020 */     final java.lang.Object obj2 = row2.getBaseObject();
/* 021 */     final long offset2 = row2.getBaseOffset();
/* 022 */
/* 023 */     Platform.putLong(buf, 16, Platform.getLong(obj1, offset1 + 0) | (Platform.getLong(obj2, offset2) << 3));
/* 024 */
/* 025 */     // Copy fixed length data for row1
/* 026 */     Platform.copyMemory(
/* 027 */       obj1, offset1 + 8,
/* 028 */       buf, 24,
/* 029 */       24);
/* 030 */
/* 031 */
/* 032 */     // Copy fixed length data for row2
/* 033 */     Platform.copyMemory(
/* 034 */       obj2, offset2 + 8,
/* 035 */       buf, 48,
/* 036 */       8);
/* 037 */
/* 038 */
/* 039 */     // Copy variable length data for row1
/* 040 */     long numBytesVariableRow1 = row1.getSizeInBytes() - 32;
/* 041 */     Platform.copyMemory(
/* 042 */       obj1, offset1 + 32,
/* 043 */       buf, 56,
/* 044 */       numBytesVariableRow1);
/* 045 */
/* 046 */
/* 047 */     // Copy variable length data for row2
/* 048 */     long numBytesVariableRow2 = row2.getSizeInBytes() - 16;
/* 049 */     Platform.copyMemory(
/* 050 */       obj2, offset2 + 16,
/* 051 */       buf, 56 + numBytesVariableRow1,
/* 052 */       numBytesVariableRow2);
/* 053 */
/* 054 */
/* 055 */
/* 056 */
/* 057 */     Platform.putLong(buf, 40, Platform.getLong(buf, 40) + (8L << 32));
/* 058 */
/* 059 */
/* 060 */
/* 061 */     out.pointTo(buf, sizeInBytes);
/* 062 */
/* 063 */     return out;
/* 064 */   }
/* 065 */ }

13299 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.expressions.codegen.package$ExpressionCanonicalizer$CleanExpressions ===
!input[0, int, true] AS value#47   input[0, int, true]
!+- input[0, int, true]            
                
13299 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 2 iterations.
13295 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 4 (16777216 bytes)
13299 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - 
=== Result of Batch CleanExpressions ===
!input[0, int, true] AS value#47   input[0, int, true]
!+- input[0, int, true]            
        
13300 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull = i.isNullAt(0);
/* 030 */     int value = isNull ? -1 : (i.getInt(0));
/* 031 */     if (isNull) {
/* 032 */       rowWriter.setNullAt(0);
/* 033 */     } else {
/* 034 */       rowWriter.write(0, value);
/* 035 */     }
/* 036 */     return result;
/* 037 */   }
/* 038 */ }

13301 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13301 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13301 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for pmod(hash(input[3, int, true], 42), 200):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     boolean isNull = false;
/* 027 */
/* 028 */     int value1 = 42;
/* 029 */
/* 030 */     boolean isNull2 = i.isNullAt(3);
/* 031 */     int value2 = isNull2 ? -1 : (i.getInt(3));
/* 032 */     if (!isNull2) {
/* 033 */       value1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value2, value1);
/* 034 */     }
/* 035 */
/* 036 */     int value = -1;
/* 037 */
/* 038 */     int remainder = value1 % 200;
/* 039 */     if (remainder < 0) {
/* 040 */       value = (remainder + 200) % 200;
/* 041 */     } else {
/* 042 */       value = remainder;
/* 043 */     }
/* 044 */     rowWriter.write(0, value);
/* 045 */     return result;
/* 046 */   }
/* 047 */ }

13302 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13302 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13302 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13302 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13302 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13303 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13303 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13297 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (65536 bytes)
13303 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 1, partitions 191-192
13303 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

13303 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 192 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@54e60f37
13304 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 2 (262144 bytes)
13304 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13304 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13305 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13305 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13305 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13305 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13305 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13305 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13306 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

13306 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 192 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@de17959
13306 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 3 (262144 bytes)
13307 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13307 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 192 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@45c85738
13307 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 4 (16777216 bytes)
13300 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 191 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@7e61f615
13307 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 5 (16777216 bytes)
13308 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 192 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@78d85c36
13308 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 5 (16777216 bytes)
13303 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
13308 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Getting 1 non-empty blocks out of 1 blocks
13308 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 5 ms
13308 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  5 ms
13308 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 191 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@427a1582
13309 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 6 (16777216 bytes)
13309 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 4 (16777216 bytes)
13309 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 191 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@3928b47f
13309 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (65536 bytes)
13309 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13310 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13310 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[0, int, false] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       int value = i.getInt(0);
/* 025 */       isNullA = false;
/* 026 */       primitiveA = value;
/* 027 */     }
/* 028 */     i = b;
/* 029 */     boolean isNullB;
/* 030 */     int primitiveB;
/* 031 */     {
/* 032 */
/* 033 */       int value = i.getInt(0);
/* 034 */       isNullB = false;
/* 035 */       primitiveB = value;
/* 036 */     }
/* 037 */     if (isNullA && isNullB) {
/* 038 */       // Nothing
/* 039 */     } else if (isNullA) {
/* 040 */       return -1;
/* 041 */     } else if (isNullB) {
/* 042 */       return 1;
/* 043 */     } else {
/* 044 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 045 */       if (comp != 0) {
/* 046 */         return comp;
/* 047 */       }
/* 048 */     }
/* 049 */
/* 050 */     return 0;
/* 051 */   }
/* 052 */ }

13310 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13310 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13311 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[0, int, false] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     int value1 = i.getInt(0);
/* 030 */     long value = -9223372036854775808L;
/* 031 */     boolean isNull = false;
/* 032 */     if (!false) {
/* 033 */       value = (long) value1;
/* 034 */     }
/* 035 */     if (isNull) {
/* 036 */       rowWriter.setNullAt(0);
/* 037 */     } else {
/* 038 */       rowWriter.write(0, value);
/* 039 */     }
/* 040 */     return result;
/* 041 */   }
/* 042 */ }

13311 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 193 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2567f86d
13311 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 1 (65536 bytes)
13312 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13312 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13312 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for createexternalrow(input[0, int, true], input[1, int, true], input[2, string, true].toString, StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private MutableRow mutableRow;
/* 009 */   private Object[] values;
/* 010 */   private org.apache.spark.sql.types.StructType schema;
/* 011 */
/* 012 */
/* 013 */   public SpecificSafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     mutableRow = (MutableRow) references[references.length - 1];
/* 016 */
/* 017 */     this.schema = (org.apache.spark.sql.types.StructType) references[0];
/* 018 */   }
/* 019 */
/* 020 */   public java.lang.Object apply(java.lang.Object _i) {
/* 021 */     InternalRow i = (InternalRow) _i;
/* 022 */
/* 023 */     values = new Object[3];
/* 024 */
/* 025 */     boolean isNull1 = i.isNullAt(0);
/* 026 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 027 */     if (isNull1) {
/* 028 */       values[0] = null;
/* 029 */     } else {
/* 030 */       values[0] = value1;
/* 031 */     }
/* 032 */
/* 033 */     boolean isNull2 = i.isNullAt(1);
/* 034 */     int value2 = isNull2 ? -1 : (i.getInt(1));
/* 035 */     if (isNull2) {
/* 036 */       values[1] = null;
/* 037 */     } else {
/* 038 */       values[1] = value2;
/* 039 */     }
/* 040 */
/* 041 */     boolean isNull4 = i.isNullAt(2);
/* 042 */     UTF8String value4 = isNull4 ? null : (i.getUTF8String(2));
/* 043 */
/* 044 */     boolean isNull3 = isNull4;
/* 045 */     final java.lang.String value3 = isNull3 ? null : (java.lang.String) value4.toString();
/* 046 */     isNull3 = value3 == null;
/* 047 */     if (isNull3) {
/* 048 */       values[2] = null;
/* 049 */     } else {
/* 050 */       values[2] = value3;
/* 051 */     }
/* 052 */
/* 053 */     final org.apache.spark.sql.Row value = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values, schema);
/* 054 */     if (false) {
/* 055 */       mutableRow.setNullAt(0);
/* 056 */     } else {
/* 057 */
/* 058 */       mutableRow.update(0, value);
/* 059 */     }
/* 060 */
/* 061 */     return mutableRow;
/* 062 */   }
/* 063 */ }

13308 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 192 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@de17959
13313 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 6 (16777216 bytes)
13313 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 4 (16777216 bytes)
13313 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 192 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@45c85738
13313 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (65536 bytes)
13314 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 192 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@45c85738
13314 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 192 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@54e60f37
13314 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (16777216 bytes)
13314 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 3 (262144 bytes)
13314 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 192 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@de17959
13314 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 6 (16777216 bytes)
13314 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 192 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@de17959
13309 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 191 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@3928b47f
13315 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 191 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@5f1341c0
13315 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (16777216 bytes)
13315 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 3 (262144 bytes)
13315 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 191 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@427a1582
13315 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 6 (16777216 bytes)
13315 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 191 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@427a1582
13309 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 5 (16777216 bytes)
13320 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 190 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@343b805
13321 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 1 (65536 bytes)
13321 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 190 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@343b805
13321 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Task 190 releasing lock for broadcast_2
13313 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - SpecificUnsafeRowJoiner(StructType(StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)), StructType(StructField(value,IntegerType,true))):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeRowJoiner();
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeRowJoiner extends org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowJoiner {
/* 006 */   private byte[] buf = new byte[64];
/* 007 */   private UnsafeRow out = new UnsafeRow(4);
/* 008 */
/* 009 */   public UnsafeRow join(UnsafeRow row1, UnsafeRow row2) {
/* 010 */     // row1: 3 fields, 1 words in bitset
/* 011 */     // row2: 1, 1 words in bitset
/* 012 */     // output: 4 fields, 1 words in bitset
/* 013 */     final int sizeInBytes = row1.getSizeInBytes() + row2.getSizeInBytes() - 8;
/* 014 */     if (sizeInBytes > buf.length) {
/* 015 */       buf = new byte[sizeInBytes];
/* 016 */     }
/* 017 */
/* 018 */     final java.lang.Object obj1 = row1.getBaseObject();
/* 019 */     final long offset1 = row1.getBaseOffset();
/* 020 */     final java.lang.Object obj2 = row2.getBaseObject();
/* 021 */     final long offset2 = row2.getBaseOffset();
/* 022 */
/* 023 */     Platform.putLong(buf, 16, Platform.getLong(obj1, offset1 + 0) | (Platform.getLong(obj2, offset2) << 3));
/* 024 */
/* 025 */     // Copy fixed length data for row1
/* 026 */     Platform.copyMemory(
/* 027 */       obj1, offset1 + 8,
/* 028 */       buf, 24,
/* 029 */       24);
/* 030 */
/* 031 */
/* 032 */     // Copy fixed length data for row2
/* 033 */     Platform.copyMemory(
/* 034 */       obj2, offset2 + 8,
/* 035 */       buf, 48,
/* 036 */       8);
/* 037 */
/* 038 */
/* 039 */     // Copy variable length data for row1
/* 040 */     long numBytesVariableRow1 = row1.getSizeInBytes() - 32;
/* 041 */     Platform.copyMemory(
/* 042 */       obj1, offset1 + 32,
/* 043 */       buf, 56,
/* 044 */       numBytesVariableRow1);
/* 045 */
/* 046 */
/* 047 */     // Copy variable length data for row2
/* 048 */     long numBytesVariableRow2 = row2.getSizeInBytes() - 16;
/* 049 */     Platform.copyMemory(
/* 050 */       obj2, offset2 + 16,
/* 051 */       buf, 56 + numBytesVariableRow1,
/* 052 */       numBytesVariableRow2);
/* 053 */
/* 054 */
/* 055 */
/* 056 */
/* 057 */     Platform.putLong(buf, 40, Platform.getLong(buf, 40) + (8L << 32));
/* 058 */
/* 059 */
/* 060 */
/* 061 */     out.pointTo(buf, sizeInBytes);
/* 062 */
/* 063 */     return out;
/* 064 */   }
/* 065 */ }

13321 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.expressions.codegen.package$ExpressionCanonicalizer$CleanExpressions ===
!input[0, int, true] AS value#47   input[0, int, true]
!+- input[0, int, true]            
                
13322 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 2 iterations.
13322 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - 
=== Result of Batch CleanExpressions ===
!input[0, int, true] AS value#47   input[0, int, true]
!+- input[0, int, true]            
        
13322 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull = i.isNullAt(0);
/* 030 */     int value = isNull ? -1 : (i.getInt(0));
/* 031 */     if (isNull) {
/* 032 */       rowWriter.setNullAt(0);
/* 033 */     } else {
/* 034 */       rowWriter.write(0, value);
/* 035 */     }
/* 036 */     return result;
/* 037 */   }
/* 038 */ }

13326 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13326 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13326 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for pmod(hash(input[3, int, true], 42), 200):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     boolean isNull = false;
/* 027 */
/* 028 */     int value1 = 42;
/* 029 */
/* 030 */     boolean isNull2 = i.isNullAt(3);
/* 031 */     int value2 = isNull2 ? -1 : (i.getInt(3));
/* 032 */     if (!isNull2) {
/* 033 */       value1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value2, value1);
/* 034 */     }
/* 035 */
/* 036 */     int value = -1;
/* 037 */
/* 038 */     int remainder = value1 % 200;
/* 039 */     if (remainder < 0) {
/* 040 */       value = (remainder + 200) % 200;
/* 041 */     } else {
/* 042 */       value = remainder;
/* 043 */     }
/* 044 */     rowWriter.write(0, value);
/* 045 */     return result;
/* 046 */   }
/* 047 */ }

13327 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13327 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13327 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13328 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13328 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13329 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13329 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13330 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

13331 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 2 (262144 bytes)
13333 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 191 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@5f1341c0
13332 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 2 (262144 bytes)
13333 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 192 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@54e60f37
13333 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (16777216 bytes)
13333 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 192 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@54e60f37
13333 [Executor task launch worker-3] INFO  org.apache.spark.internal.Logging$class  - Finished task 188.0 in stage 2.0 (TID 190). 4221 bytes result sent to driver
13333 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (16777216 bytes)
13333 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 191 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@5f1341c0
13336 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 193 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@3f81b85f
13336 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 2 (262144 bytes)
13337 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13337 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13337 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13337 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13338 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13338 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13338 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13338 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13339 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

13339 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 193 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@397847ab
13339 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 3 (262144 bytes)
13339 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13340 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 193 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2263e820
13340 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 4 (16777216 bytes)
13341 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 193 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2567f86d
13341 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 5 (16777216 bytes)
13341 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 193 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@397847ab
13341 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 6 (16777216 bytes)
13341 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 4 (16777216 bytes)
13342 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 193 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2263e820
13342 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (65536 bytes)
13342 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 193 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2263e820
13342 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 193 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@3f81b85f
13342 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (16777216 bytes)
13343 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 3 (262144 bytes)
13343 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 193 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@397847ab
13343 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 6 (16777216 bytes)
13343 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 193 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@397847ab
13345 [dispatcher-event-loop-5] DEBUG org.apache.spark.internal.Logging$class  - parentName: , name: TaskSet_2, runningTasks: 3
13345 [dispatcher-event-loop-5] INFO  org.apache.spark.internal.Logging$class  - Starting task 192.0 in stage 2.0 (TID 194, localhost, partition 192, ANY, 5536 bytes)
13346 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13346 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 2)
13346 [task-result-getter-2] INFO  org.apache.spark.internal.Logging$class  - Finished task 188.0 in stage 2.0 (TID 190) in 114 ms on localhost (189/200)
13346 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13346 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13346 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13346 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 2)
13346 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 2)
13346 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - ShuffleMapTask finished on driver
13347 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13347 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 2)
13347 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13347 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13347 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13347 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 2)
13347 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 2)
13348 [Executor task launch worker-3] INFO  org.apache.spark.internal.Logging$class  - Running task 192.0 in stage 2.0 (TID 194)
13354 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Task 194's epoch is 2
13354 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Getting local block broadcast_2
13355 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Task 194 trying to acquire read lock for broadcast_2
13355 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Task 194 acquired read lock for broadcast_2
13355 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Level for block broadcast_2 is StorageLevel(disk, memory, deserialized, 1 replicas)
13355 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 2 (262144 bytes)
13356 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 193 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@3f81b85f
13356 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (16777216 bytes)
13356 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 193 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@3f81b85f
13357 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 0, partitions 192-193
13358 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 5 (16777216 bytes)
13358 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 192 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@78d85c36
13358 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 1 (65536 bytes)
13358 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 192 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@78d85c36
13358 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Task 192 releasing lock for broadcast_2
13358 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
13359 [Executor task launch worker-3] INFO  org.apache.spark.internal.Logging$class  - Getting 1 non-empty blocks out of 1 blocks
13359 [Executor task launch worker-3] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 1 ms
13359 [Executor task launch worker-0] INFO  org.apache.spark.internal.Logging$class  - Finished task 190.0 in stage 2.0 (TID 192). 4221 bytes result sent to driver
13359 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  1 ms
13359 [dispatcher-event-loop-2] DEBUG org.apache.spark.internal.Logging$class  - parentName: , name: TaskSet_2, runningTasks: 3
13359 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13360 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13360 [dispatcher-event-loop-2] INFO  org.apache.spark.internal.Logging$class  - Starting task 193.0 in stage 2.0 (TID 195, localhost, partition 193, ANY, 5536 bytes)
13360 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13360 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 2)
13360 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13360 [Executor task launch worker-0] INFO  org.apache.spark.internal.Logging$class  - Running task 193.0 in stage 2.0 (TID 195)
13360 [task-result-getter-3] INFO  org.apache.spark.internal.Logging$class  - Finished task 190.0 in stage 2.0 (TID 192) in 90 ms on localhost (190/200)
13360 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[1, int, true] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       boolean isNull = i.isNullAt(1);
/* 025 */       int value = isNull ? -1 : (i.getInt(1));
/* 026 */       isNullA = isNull;
/* 027 */       primitiveA = value;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB;
/* 031 */     int primitiveB;
/* 032 */     {
/* 033 */
/* 034 */       boolean isNull = i.isNullAt(1);
/* 035 */       int value = isNull ? -1 : (i.getInt(1));
/* 036 */       isNullB = isNull;
/* 037 */       primitiveB = value;
/* 038 */     }
/* 039 */     if (isNullA && isNullB) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */     return 0;
/* 053 */   }
/* 054 */ }

13360 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13361 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13361 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 2)
13361 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 2)
13361 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13361 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13361 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - ShuffleMapTask finished on driver
13362 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13362 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 2)
13362 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13362 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13362 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13361 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Task 195's epoch is 2
13362 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 2)
13362 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 2)
13362 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[1, int, true] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull1 = i.isNullAt(1);
/* 030 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 031 */     long value = -9223372036854775808L;
/* 032 */     boolean isNull = false;
/* 033 */     if (!isNull1) {
/* 034 */       value = (long) value1;
/* 035 */     }
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */     return result;
/* 042 */   }
/* 043 */ }

13362 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Getting local block broadcast_2
13363 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Task 195 trying to acquire read lock for broadcast_2
13363 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Task 195 acquired read lock for broadcast_2
13363 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Level for block broadcast_2 is StorageLevel(disk, memory, deserialized, 1 replicas)
13362 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 194 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@5ce21ff7
13363 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (65536 bytes)
13363 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 1, partitions 192-193
13363 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
13364 [Executor task launch worker-3] INFO  org.apache.spark.internal.Logging$class  - Getting 1 non-empty blocks out of 1 blocks
13364 [Executor task launch worker-3] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 1 ms
13364 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  1 ms
13364 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13364 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13365 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 0, partitions 193-194
13365 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[0, int, false] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       int value = i.getInt(0);
/* 025 */       isNullA = false;
/* 026 */       primitiveA = value;
/* 027 */     }
/* 028 */     i = b;
/* 029 */     boolean isNullB;
/* 030 */     int primitiveB;
/* 031 */     {
/* 032 */
/* 033 */       int value = i.getInt(0);
/* 034 */       isNullB = false;
/* 035 */       primitiveB = value;
/* 036 */     }
/* 037 */     if (isNullA && isNullB) {
/* 038 */       // Nothing
/* 039 */     } else if (isNullA) {
/* 040 */       return -1;
/* 041 */     } else if (isNullB) {
/* 042 */       return 1;
/* 043 */     } else {
/* 044 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 045 */       if (comp != 0) {
/* 046 */         return comp;
/* 047 */       }
/* 048 */     }
/* 049 */
/* 050 */     return 0;
/* 051 */   }
/* 052 */ }

13365 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
13365 [Executor task launch worker-0] INFO  org.apache.spark.internal.Logging$class  - Getting 1 non-empty blocks out of 1 blocks
13365 [Executor task launch worker-0] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 0 ms
13366 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  1 ms
13366 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13366 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13365 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13366 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13366 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[1, int, true] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       boolean isNull = i.isNullAt(1);
/* 025 */       int value = isNull ? -1 : (i.getInt(1));
/* 026 */       isNullA = isNull;
/* 027 */       primitiveA = value;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB;
/* 031 */     int primitiveB;
/* 032 */     {
/* 033 */
/* 034 */       boolean isNull = i.isNullAt(1);
/* 035 */       int value = isNull ? -1 : (i.getInt(1));
/* 036 */       isNullB = isNull;
/* 037 */       primitiveB = value;
/* 038 */     }
/* 039 */     if (isNullA && isNullB) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */     return 0;
/* 053 */   }
/* 054 */ }

13367 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13367 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[0, int, false] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     int value1 = i.getInt(0);
/* 030 */     long value = -9223372036854775808L;
/* 031 */     boolean isNull = false;
/* 032 */     if (!false) {
/* 033 */       value = (long) value1;
/* 034 */     }
/* 035 */     if (isNull) {
/* 036 */       rowWriter.setNullAt(0);
/* 037 */     } else {
/* 038 */       rowWriter.write(0, value);
/* 039 */     }
/* 040 */     return result;
/* 041 */   }
/* 042 */ }

13367 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13367 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 194 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@738b718e
13367 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 1 (65536 bytes)
13368 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[1, int, true] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull1 = i.isNullAt(1);
/* 030 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 031 */     long value = -9223372036854775808L;
/* 032 */     boolean isNull = false;
/* 033 */     if (!isNull1) {
/* 034 */       value = (long) value1;
/* 035 */     }
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */     return result;
/* 042 */   }
/* 043 */ }

13368 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 195 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@27587ac7
13368 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (65536 bytes)
13368 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 1, partitions 193-194
13368 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13368 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13368 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
13369 [Executor task launch worker-0] INFO  org.apache.spark.internal.Logging$class  - Getting 1 non-empty blocks out of 1 blocks
13369 [Executor task launch worker-0] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 1 ms
13369 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for createexternalrow(input[0, int, true], input[1, int, true], input[2, string, true].toString, StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private MutableRow mutableRow;
/* 009 */   private Object[] values;
/* 010 */   private org.apache.spark.sql.types.StructType schema;
/* 011 */
/* 012 */
/* 013 */   public SpecificSafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     mutableRow = (MutableRow) references[references.length - 1];
/* 016 */
/* 017 */     this.schema = (org.apache.spark.sql.types.StructType) references[0];
/* 018 */   }
/* 019 */
/* 020 */   public java.lang.Object apply(java.lang.Object _i) {
/* 021 */     InternalRow i = (InternalRow) _i;
/* 022 */
/* 023 */     values = new Object[3];
/* 024 */
/* 025 */     boolean isNull1 = i.isNullAt(0);
/* 026 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 027 */     if (isNull1) {
/* 028 */       values[0] = null;
/* 029 */     } else {
/* 030 */       values[0] = value1;
/* 031 */     }
/* 032 */
/* 033 */     boolean isNull2 = i.isNullAt(1);
/* 034 */     int value2 = isNull2 ? -1 : (i.getInt(1));
/* 035 */     if (isNull2) {
/* 036 */       values[1] = null;
/* 037 */     } else {
/* 038 */       values[1] = value2;
/* 039 */     }
/* 040 */
/* 041 */     boolean isNull4 = i.isNullAt(2);
/* 042 */     UTF8String value4 = isNull4 ? null : (i.getUTF8String(2));
/* 043 */
/* 044 */     boolean isNull3 = isNull4;
/* 045 */     final java.lang.String value3 = isNull3 ? null : (java.lang.String) value4.toString();
/* 046 */     isNull3 = value3 == null;
/* 047 */     if (isNull3) {
/* 048 */       values[2] = null;
/* 049 */     } else {
/* 050 */       values[2] = value3;
/* 051 */     }
/* 052 */
/* 053 */     final org.apache.spark.sql.Row value = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values, schema);
/* 054 */     if (false) {
/* 055 */       mutableRow.setNullAt(0);
/* 056 */     } else {
/* 057 */
/* 058 */       mutableRow.update(0, value);
/* 059 */     }
/* 060 */
/* 061 */     return mutableRow;
/* 062 */   }
/* 063 */ }

13369 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  1 ms
13369 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13370 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13370 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - SpecificUnsafeRowJoiner(StructType(StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)), StructType(StructField(value,IntegerType,true))):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeRowJoiner();
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeRowJoiner extends org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowJoiner {
/* 006 */   private byte[] buf = new byte[64];
/* 007 */   private UnsafeRow out = new UnsafeRow(4);
/* 008 */
/* 009 */   public UnsafeRow join(UnsafeRow row1, UnsafeRow row2) {
/* 010 */     // row1: 3 fields, 1 words in bitset
/* 011 */     // row2: 1, 1 words in bitset
/* 012 */     // output: 4 fields, 1 words in bitset
/* 013 */     final int sizeInBytes = row1.getSizeInBytes() + row2.getSizeInBytes() - 8;
/* 014 */     if (sizeInBytes > buf.length) {
/* 015 */       buf = new byte[sizeInBytes];
/* 016 */     }
/* 017 */
/* 018 */     final java.lang.Object obj1 = row1.getBaseObject();
/* 019 */     final long offset1 = row1.getBaseOffset();
/* 020 */     final java.lang.Object obj2 = row2.getBaseObject();
/* 021 */     final long offset2 = row2.getBaseOffset();
/* 022 */
/* 023 */     Platform.putLong(buf, 16, Platform.getLong(obj1, offset1 + 0) | (Platform.getLong(obj2, offset2) << 3));
/* 024 */
/* 025 */     // Copy fixed length data for row1
/* 026 */     Platform.copyMemory(
/* 027 */       obj1, offset1 + 8,
/* 028 */       buf, 24,
/* 029 */       24);
/* 030 */
/* 031 */
/* 032 */     // Copy fixed length data for row2
/* 033 */     Platform.copyMemory(
/* 034 */       obj2, offset2 + 8,
/* 035 */       buf, 48,
/* 036 */       8);
/* 037 */
/* 038 */
/* 039 */     // Copy variable length data for row1
/* 040 */     long numBytesVariableRow1 = row1.getSizeInBytes() - 32;
/* 041 */     Platform.copyMemory(
/* 042 */       obj1, offset1 + 32,
/* 043 */       buf, 56,
/* 044 */       numBytesVariableRow1);
/* 045 */
/* 046 */
/* 047 */     // Copy variable length data for row2
/* 048 */     long numBytesVariableRow2 = row2.getSizeInBytes() - 16;
/* 049 */     Platform.copyMemory(
/* 050 */       obj2, offset2 + 16,
/* 051 */       buf, 56 + numBytesVariableRow1,
/* 052 */       numBytesVariableRow2);
/* 053 */
/* 054 */
/* 055 */
/* 056 */
/* 057 */     Platform.putLong(buf, 40, Platform.getLong(buf, 40) + (8L << 32));
/* 058 */
/* 059 */
/* 060 */
/* 061 */     out.pointTo(buf, sizeInBytes);
/* 062 */
/* 063 */     return out;
/* 064 */   }
/* 065 */ }

13370 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.expressions.codegen.package$ExpressionCanonicalizer$CleanExpressions ===
!input[0, int, true] AS value#47   input[0, int, true]
!+- input[0, int, true]            
                
13370 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 2 iterations.
13370 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[0, int, false] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       int value = i.getInt(0);
/* 025 */       isNullA = false;
/* 026 */       primitiveA = value;
/* 027 */     }
/* 028 */     i = b;
/* 029 */     boolean isNullB;
/* 030 */     int primitiveB;
/* 031 */     {
/* 032 */
/* 033 */       int value = i.getInt(0);
/* 034 */       isNullB = false;
/* 035 */       primitiveB = value;
/* 036 */     }
/* 037 */     if (isNullA && isNullB) {
/* 038 */       // Nothing
/* 039 */     } else if (isNullA) {
/* 040 */       return -1;
/* 041 */     } else if (isNullB) {
/* 042 */       return 1;
/* 043 */     } else {
/* 044 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 045 */       if (comp != 0) {
/* 046 */         return comp;
/* 047 */       }
/* 048 */     }
/* 049 */
/* 050 */     return 0;
/* 051 */   }
/* 052 */ }

13371 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - 
=== Result of Batch CleanExpressions ===
!input[0, int, true] AS value#47   input[0, int, true]
!+- input[0, int, true]            
        
13371 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13372 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13372 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull = i.isNullAt(0);
/* 030 */     int value = isNull ? -1 : (i.getInt(0));
/* 031 */     if (isNull) {
/* 032 */       rowWriter.setNullAt(0);
/* 033 */     } else {
/* 034 */       rowWriter.write(0, value);
/* 035 */     }
/* 036 */     return result;
/* 037 */   }
/* 038 */ }

13372 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[0, int, false] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     int value1 = i.getInt(0);
/* 030 */     long value = -9223372036854775808L;
/* 031 */     boolean isNull = false;
/* 032 */     if (!false) {
/* 033 */       value = (long) value1;
/* 034 */     }
/* 035 */     if (isNull) {
/* 036 */       rowWriter.setNullAt(0);
/* 037 */     } else {
/* 038 */       rowWriter.write(0, value);
/* 039 */     }
/* 040 */     return result;
/* 041 */   }
/* 042 */ }

13373 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 195 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@14336e0c
13373 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 1 (65536 bytes)
13373 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13373 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13373 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13374 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13374 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for pmod(hash(input[3, int, true], 42), 200):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     boolean isNull = false;
/* 027 */
/* 028 */     int value1 = 42;
/* 029 */
/* 030 */     boolean isNull2 = i.isNullAt(3);
/* 031 */     int value2 = isNull2 ? -1 : (i.getInt(3));
/* 032 */     if (!isNull2) {
/* 033 */       value1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value2, value1);
/* 034 */     }
/* 035 */
/* 036 */     int value = -1;
/* 037 */
/* 038 */     int remainder = value1 % 200;
/* 039 */     if (remainder < 0) {
/* 040 */       value = (remainder + 200) % 200;
/* 041 */     } else {
/* 042 */       value = remainder;
/* 043 */     }
/* 044 */     rowWriter.write(0, value);
/* 045 */     return result;
/* 046 */   }
/* 047 */ }

13375 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for createexternalrow(input[0, int, true], input[1, int, true], input[2, string, true].toString, StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private MutableRow mutableRow;
/* 009 */   private Object[] values;
/* 010 */   private org.apache.spark.sql.types.StructType schema;
/* 011 */
/* 012 */
/* 013 */   public SpecificSafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     mutableRow = (MutableRow) references[references.length - 1];
/* 016 */
/* 017 */     this.schema = (org.apache.spark.sql.types.StructType) references[0];
/* 018 */   }
/* 019 */
/* 020 */   public java.lang.Object apply(java.lang.Object _i) {
/* 021 */     InternalRow i = (InternalRow) _i;
/* 022 */
/* 023 */     values = new Object[3];
/* 024 */
/* 025 */     boolean isNull1 = i.isNullAt(0);
/* 026 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 027 */     if (isNull1) {
/* 028 */       values[0] = null;
/* 029 */     } else {
/* 030 */       values[0] = value1;
/* 031 */     }
/* 032 */
/* 033 */     boolean isNull2 = i.isNullAt(1);
/* 034 */     int value2 = isNull2 ? -1 : (i.getInt(1));
/* 035 */     if (isNull2) {
/* 036 */       values[1] = null;
/* 037 */     } else {
/* 038 */       values[1] = value2;
/* 039 */     }
/* 040 */
/* 041 */     boolean isNull4 = i.isNullAt(2);
/* 042 */     UTF8String value4 = isNull4 ? null : (i.getUTF8String(2));
/* 043 */
/* 044 */     boolean isNull3 = isNull4;
/* 045 */     final java.lang.String value3 = isNull3 ? null : (java.lang.String) value4.toString();
/* 046 */     isNull3 = value3 == null;
/* 047 */     if (isNull3) {
/* 048 */       values[2] = null;
/* 049 */     } else {
/* 050 */       values[2] = value3;
/* 051 */     }
/* 052 */
/* 053 */     final org.apache.spark.sql.Row value = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values, schema);
/* 054 */     if (false) {
/* 055 */       mutableRow.setNullAt(0);
/* 056 */     } else {
/* 057 */
/* 058 */       mutableRow.update(0, value);
/* 059 */     }
/* 060 */
/* 061 */     return mutableRow;
/* 062 */   }
/* 063 */ }

13375 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13375 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13375 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13376 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - SpecificUnsafeRowJoiner(StructType(StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)), StructType(StructField(value,IntegerType,true))):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeRowJoiner();
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeRowJoiner extends org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowJoiner {
/* 006 */   private byte[] buf = new byte[64];
/* 007 */   private UnsafeRow out = new UnsafeRow(4);
/* 008 */
/* 009 */   public UnsafeRow join(UnsafeRow row1, UnsafeRow row2) {
/* 010 */     // row1: 3 fields, 1 words in bitset
/* 011 */     // row2: 1, 1 words in bitset
/* 012 */     // output: 4 fields, 1 words in bitset
/* 013 */     final int sizeInBytes = row1.getSizeInBytes() + row2.getSizeInBytes() - 8;
/* 014 */     if (sizeInBytes > buf.length) {
/* 015 */       buf = new byte[sizeInBytes];
/* 016 */     }
/* 017 */
/* 018 */     final java.lang.Object obj1 = row1.getBaseObject();
/* 019 */     final long offset1 = row1.getBaseOffset();
/* 020 */     final java.lang.Object obj2 = row2.getBaseObject();
/* 021 */     final long offset2 = row2.getBaseOffset();
/* 022 */
/* 023 */     Platform.putLong(buf, 16, Platform.getLong(obj1, offset1 + 0) | (Platform.getLong(obj2, offset2) << 3));
/* 024 */
/* 025 */     // Copy fixed length data for row1
/* 026 */     Platform.copyMemory(
/* 027 */       obj1, offset1 + 8,
/* 028 */       buf, 24,
/* 029 */       24);
/* 030 */
/* 031 */
/* 032 */     // Copy fixed length data for row2
/* 033 */     Platform.copyMemory(
/* 034 */       obj2, offset2 + 8,
/* 035 */       buf, 48,
/* 036 */       8);
/* 037 */
/* 038 */
/* 039 */     // Copy variable length data for row1
/* 040 */     long numBytesVariableRow1 = row1.getSizeInBytes() - 32;
/* 041 */     Platform.copyMemory(
/* 042 */       obj1, offset1 + 32,
/* 043 */       buf, 56,
/* 044 */       numBytesVariableRow1);
/* 045 */
/* 046 */
/* 047 */     // Copy variable length data for row2
/* 048 */     long numBytesVariableRow2 = row2.getSizeInBytes() - 16;
/* 049 */     Platform.copyMemory(
/* 050 */       obj2, offset2 + 16,
/* 051 */       buf, 56 + numBytesVariableRow1,
/* 052 */       numBytesVariableRow2);
/* 053 */
/* 054 */
/* 055 */
/* 056 */
/* 057 */     Platform.putLong(buf, 40, Platform.getLong(buf, 40) + (8L << 32));
/* 058 */
/* 059 */
/* 060 */
/* 061 */     out.pointTo(buf, sizeInBytes);
/* 062 */
/* 063 */     return out;
/* 064 */   }
/* 065 */ }

13376 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13376 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13376 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13376 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13377 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

13377 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 194 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@62bc6aea
13377 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 2 (262144 bytes)
13378 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13376 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.expressions.codegen.package$ExpressionCanonicalizer$CleanExpressions ===
!input[0, int, true] AS value#47   input[0, int, true]
!+- input[0, int, true]            
                
13378 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 2 iterations.
13378 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - 
=== Result of Batch CleanExpressions ===
!input[0, int, true] AS value#47   input[0, int, true]
!+- input[0, int, true]            
        
13378 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13379 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13379 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13379 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13379 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13379 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13379 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13379 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull = i.isNullAt(0);
/* 030 */     int value = isNull ? -1 : (i.getInt(0));
/* 031 */     if (isNull) {
/* 032 */       rowWriter.setNullAt(0);
/* 033 */     } else {
/* 034 */       rowWriter.write(0, value);
/* 035 */     }
/* 036 */     return result;
/* 037 */   }
/* 038 */ }

13380 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13380 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13380 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for pmod(hash(input[3, int, true], 42), 200):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     boolean isNull = false;
/* 027 */
/* 028 */     int value1 = 42;
/* 029 */
/* 030 */     boolean isNull2 = i.isNullAt(3);
/* 031 */     int value2 = isNull2 ? -1 : (i.getInt(3));
/* 032 */     if (!isNull2) {
/* 033 */       value1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value2, value1);
/* 034 */     }
/* 035 */
/* 036 */     int value = -1;
/* 037 */
/* 038 */     int remainder = value1 % 200;
/* 039 */     if (remainder < 0) {
/* 040 */       value = (remainder + 200) % 200;
/* 041 */     } else {
/* 042 */       value = remainder;
/* 043 */     }
/* 044 */     rowWriter.write(0, value);
/* 045 */     return result;
/* 046 */   }
/* 047 */ }

13381 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13381 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13381 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13381 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13381 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13381 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13382 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13380 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

13382 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 194 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@3355a967
13382 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 3 (262144 bytes)
13383 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13383 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 194 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@5ce21ff7
13383 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 4 (16777216 bytes)
13382 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

13383 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 195 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@44727268
13383 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 2 (262144 bytes)
13383 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 194 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@738b718e
13384 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 5 (16777216 bytes)
13384 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 194 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@3355a967
13384 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 6 (16777216 bytes)
13384 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 4 (16777216 bytes)
13385 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 194 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@5ce21ff7
13385 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (65536 bytes)
13385 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 194 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@5ce21ff7
13385 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 194 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@62bc6aea
13385 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (16777216 bytes)
13385 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 3 (262144 bytes)
13385 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 194 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@3355a967
13385 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 6 (16777216 bytes)
13386 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 194 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@3355a967
13384 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13386 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13386 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13386 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13387 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13387 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13387 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13387 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13388 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

13388 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 195 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@205f8dbf
13388 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 3 (262144 bytes)
13388 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13389 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 195 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@27587ac7
13389 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 4 (16777216 bytes)
13389 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 195 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@14336e0c
13389 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 5 (16777216 bytes)
13390 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 195 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@205f8dbf
13390 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 6 (16777216 bytes)
13390 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 5 (16777216 bytes)
13390 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 191 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@7e61f615
13390 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 1 (65536 bytes)
13390 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 191 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@7e61f615
13390 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Task 191 releasing lock for broadcast_2
13391 [Executor task launch worker-4] INFO  org.apache.spark.internal.Logging$class  - Finished task 189.0 in stage 2.0 (TID 191). 4221 bytes result sent to driver
13390 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 4 (16777216 bytes)
13391 [dispatcher-event-loop-3] DEBUG org.apache.spark.internal.Logging$class  - parentName: , name: TaskSet_2, runningTasks: 3
13391 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 195 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@27587ac7
13391 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (65536 bytes)
13391 [dispatcher-event-loop-3] INFO  org.apache.spark.internal.Logging$class  - Starting task 194.0 in stage 2.0 (TID 196, localhost, partition 194, ANY, 5536 bytes)
13391 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 195 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@27587ac7
13392 [task-result-getter-0] INFO  org.apache.spark.internal.Logging$class  - Finished task 189.0 in stage 2.0 (TID 191) in 129 ms on localhost (191/200)
13392 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13392 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 2)
13392 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13392 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13392 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13392 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 2)
13392 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 2)
13393 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - ShuffleMapTask finished on driver
13393 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13393 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 2)
13393 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13393 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13393 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13393 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 2)
13393 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 2)
13392 [Executor task launch worker-4] INFO  org.apache.spark.internal.Logging$class  - Running task 194.0 in stage 2.0 (TID 196)
13392 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 5 (16777216 bytes)
13394 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 193 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2567f86d
13394 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 1 (65536 bytes)
13392 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 195 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@44727268
13394 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (16777216 bytes)
13394 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 3 (262144 bytes)
13394 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 195 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@205f8dbf
13394 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 6 (16777216 bytes)
13394 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 195 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@205f8dbf
13394 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 193 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2567f86d
13394 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Task 196's epoch is 2
13395 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Task 193 releasing lock for broadcast_2
13396 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Getting local block broadcast_2
13396 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Task 196 trying to acquire read lock for broadcast_2
13396 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Task 196 acquired read lock for broadcast_2
13396 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Level for block broadcast_2 is StorageLevel(disk, memory, deserialized, 1 replicas)
13396 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Finished task 191.0 in stage 2.0 (TID 193). 4221 bytes result sent to driver
13396 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 2 (262144 bytes)
13397 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 194 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@62bc6aea
13397 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (16777216 bytes)
13397 [dispatcher-event-loop-1] DEBUG org.apache.spark.internal.Logging$class  - parentName: , name: TaskSet_2, runningTasks: 3
13397 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 194 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@62bc6aea
13397 [dispatcher-event-loop-1] INFO  org.apache.spark.internal.Logging$class  - Starting task 195.0 in stage 2.0 (TID 197, localhost, partition 195, ANY, 5536 bytes)
13397 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13397 [task-result-getter-1] INFO  org.apache.spark.internal.Logging$class  - Finished task 191.0 in stage 2.0 (TID 193) in 120 ms on localhost (192/200)
13398 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Running task 195.0 in stage 2.0 (TID 197)
13398 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 2)
13398 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13398 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13398 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13398 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 2)
13398 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Task 197's epoch is 2
13398 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Getting local block broadcast_2
13398 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Task 197 trying to acquire read lock for broadcast_2
13398 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Task 197 acquired read lock for broadcast_2
13399 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Level for block broadcast_2 is StorageLevel(disk, memory, deserialized, 1 replicas)
13398 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 2)
13399 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 0, partitions 194-195
13399 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - ShuffleMapTask finished on driver
13399 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13399 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 2)
13399 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13399 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13399 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13399 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 2)
13399 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 2)
13399 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
13400 [Executor task launch worker-4] INFO  org.apache.spark.internal.Logging$class  - Getting 1 non-empty blocks out of 1 blocks
13400 [Executor task launch worker-4] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 1 ms
13400 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 0, partitions 195-196
13400 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  1 ms
13400 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
13401 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Getting 1 non-empty blocks out of 1 blocks
13401 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 1 ms
13401 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13401 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13401 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  1 ms
13402 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13402 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13402 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[1, int, true] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       boolean isNull = i.isNullAt(1);
/* 025 */       int value = isNull ? -1 : (i.getInt(1));
/* 026 */       isNullA = isNull;
/* 027 */       primitiveA = value;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB;
/* 031 */     int primitiveB;
/* 032 */     {
/* 033 */
/* 034 */       boolean isNull = i.isNullAt(1);
/* 035 */       int value = isNull ? -1 : (i.getInt(1));
/* 036 */       isNullB = isNull;
/* 037 */       primitiveB = value;
/* 038 */     }
/* 039 */     if (isNullA && isNullB) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */     return 0;
/* 053 */   }
/* 054 */ }

13402 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[1, int, true] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       boolean isNull = i.isNullAt(1);
/* 025 */       int value = isNull ? -1 : (i.getInt(1));
/* 026 */       isNullA = isNull;
/* 027 */       primitiveA = value;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB;
/* 031 */     int primitiveB;
/* 032 */     {
/* 033 */
/* 034 */       boolean isNull = i.isNullAt(1);
/* 035 */       int value = isNull ? -1 : (i.getInt(1));
/* 036 */       isNullB = isNull;
/* 037 */       primitiveB = value;
/* 038 */     }
/* 039 */     if (isNullA && isNullB) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */     return 0;
/* 053 */   }
/* 054 */ }

13402 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13403 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13403 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[1, int, true] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull1 = i.isNullAt(1);
/* 030 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 031 */     long value = -9223372036854775808L;
/* 032 */     boolean isNull = false;
/* 033 */     if (!isNull1) {
/* 034 */       value = (long) value1;
/* 035 */     }
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */     return result;
/* 042 */   }
/* 043 */ }

13403 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 196 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@6255db94
13403 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (65536 bytes)
13403 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 1, partitions 194-195
13404 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
13404 [Executor task launch worker-4] INFO  org.apache.spark.internal.Logging$class  - Getting 1 non-empty blocks out of 1 blocks
13404 [Executor task launch worker-4] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 0 ms
13402 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13404 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13404 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  0 ms
13405 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13405 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13405 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[1, int, true] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull1 = i.isNullAt(1);
/* 030 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 031 */     long value = -9223372036854775808L;
/* 032 */     boolean isNull = false;
/* 033 */     if (!isNull1) {
/* 034 */       value = (long) value1;
/* 035 */     }
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */     return result;
/* 042 */   }
/* 043 */ }

13405 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 197 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@219c69c7
13405 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (65536 bytes)
13406 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 1, partitions 195-196
13405 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 2 (262144 bytes)
13406 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 195 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@44727268
13406 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (16777216 bytes)
13406 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 195 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@44727268
13405 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[0, int, false] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       int value = i.getInt(0);
/* 025 */       isNullA = false;
/* 026 */       primitiveA = value;
/* 027 */     }
/* 028 */     i = b;
/* 029 */     boolean isNullB;
/* 030 */     int primitiveB;
/* 031 */     {
/* 032 */
/* 033 */       int value = i.getInt(0);
/* 034 */       isNullB = false;
/* 035 */       primitiveB = value;
/* 036 */     }
/* 037 */     if (isNullA && isNullB) {
/* 038 */       // Nothing
/* 039 */     } else if (isNullA) {
/* 040 */       return -1;
/* 041 */     } else if (isNullB) {
/* 042 */       return 1;
/* 043 */     } else {
/* 044 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 045 */       if (comp != 0) {
/* 046 */         return comp;
/* 047 */       }
/* 048 */     }
/* 049 */
/* 050 */     return 0;
/* 051 */   }
/* 052 */ }

13406 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
13407 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13407 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13407 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[0, int, false] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     int value1 = i.getInt(0);
/* 030 */     long value = -9223372036854775808L;
/* 031 */     boolean isNull = false;
/* 032 */     if (!false) {
/* 033 */       value = (long) value1;
/* 034 */     }
/* 035 */     if (isNull) {
/* 036 */       rowWriter.setNullAt(0);
/* 037 */     } else {
/* 038 */       rowWriter.write(0, value);
/* 039 */     }
/* 040 */     return result;
/* 041 */   }
/* 042 */ }

13407 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Getting 1 non-empty blocks out of 1 blocks
13408 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 2 ms
13408 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  2 ms
13408 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13408 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13408 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 196 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@20bb3e13
13409 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 1 (65536 bytes)
13409 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[0, int, false] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       int value = i.getInt(0);
/* 025 */       isNullA = false;
/* 026 */       primitiveA = value;
/* 027 */     }
/* 028 */     i = b;
/* 029 */     boolean isNullB;
/* 030 */     int primitiveB;
/* 031 */     {
/* 032 */
/* 033 */       int value = i.getInt(0);
/* 034 */       isNullB = false;
/* 035 */       primitiveB = value;
/* 036 */     }
/* 037 */     if (isNullA && isNullB) {
/* 038 */       // Nothing
/* 039 */     } else if (isNullA) {
/* 040 */       return -1;
/* 041 */     } else if (isNullB) {
/* 042 */       return 1;
/* 043 */     } else {
/* 044 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 045 */       if (comp != 0) {
/* 046 */         return comp;
/* 047 */       }
/* 048 */     }
/* 049 */
/* 050 */     return 0;
/* 051 */   }
/* 052 */ }

13409 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13409 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13409 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13409 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13410 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for createexternalrow(input[0, int, true], input[1, int, true], input[2, string, true].toString, StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private MutableRow mutableRow;
/* 009 */   private Object[] values;
/* 010 */   private org.apache.spark.sql.types.StructType schema;
/* 011 */
/* 012 */
/* 013 */   public SpecificSafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     mutableRow = (MutableRow) references[references.length - 1];
/* 016 */
/* 017 */     this.schema = (org.apache.spark.sql.types.StructType) references[0];
/* 018 */   }
/* 019 */
/* 020 */   public java.lang.Object apply(java.lang.Object _i) {
/* 021 */     InternalRow i = (InternalRow) _i;
/* 022 */
/* 023 */     values = new Object[3];
/* 024 */
/* 025 */     boolean isNull1 = i.isNullAt(0);
/* 026 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 027 */     if (isNull1) {
/* 028 */       values[0] = null;
/* 029 */     } else {
/* 030 */       values[0] = value1;
/* 031 */     }
/* 032 */
/* 033 */     boolean isNull2 = i.isNullAt(1);
/* 034 */     int value2 = isNull2 ? -1 : (i.getInt(1));
/* 035 */     if (isNull2) {
/* 036 */       values[1] = null;
/* 037 */     } else {
/* 038 */       values[1] = value2;
/* 039 */     }
/* 040 */
/* 041 */     boolean isNull4 = i.isNullAt(2);
/* 042 */     UTF8String value4 = isNull4 ? null : (i.getUTF8String(2));
/* 043 */
/* 044 */     boolean isNull3 = isNull4;
/* 045 */     final java.lang.String value3 = isNull3 ? null : (java.lang.String) value4.toString();
/* 046 */     isNull3 = value3 == null;
/* 047 */     if (isNull3) {
/* 048 */       values[2] = null;
/* 049 */     } else {
/* 050 */       values[2] = value3;
/* 051 */     }
/* 052 */
/* 053 */     final org.apache.spark.sql.Row value = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values, schema);
/* 054 */     if (false) {
/* 055 */       mutableRow.setNullAt(0);
/* 056 */     } else {
/* 057 */
/* 058 */       mutableRow.update(0, value);
/* 059 */     }
/* 060 */
/* 061 */     return mutableRow;
/* 062 */   }
/* 063 */ }

13410 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[0, int, false] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     int value1 = i.getInt(0);
/* 030 */     long value = -9223372036854775808L;
/* 031 */     boolean isNull = false;
/* 032 */     if (!false) {
/* 033 */       value = (long) value1;
/* 034 */     }
/* 035 */     if (isNull) {
/* 036 */       rowWriter.setNullAt(0);
/* 037 */     } else {
/* 038 */       rowWriter.write(0, value);
/* 039 */     }
/* 040 */     return result;
/* 041 */   }
/* 042 */ }

13410 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 197 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@636a7c0b
13410 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - SpecificUnsafeRowJoiner(StructType(StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)), StructType(StructField(value,IntegerType,true))):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeRowJoiner();
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeRowJoiner extends org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowJoiner {
/* 006 */   private byte[] buf = new byte[64];
/* 007 */   private UnsafeRow out = new UnsafeRow(4);
/* 008 */
/* 009 */   public UnsafeRow join(UnsafeRow row1, UnsafeRow row2) {
/* 010 */     // row1: 3 fields, 1 words in bitset
/* 011 */     // row2: 1, 1 words in bitset
/* 012 */     // output: 4 fields, 1 words in bitset
/* 013 */     final int sizeInBytes = row1.getSizeInBytes() + row2.getSizeInBytes() - 8;
/* 014 */     if (sizeInBytes > buf.length) {
/* 015 */       buf = new byte[sizeInBytes];
/* 016 */     }
/* 017 */
/* 018 */     final java.lang.Object obj1 = row1.getBaseObject();
/* 019 */     final long offset1 = row1.getBaseOffset();
/* 020 */     final java.lang.Object obj2 = row2.getBaseObject();
/* 021 */     final long offset2 = row2.getBaseOffset();
/* 022 */
/* 023 */     Platform.putLong(buf, 16, Platform.getLong(obj1, offset1 + 0) | (Platform.getLong(obj2, offset2) << 3));
/* 024 */
/* 025 */     // Copy fixed length data for row1
/* 026 */     Platform.copyMemory(
/* 027 */       obj1, offset1 + 8,
/* 028 */       buf, 24,
/* 029 */       24);
/* 030 */
/* 031 */
/* 032 */     // Copy fixed length data for row2
/* 033 */     Platform.copyMemory(
/* 034 */       obj2, offset2 + 8,
/* 035 */       buf, 48,
/* 036 */       8);
/* 037 */
/* 038 */
/* 039 */     // Copy variable length data for row1
/* 040 */     long numBytesVariableRow1 = row1.getSizeInBytes() - 32;
/* 041 */     Platform.copyMemory(
/* 042 */       obj1, offset1 + 32,
/* 043 */       buf, 56,
/* 044 */       numBytesVariableRow1);
/* 045 */
/* 046 */
/* 047 */     // Copy variable length data for row2
/* 048 */     long numBytesVariableRow2 = row2.getSizeInBytes() - 16;
/* 049 */     Platform.copyMemory(
/* 050 */       obj2, offset2 + 16,
/* 051 */       buf, 56 + numBytesVariableRow1,
/* 052 */       numBytesVariableRow2);
/* 053 */
/* 054 */
/* 055 */
/* 056 */
/* 057 */     Platform.putLong(buf, 40, Platform.getLong(buf, 40) + (8L << 32));
/* 058 */
/* 059 */
/* 060 */
/* 061 */     out.pointTo(buf, sizeInBytes);
/* 062 */
/* 063 */     return out;
/* 064 */   }
/* 065 */ }

13410 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.expressions.codegen.package$ExpressionCanonicalizer$CleanExpressions ===
!input[0, int, true] AS value#47   input[0, int, true]
!+- input[0, int, true]            
                
13411 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 2 iterations.
13410 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 1 (65536 bytes)
13411 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - 
=== Result of Batch CleanExpressions ===
!input[0, int, true] AS value#47   input[0, int, true]
!+- input[0, int, true]            
        
13411 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13411 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13411 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull = i.isNullAt(0);
/* 030 */     int value = isNull ? -1 : (i.getInt(0));
/* 031 */     if (isNull) {
/* 032 */       rowWriter.setNullAt(0);
/* 033 */     } else {
/* 034 */       rowWriter.write(0, value);
/* 035 */     }
/* 036 */     return result;
/* 037 */   }
/* 038 */ }

13412 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13412 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13412 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for createexternalrow(input[0, int, true], input[1, int, true], input[2, string, true].toString, StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private MutableRow mutableRow;
/* 009 */   private Object[] values;
/* 010 */   private org.apache.spark.sql.types.StructType schema;
/* 011 */
/* 012 */
/* 013 */   public SpecificSafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     mutableRow = (MutableRow) references[references.length - 1];
/* 016 */
/* 017 */     this.schema = (org.apache.spark.sql.types.StructType) references[0];
/* 018 */   }
/* 019 */
/* 020 */   public java.lang.Object apply(java.lang.Object _i) {
/* 021 */     InternalRow i = (InternalRow) _i;
/* 022 */
/* 023 */     values = new Object[3];
/* 024 */
/* 025 */     boolean isNull1 = i.isNullAt(0);
/* 026 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 027 */     if (isNull1) {
/* 028 */       values[0] = null;
/* 029 */     } else {
/* 030 */       values[0] = value1;
/* 031 */     }
/* 032 */
/* 033 */     boolean isNull2 = i.isNullAt(1);
/* 034 */     int value2 = isNull2 ? -1 : (i.getInt(1));
/* 035 */     if (isNull2) {
/* 036 */       values[1] = null;
/* 037 */     } else {
/* 038 */       values[1] = value2;
/* 039 */     }
/* 040 */
/* 041 */     boolean isNull4 = i.isNullAt(2);
/* 042 */     UTF8String value4 = isNull4 ? null : (i.getUTF8String(2));
/* 043 */
/* 044 */     boolean isNull3 = isNull4;
/* 045 */     final java.lang.String value3 = isNull3 ? null : (java.lang.String) value4.toString();
/* 046 */     isNull3 = value3 == null;
/* 047 */     if (isNull3) {
/* 048 */       values[2] = null;
/* 049 */     } else {
/* 050 */       values[2] = value3;
/* 051 */     }
/* 052 */
/* 053 */     final org.apache.spark.sql.Row value = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values, schema);
/* 054 */     if (false) {
/* 055 */       mutableRow.setNullAt(0);
/* 056 */     } else {
/* 057 */
/* 058 */       mutableRow.update(0, value);
/* 059 */     }
/* 060 */
/* 061 */     return mutableRow;
/* 062 */   }
/* 063 */ }

13412 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for pmod(hash(input[3, int, true], 42), 200):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     boolean isNull = false;
/* 027 */
/* 028 */     int value1 = 42;
/* 029 */
/* 030 */     boolean isNull2 = i.isNullAt(3);
/* 031 */     int value2 = isNull2 ? -1 : (i.getInt(3));
/* 032 */     if (!isNull2) {
/* 033 */       value1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value2, value1);
/* 034 */     }
/* 035 */
/* 036 */     int value = -1;
/* 037 */
/* 038 */     int remainder = value1 % 200;
/* 039 */     if (remainder < 0) {
/* 040 */       value = (remainder + 200) % 200;
/* 041 */     } else {
/* 042 */       value = remainder;
/* 043 */     }
/* 044 */     rowWriter.write(0, value);
/* 045 */     return result;
/* 046 */   }
/* 047 */ }

13412 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - SpecificUnsafeRowJoiner(StructType(StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)), StructType(StructField(value,IntegerType,true))):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeRowJoiner();
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeRowJoiner extends org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowJoiner {
/* 006 */   private byte[] buf = new byte[64];
/* 007 */   private UnsafeRow out = new UnsafeRow(4);
/* 008 */
/* 009 */   public UnsafeRow join(UnsafeRow row1, UnsafeRow row2) {
/* 010 */     // row1: 3 fields, 1 words in bitset
/* 011 */     // row2: 1, 1 words in bitset
/* 012 */     // output: 4 fields, 1 words in bitset
/* 013 */     final int sizeInBytes = row1.getSizeInBytes() + row2.getSizeInBytes() - 8;
/* 014 */     if (sizeInBytes > buf.length) {
/* 015 */       buf = new byte[sizeInBytes];
/* 016 */     }
/* 017 */
/* 018 */     final java.lang.Object obj1 = row1.getBaseObject();
/* 019 */     final long offset1 = row1.getBaseOffset();
/* 020 */     final java.lang.Object obj2 = row2.getBaseObject();
/* 021 */     final long offset2 = row2.getBaseOffset();
/* 022 */
/* 023 */     Platform.putLong(buf, 16, Platform.getLong(obj1, offset1 + 0) | (Platform.getLong(obj2, offset2) << 3));
/* 024 */
/* 025 */     // Copy fixed length data for row1
/* 026 */     Platform.copyMemory(
/* 027 */       obj1, offset1 + 8,
/* 028 */       buf, 24,
/* 029 */       24);
/* 030 */
/* 031 */
/* 032 */     // Copy fixed length data for row2
/* 033 */     Platform.copyMemory(
/* 034 */       obj2, offset2 + 8,
/* 035 */       buf, 48,
/* 036 */       8);
/* 037 */
/* 038 */
/* 039 */     // Copy variable length data for row1
/* 040 */     long numBytesVariableRow1 = row1.getSizeInBytes() - 32;
/* 041 */     Platform.copyMemory(
/* 042 */       obj1, offset1 + 32,
/* 043 */       buf, 56,
/* 044 */       numBytesVariableRow1);
/* 045 */
/* 046 */
/* 047 */     // Copy variable length data for row2
/* 048 */     long numBytesVariableRow2 = row2.getSizeInBytes() - 16;
/* 049 */     Platform.copyMemory(
/* 050 */       obj2, offset2 + 16,
/* 051 */       buf, 56 + numBytesVariableRow1,
/* 052 */       numBytesVariableRow2);
/* 053 */
/* 054 */
/* 055 */
/* 056 */
/* 057 */     Platform.putLong(buf, 40, Platform.getLong(buf, 40) + (8L << 32));
/* 058 */
/* 059 */
/* 060 */
/* 061 */     out.pointTo(buf, sizeInBytes);
/* 062 */
/* 063 */     return out;
/* 064 */   }
/* 065 */ }

13413 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13413 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13413 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13413 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13413 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13413 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13413 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13413 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.expressions.codegen.package$ExpressionCanonicalizer$CleanExpressions ===
!input[0, int, true] AS value#47   input[0, int, true]
!+- input[0, int, true]            
                
13414 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 2 iterations.
13414 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - 
=== Result of Batch CleanExpressions ===
!input[0, int, true] AS value#47   input[0, int, true]
!+- input[0, int, true]            
        
13414 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

13415 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 196 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@134a5f79
13415 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 2 (262144 bytes)
13415 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13415 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull = i.isNullAt(0);
/* 030 */     int value = isNull ? -1 : (i.getInt(0));
/* 031 */     if (isNull) {
/* 032 */       rowWriter.setNullAt(0);
/* 033 */     } else {
/* 034 */       rowWriter.write(0, value);
/* 035 */     }
/* 036 */     return result;
/* 037 */   }
/* 038 */ }

13415 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13416 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13416 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13416 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13416 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13416 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13416 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13417 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

13417 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 196 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@44c780dc
13417 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 3 (262144 bytes)
13418 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13418 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 196 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@6255db94
13418 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 4 (16777216 bytes)
13418 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 196 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@20bb3e13
13419 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 5 (16777216 bytes)
13419 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 196 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@44c780dc
13419 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 6 (16777216 bytes)
13419 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 4 (16777216 bytes)
13419 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 196 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@6255db94
13420 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (65536 bytes)
13420 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 196 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@6255db94
13420 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 196 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@134a5f79
13420 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (16777216 bytes)
13420 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 3 (262144 bytes)
13420 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 196 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@44c780dc
13420 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 6 (16777216 bytes)
13420 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 196 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@44c780dc
13415 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 5 (16777216 bytes)
13425 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 194 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@738b718e
13415 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13426 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13426 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for pmod(hash(input[3, int, true], 42), 200):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     boolean isNull = false;
/* 027 */
/* 028 */     int value1 = 42;
/* 029 */
/* 030 */     boolean isNull2 = i.isNullAt(3);
/* 031 */     int value2 = isNull2 ? -1 : (i.getInt(3));
/* 032 */     if (!isNull2) {
/* 033 */       value1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value2, value1);
/* 034 */     }
/* 035 */
/* 036 */     int value = -1;
/* 037 */
/* 038 */     int remainder = value1 % 200;
/* 039 */     if (remainder < 0) {
/* 040 */       value = (remainder + 200) % 200;
/* 041 */     } else {
/* 042 */       value = remainder;
/* 043 */     }
/* 044 */     rowWriter.write(0, value);
/* 045 */     return result;
/* 046 */   }
/* 047 */ }

13426 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13427 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13427 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13427 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13427 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13427 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13427 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13428 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

13425 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 1 (65536 bytes)
13428 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 197 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@7b241287
13427 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 5 (16777216 bytes)
13428 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 195 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@14336e0c
13429 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 1 (65536 bytes)
13429 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 195 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@14336e0c
13429 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Task 195 releasing lock for broadcast_2
13429 [Executor task launch worker-0] INFO  org.apache.spark.internal.Logging$class  - Finished task 193.0 in stage 2.0 (TID 195). 4221 bytes result sent to driver
13428 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 2 (262144 bytes)
13428 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 194 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@738b718e
13430 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Task 194 releasing lock for broadcast_2
13430 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13430 [Executor task launch worker-3] INFO  org.apache.spark.internal.Logging$class  - Finished task 192.0 in stage 2.0 (TID 194). 4221 bytes result sent to driver
13429 [dispatcher-event-loop-5] DEBUG org.apache.spark.internal.Logging$class  - parentName: , name: TaskSet_2, runningTasks: 3
13431 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13431 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13431 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13431 [dispatcher-event-loop-5] INFO  org.apache.spark.internal.Logging$class  - Starting task 196.0 in stage 2.0 (TID 198, localhost, partition 196, ANY, 5536 bytes)
13431 [dispatcher-event-loop-5] DEBUG org.apache.spark.internal.Logging$class  - parentName: , name: TaskSet_2, runningTasks: 3
13431 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13432 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13432 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13432 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13432 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

13432 [Executor task launch worker-3] INFO  org.apache.spark.internal.Logging$class  - Running task 196.0 in stage 2.0 (TID 198)
13431 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13433 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 2)
13433 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13433 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13433 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13433 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 197 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@1f622788
13432 [dispatcher-event-loop-5] INFO  org.apache.spark.internal.Logging$class  - Starting task 197.0 in stage 2.0 (TID 199, localhost, partition 197, ANY, 5536 bytes)
13433 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 3 (262144 bytes)
13434 [Executor task launch worker-0] INFO  org.apache.spark.internal.Logging$class  - Running task 197.0 in stage 2.0 (TID 199)
13433 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 2)
13434 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 2)
13434 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13434 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 2)
13434 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13434 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 2 (262144 bytes)
13434 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 196 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@134a5f79
13434 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (16777216 bytes)
13435 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 196 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@134a5f79
13433 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Task 198's epoch is 2
13435 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Getting local block broadcast_2
13435 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Task 198 trying to acquire read lock for broadcast_2
13434 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13434 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Task 199's epoch is 2
13434 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13434 [task-result-getter-2] INFO  org.apache.spark.internal.Logging$class  - Finished task 193.0 in stage 2.0 (TID 195) in 75 ms on localhost (193/200)
13435 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13436 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 2)
13436 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 2)
13435 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Task 198 acquired read lock for broadcast_2
13436 [task-result-getter-3] INFO  org.apache.spark.internal.Logging$class  - Finished task 192.0 in stage 2.0 (TID 194) in 91 ms on localhost (194/200)
13436 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 197 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@219c69c7
13436 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 4 (16777216 bytes)
13436 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Level for block broadcast_2 is StorageLevel(disk, memory, deserialized, 1 replicas)
13436 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - ShuffleMapTask finished on driver
13437 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Getting local block broadcast_2
13437 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Task 199 trying to acquire read lock for broadcast_2
13437 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Task 199 acquired read lock for broadcast_2
13437 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Level for block broadcast_2 is StorageLevel(disk, memory, deserialized, 1 replicas)
13437 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 197 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@636a7c0b
13437 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 5 (16777216 bytes)
13437 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13438 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 2)
13438 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13438 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13438 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13438 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 2)
13438 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 2)
13438 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 197 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@1f622788
13439 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 6 (16777216 bytes)
13439 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - ShuffleMapTask finished on driver
13439 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 0, partitions 197-198
13439 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 4 (16777216 bytes)
13439 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 197 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@219c69c7
13439 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (65536 bytes)
13440 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 197 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@219c69c7
13440 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 197 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@7b241287
13440 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (16777216 bytes)
13439 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13440 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 2)
13440 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13440 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13440 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13440 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 3 (262144 bytes)
13441 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 197 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@1f622788
13441 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 6 (16777216 bytes)
13441 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 197 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@1f622788
13440 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 0, partitions 196-197
13439 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
13441 [Executor task launch worker-0] INFO  org.apache.spark.internal.Logging$class  - Getting 1 non-empty blocks out of 1 blocks
13441 [Executor task launch worker-0] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 2 ms
13441 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
13442 [Executor task launch worker-3] INFO  org.apache.spark.internal.Logging$class  - Getting 1 non-empty blocks out of 1 blocks
13442 [Executor task launch worker-3] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 1 ms
13442 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  1 ms
13442 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13442 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13443 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[1, int, true] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       boolean isNull = i.isNullAt(1);
/* 025 */       int value = isNull ? -1 : (i.getInt(1));
/* 026 */       isNullA = isNull;
/* 027 */       primitiveA = value;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB;
/* 031 */     int primitiveB;
/* 032 */     {
/* 033 */
/* 034 */       boolean isNull = i.isNullAt(1);
/* 035 */       int value = isNull ? -1 : (i.getInt(1));
/* 036 */       isNullB = isNull;
/* 037 */       primitiveB = value;
/* 038 */     }
/* 039 */     if (isNullA && isNullB) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */     return 0;
/* 053 */   }
/* 054 */ }

13443 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13443 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13440 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 2)
13442 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  3 ms
13444 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13444 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13444 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 2)
13444 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[1, int, true] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull1 = i.isNullAt(1);
/* 030 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 031 */     long value = -9223372036854775808L;
/* 032 */     boolean isNull = false;
/* 033 */     if (!isNull1) {
/* 034 */       value = (long) value1;
/* 035 */     }
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */     return result;
/* 042 */   }
/* 043 */ }

13445 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[1, int, true] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       boolean isNull = i.isNullAt(1);
/* 025 */       int value = isNull ? -1 : (i.getInt(1));
/* 026 */       isNullA = isNull;
/* 027 */       primitiveA = value;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB;
/* 031 */     int primitiveB;
/* 032 */     {
/* 033 */
/* 034 */       boolean isNull = i.isNullAt(1);
/* 035 */       int value = isNull ? -1 : (i.getInt(1));
/* 036 */       isNullB = isNull;
/* 037 */       primitiveB = value;
/* 038 */     }
/* 039 */     if (isNullA && isNullB) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */     return 0;
/* 053 */   }
/* 054 */ }

13445 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13445 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13445 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[1, int, true] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull1 = i.isNullAt(1);
/* 030 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 031 */     long value = -9223372036854775808L;
/* 032 */     boolean isNull = false;
/* 033 */     if (!isNull1) {
/* 034 */       value = (long) value1;
/* 035 */     }
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */     return result;
/* 042 */   }
/* 043 */ }

13446 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 199 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@17db7c2c
13446 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (65536 bytes)
13446 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 1, partitions 197-198
13446 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
13446 [Executor task launch worker-0] INFO  org.apache.spark.internal.Logging$class  - Getting 1 non-empty blocks out of 1 blocks
13446 [Executor task launch worker-0] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 0 ms
13445 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 198 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@60f20dca
13447 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (65536 bytes)
13447 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 1, partitions 196-197
13447 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
13447 [Executor task launch worker-3] INFO  org.apache.spark.internal.Logging$class  - Getting 1 non-empty blocks out of 1 blocks
13447 [Executor task launch worker-3] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 0 ms
13447 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  0 ms
13448 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13448 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13446 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  0 ms
13448 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[0, int, false] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       int value = i.getInt(0);
/* 025 */       isNullA = false;
/* 026 */       primitiveA = value;
/* 027 */     }
/* 028 */     i = b;
/* 029 */     boolean isNullB;
/* 030 */     int primitiveB;
/* 031 */     {
/* 032 */
/* 033 */       int value = i.getInt(0);
/* 034 */       isNullB = false;
/* 035 */       primitiveB = value;
/* 036 */     }
/* 037 */     if (isNullA && isNullB) {
/* 038 */       // Nothing
/* 039 */     } else if (isNullA) {
/* 040 */       return -1;
/* 041 */     } else if (isNullB) {
/* 042 */       return 1;
/* 043 */     } else {
/* 044 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 045 */       if (comp != 0) {
/* 046 */         return comp;
/* 047 */       }
/* 048 */     }
/* 049 */
/* 050 */     return 0;
/* 051 */   }
/* 052 */ }

13448 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13449 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13448 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13449 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13449 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[0, int, false] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     int value1 = i.getInt(0);
/* 030 */     long value = -9223372036854775808L;
/* 031 */     boolean isNull = false;
/* 032 */     if (!false) {
/* 033 */       value = (long) value1;
/* 034 */     }
/* 035 */     if (isNull) {
/* 036 */       rowWriter.setNullAt(0);
/* 037 */     } else {
/* 038 */       rowWriter.write(0, value);
/* 039 */     }
/* 040 */     return result;
/* 041 */   }
/* 042 */ }

13449 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 198 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@470069b5
13449 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 1 (65536 bytes)
13450 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13450 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13449 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[0, int, false] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       int value = i.getInt(0);
/* 025 */       isNullA = false;
/* 026 */       primitiveA = value;
/* 027 */     }
/* 028 */     i = b;
/* 029 */     boolean isNullB;
/* 030 */     int primitiveB;
/* 031 */     {
/* 032 */
/* 033 */       int value = i.getInt(0);
/* 034 */       isNullB = false;
/* 035 */       primitiveB = value;
/* 036 */     }
/* 037 */     if (isNullA && isNullB) {
/* 038 */       // Nothing
/* 039 */     } else if (isNullA) {
/* 040 */       return -1;
/* 041 */     } else if (isNullB) {
/* 042 */       return 1;
/* 043 */     } else {
/* 044 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 045 */       if (comp != 0) {
/* 046 */         return comp;
/* 047 */       }
/* 048 */     }
/* 049 */
/* 050 */     return 0;
/* 051 */   }
/* 052 */ }

13450 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13450 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13451 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for createexternalrow(input[0, int, true], input[1, int, true], input[2, string, true].toString, StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private MutableRow mutableRow;
/* 009 */   private Object[] values;
/* 010 */   private org.apache.spark.sql.types.StructType schema;
/* 011 */
/* 012 */
/* 013 */   public SpecificSafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     mutableRow = (MutableRow) references[references.length - 1];
/* 016 */
/* 017 */     this.schema = (org.apache.spark.sql.types.StructType) references[0];
/* 018 */   }
/* 019 */
/* 020 */   public java.lang.Object apply(java.lang.Object _i) {
/* 021 */     InternalRow i = (InternalRow) _i;
/* 022 */
/* 023 */     values = new Object[3];
/* 024 */
/* 025 */     boolean isNull1 = i.isNullAt(0);
/* 026 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 027 */     if (isNull1) {
/* 028 */       values[0] = null;
/* 029 */     } else {
/* 030 */       values[0] = value1;
/* 031 */     }
/* 032 */
/* 033 */     boolean isNull2 = i.isNullAt(1);
/* 034 */     int value2 = isNull2 ? -1 : (i.getInt(1));
/* 035 */     if (isNull2) {
/* 036 */       values[1] = null;
/* 037 */     } else {
/* 038 */       values[1] = value2;
/* 039 */     }
/* 040 */
/* 041 */     boolean isNull4 = i.isNullAt(2);
/* 042 */     UTF8String value4 = isNull4 ? null : (i.getUTF8String(2));
/* 043 */
/* 044 */     boolean isNull3 = isNull4;
/* 045 */     final java.lang.String value3 = isNull3 ? null : (java.lang.String) value4.toString();
/* 046 */     isNull3 = value3 == null;
/* 047 */     if (isNull3) {
/* 048 */       values[2] = null;
/* 049 */     } else {
/* 050 */       values[2] = value3;
/* 051 */     }
/* 052 */
/* 053 */     final org.apache.spark.sql.Row value = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values, schema);
/* 054 */     if (false) {
/* 055 */       mutableRow.setNullAt(0);
/* 056 */     } else {
/* 057 */
/* 058 */       mutableRow.update(0, value);
/* 059 */     }
/* 060 */
/* 061 */     return mutableRow;
/* 062 */   }
/* 063 */ }

13451 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[0, int, false] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     int value1 = i.getInt(0);
/* 030 */     long value = -9223372036854775808L;
/* 031 */     boolean isNull = false;
/* 032 */     if (!false) {
/* 033 */       value = (long) value1;
/* 034 */     }
/* 035 */     if (isNull) {
/* 036 */       rowWriter.setNullAt(0);
/* 037 */     } else {
/* 038 */       rowWriter.write(0, value);
/* 039 */     }
/* 040 */     return result;
/* 041 */   }
/* 042 */ }

13451 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 199 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@907234e
13451 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 1 (65536 bytes)
13451 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - SpecificUnsafeRowJoiner(StructType(StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)), StructType(StructField(value,IntegerType,true))):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeRowJoiner();
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeRowJoiner extends org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowJoiner {
/* 006 */   private byte[] buf = new byte[64];
/* 007 */   private UnsafeRow out = new UnsafeRow(4);
/* 008 */
/* 009 */   public UnsafeRow join(UnsafeRow row1, UnsafeRow row2) {
/* 010 */     // row1: 3 fields, 1 words in bitset
/* 011 */     // row2: 1, 1 words in bitset
/* 012 */     // output: 4 fields, 1 words in bitset
/* 013 */     final int sizeInBytes = row1.getSizeInBytes() + row2.getSizeInBytes() - 8;
/* 014 */     if (sizeInBytes > buf.length) {
/* 015 */       buf = new byte[sizeInBytes];
/* 016 */     }
/* 017 */
/* 018 */     final java.lang.Object obj1 = row1.getBaseObject();
/* 019 */     final long offset1 = row1.getBaseOffset();
/* 020 */     final java.lang.Object obj2 = row2.getBaseObject();
/* 021 */     final long offset2 = row2.getBaseOffset();
/* 022 */
/* 023 */     Platform.putLong(buf, 16, Platform.getLong(obj1, offset1 + 0) | (Platform.getLong(obj2, offset2) << 3));
/* 024 */
/* 025 */     // Copy fixed length data for row1
/* 026 */     Platform.copyMemory(
/* 027 */       obj1, offset1 + 8,
/* 028 */       buf, 24,
/* 029 */       24);
/* 030 */
/* 031 */
/* 032 */     // Copy fixed length data for row2
/* 033 */     Platform.copyMemory(
/* 034 */       obj2, offset2 + 8,
/* 035 */       buf, 48,
/* 036 */       8);
/* 037 */
/* 038 */
/* 039 */     // Copy variable length data for row1
/* 040 */     long numBytesVariableRow1 = row1.getSizeInBytes() - 32;
/* 041 */     Platform.copyMemory(
/* 042 */       obj1, offset1 + 32,
/* 043 */       buf, 56,
/* 044 */       numBytesVariableRow1);
/* 045 */
/* 046 */
/* 047 */     // Copy variable length data for row2
/* 048 */     long numBytesVariableRow2 = row2.getSizeInBytes() - 16;
/* 049 */     Platform.copyMemory(
/* 050 */       obj2, offset2 + 16,
/* 051 */       buf, 56 + numBytesVariableRow1,
/* 052 */       numBytesVariableRow2);
/* 053 */
/* 054 */
/* 055 */
/* 056 */
/* 057 */     Platform.putLong(buf, 40, Platform.getLong(buf, 40) + (8L << 32));
/* 058 */
/* 059 */
/* 060 */
/* 061 */     out.pointTo(buf, sizeInBytes);
/* 062 */
/* 063 */     return out;
/* 064 */   }
/* 065 */ }

13452 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13452 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13452 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.expressions.codegen.package$ExpressionCanonicalizer$CleanExpressions ===
!input[0, int, true] AS value#47   input[0, int, true]
!+- input[0, int, true]            
                
13452 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 2 iterations.
13452 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - 
=== Result of Batch CleanExpressions ===
!input[0, int, true] AS value#47   input[0, int, true]
!+- input[0, int, true]            
        
13452 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for createexternalrow(input[0, int, true], input[1, int, true], input[2, string, true].toString, StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private MutableRow mutableRow;
/* 009 */   private Object[] values;
/* 010 */   private org.apache.spark.sql.types.StructType schema;
/* 011 */
/* 012 */
/* 013 */   public SpecificSafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     mutableRow = (MutableRow) references[references.length - 1];
/* 016 */
/* 017 */     this.schema = (org.apache.spark.sql.types.StructType) references[0];
/* 018 */   }
/* 019 */
/* 020 */   public java.lang.Object apply(java.lang.Object _i) {
/* 021 */     InternalRow i = (InternalRow) _i;
/* 022 */
/* 023 */     values = new Object[3];
/* 024 */
/* 025 */     boolean isNull1 = i.isNullAt(0);
/* 026 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 027 */     if (isNull1) {
/* 028 */       values[0] = null;
/* 029 */     } else {
/* 030 */       values[0] = value1;
/* 031 */     }
/* 032 */
/* 033 */     boolean isNull2 = i.isNullAt(1);
/* 034 */     int value2 = isNull2 ? -1 : (i.getInt(1));
/* 035 */     if (isNull2) {
/* 036 */       values[1] = null;
/* 037 */     } else {
/* 038 */       values[1] = value2;
/* 039 */     }
/* 040 */
/* 041 */     boolean isNull4 = i.isNullAt(2);
/* 042 */     UTF8String value4 = isNull4 ? null : (i.getUTF8String(2));
/* 043 */
/* 044 */     boolean isNull3 = isNull4;
/* 045 */     final java.lang.String value3 = isNull3 ? null : (java.lang.String) value4.toString();
/* 046 */     isNull3 = value3 == null;
/* 047 */     if (isNull3) {
/* 048 */       values[2] = null;
/* 049 */     } else {
/* 050 */       values[2] = value3;
/* 051 */     }
/* 052 */
/* 053 */     final org.apache.spark.sql.Row value = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values, schema);
/* 054 */     if (false) {
/* 055 */       mutableRow.setNullAt(0);
/* 056 */     } else {
/* 057 */
/* 058 */       mutableRow.update(0, value);
/* 059 */     }
/* 060 */
/* 061 */     return mutableRow;
/* 062 */   }
/* 063 */ }

13453 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull = i.isNullAt(0);
/* 030 */     int value = isNull ? -1 : (i.getInt(0));
/* 031 */     if (isNull) {
/* 032 */       rowWriter.setNullAt(0);
/* 033 */     } else {
/* 034 */       rowWriter.write(0, value);
/* 035 */     }
/* 036 */     return result;
/* 037 */   }
/* 038 */ }

13453 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13453 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13453 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - SpecificUnsafeRowJoiner(StructType(StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)), StructType(StructField(value,IntegerType,true))):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeRowJoiner();
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeRowJoiner extends org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowJoiner {
/* 006 */   private byte[] buf = new byte[64];
/* 007 */   private UnsafeRow out = new UnsafeRow(4);
/* 008 */
/* 009 */   public UnsafeRow join(UnsafeRow row1, UnsafeRow row2) {
/* 010 */     // row1: 3 fields, 1 words in bitset
/* 011 */     // row2: 1, 1 words in bitset
/* 012 */     // output: 4 fields, 1 words in bitset
/* 013 */     final int sizeInBytes = row1.getSizeInBytes() + row2.getSizeInBytes() - 8;
/* 014 */     if (sizeInBytes > buf.length) {
/* 015 */       buf = new byte[sizeInBytes];
/* 016 */     }
/* 017 */
/* 018 */     final java.lang.Object obj1 = row1.getBaseObject();
/* 019 */     final long offset1 = row1.getBaseOffset();
/* 020 */     final java.lang.Object obj2 = row2.getBaseObject();
/* 021 */     final long offset2 = row2.getBaseOffset();
/* 022 */
/* 023 */     Platform.putLong(buf, 16, Platform.getLong(obj1, offset1 + 0) | (Platform.getLong(obj2, offset2) << 3));
/* 024 */
/* 025 */     // Copy fixed length data for row1
/* 026 */     Platform.copyMemory(
/* 027 */       obj1, offset1 + 8,
/* 028 */       buf, 24,
/* 029 */       24);
/* 030 */
/* 031 */
/* 032 */     // Copy fixed length data for row2
/* 033 */     Platform.copyMemory(
/* 034 */       obj2, offset2 + 8,
/* 035 */       buf, 48,
/* 036 */       8);
/* 037 */
/* 038 */
/* 039 */     // Copy variable length data for row1
/* 040 */     long numBytesVariableRow1 = row1.getSizeInBytes() - 32;
/* 041 */     Platform.copyMemory(
/* 042 */       obj1, offset1 + 32,
/* 043 */       buf, 56,
/* 044 */       numBytesVariableRow1);
/* 045 */
/* 046 */
/* 047 */     // Copy variable length data for row2
/* 048 */     long numBytesVariableRow2 = row2.getSizeInBytes() - 16;
/* 049 */     Platform.copyMemory(
/* 050 */       obj2, offset2 + 16,
/* 051 */       buf, 56 + numBytesVariableRow1,
/* 052 */       numBytesVariableRow2);
/* 053 */
/* 054 */
/* 055 */
/* 056 */
/* 057 */     Platform.putLong(buf, 40, Platform.getLong(buf, 40) + (8L << 32));
/* 058 */
/* 059 */
/* 060 */
/* 061 */     out.pointTo(buf, sizeInBytes);
/* 062 */
/* 063 */     return out;
/* 064 */   }
/* 065 */ }

13454 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.expressions.codegen.package$ExpressionCanonicalizer$CleanExpressions ===
!input[0, int, true] AS value#47   input[0, int, true]
!+- input[0, int, true]            
                
13454 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 2 iterations.
13454 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - 
=== Result of Batch CleanExpressions ===
!input[0, int, true] AS value#47   input[0, int, true]
!+- input[0, int, true]            
        
13454 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for pmod(hash(input[3, int, true], 42), 200):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     boolean isNull = false;
/* 027 */
/* 028 */     int value1 = 42;
/* 029 */
/* 030 */     boolean isNull2 = i.isNullAt(3);
/* 031 */     int value2 = isNull2 ? -1 : (i.getInt(3));
/* 032 */     if (!isNull2) {
/* 033 */       value1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value2, value1);
/* 034 */     }
/* 035 */
/* 036 */     int value = -1;
/* 037 */
/* 038 */     int remainder = value1 % 200;
/* 039 */     if (remainder < 0) {
/* 040 */       value = (remainder + 200) % 200;
/* 041 */     } else {
/* 042 */       value = remainder;
/* 043 */     }
/* 044 */     rowWriter.write(0, value);
/* 045 */     return result;
/* 046 */   }
/* 047 */ }

13454 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull = i.isNullAt(0);
/* 030 */     int value = isNull ? -1 : (i.getInt(0));
/* 031 */     if (isNull) {
/* 032 */       rowWriter.setNullAt(0);
/* 033 */     } else {
/* 034 */       rowWriter.write(0, value);
/* 035 */     }
/* 036 */     return result;
/* 037 */   }
/* 038 */ }

13454 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13455 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13455 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13455 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13455 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13455 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13455 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13455 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 2 (262144 bytes)
13455 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 197 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@7b241287
13456 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (16777216 bytes)
13456 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 197 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@7b241287
13454 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13456 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13456 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

13457 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 198 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@61f7a083
13457 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 2 (262144 bytes)
13457 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13457 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for pmod(hash(input[3, int, true], 42), 200):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     boolean isNull = false;
/* 027 */
/* 028 */     int value1 = 42;
/* 029 */
/* 030 */     boolean isNull2 = i.isNullAt(3);
/* 031 */     int value2 = isNull2 ? -1 : (i.getInt(3));
/* 032 */     if (!isNull2) {
/* 033 */       value1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value2, value1);
/* 034 */     }
/* 035 */
/* 036 */     int value = -1;
/* 037 */
/* 038 */     int remainder = value1 % 200;
/* 039 */     if (remainder < 0) {
/* 040 */       value = (remainder + 200) % 200;
/* 041 */     } else {
/* 042 */       value = remainder;
/* 043 */     }
/* 044 */     rowWriter.write(0, value);
/* 045 */     return result;
/* 046 */   }
/* 047 */ }

13458 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13458 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13458 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13458 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13458 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13458 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13458 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13458 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13459 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13459 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13459 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13459 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13459 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13459 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13459 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

13460 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

13460 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 199 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@3589f0ad
13460 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 2 (262144 bytes)
13461 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13461 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13461 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13461 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13461 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13462 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13462 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13462 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13460 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 198 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@36748f0
13462 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 5 (16777216 bytes)
13462 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 196 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@20bb3e13
13462 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 1 (65536 bytes)
13463 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 196 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@20bb3e13
13463 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Task 196 releasing lock for broadcast_2
13462 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 3 (262144 bytes)
13462 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

13463 [Executor task launch worker-4] INFO  org.apache.spark.internal.Logging$class  - Finished task 194.0 in stage 2.0 (TID 196). 4221 bytes result sent to driver
13463 [Executor task launch worker-3] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13463 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 199 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@2409db39
13464 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 198 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@60f20dca
13464 [dispatcher-event-loop-3] DEBUG org.apache.spark.internal.Logging$class  - parentName: , name: TaskSet_2, runningTasks: 3
13464 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 4 (16777216 bytes)
13464 [dispatcher-event-loop-3] INFO  org.apache.spark.internal.Logging$class  - Starting task 198.0 in stage 2.0 (TID 200, localhost, partition 198, ANY, 5536 bytes)
13464 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 3 (262144 bytes)
13465 [task-result-getter-0] INFO  org.apache.spark.internal.Logging$class  - Finished task 194.0 in stage 2.0 (TID 196) in 73 ms on localhost (195/200)
13464 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13464 [Executor task launch worker-4] INFO  org.apache.spark.internal.Logging$class  - Running task 198.0 in stage 2.0 (TID 200)
13465 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 2)
13465 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13465 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13465 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13465 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 198 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@470069b5
13465 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 5 (16777216 bytes)
13465 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Task 200's epoch is 2
13465 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 2)
13465 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Getting local block broadcast_2
13466 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Task 200 trying to acquire read lock for broadcast_2
13466 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Task 200 acquired read lock for broadcast_2
13465 [Executor task launch worker-0] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13466 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Level for block broadcast_2 is StorageLevel(disk, memory, deserialized, 1 replicas)
13466 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 198 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@36748f0
13466 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 6 (16777216 bytes)
13466 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 2)
13466 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 199 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@17db7c2c
13467 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 4 (16777216 bytes)
13466 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 4 (16777216 bytes)
13467 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 198 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@60f20dca
13467 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (65536 bytes)
13467 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 198 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@60f20dca
13467 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 198 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@61f7a083
13467 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (16777216 bytes)
13468 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 3 (262144 bytes)
13468 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 198 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@36748f0
13468 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 6 (16777216 bytes)
13468 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 198 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@36748f0
13467 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - ShuffleMapTask finished on driver
13468 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 0, partitions 198-199
13468 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
13469 [Executor task launch worker-4] INFO  org.apache.spark.internal.Logging$class  - Getting 1 non-empty blocks out of 1 blocks
13469 [Executor task launch worker-4] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 1 ms
13469 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  1 ms
13469 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13469 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13467 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 199 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@907234e
13470 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 5 (16777216 bytes)
13470 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[1, int, true] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       boolean isNull = i.isNullAt(1);
/* 025 */       int value = isNull ? -1 : (i.getInt(1));
/* 026 */       isNullA = isNull;
/* 027 */       primitiveA = value;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB;
/* 031 */     int primitiveB;
/* 032 */     {
/* 033 */
/* 034 */       boolean isNull = i.isNullAt(1);
/* 035 */       int value = isNull ? -1 : (i.getInt(1));
/* 036 */       isNullB = isNull;
/* 037 */       primitiveB = value;
/* 038 */     }
/* 039 */     if (isNullA && isNullB) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */     return 0;
/* 053 */   }
/* 054 */ }

13470 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 199 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@2409db39
13470 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 6 (16777216 bytes)
13468 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13470 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13471 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13471 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 4 (16777216 bytes)
13471 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 199 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@17db7c2c
13471 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (65536 bytes)
13471 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 199 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@17db7c2c
13471 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 199 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@3589f0ad
13471 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (16777216 bytes)
13472 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 3 (262144 bytes)
13472 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 199 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@2409db39
13472 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 6 (16777216 bytes)
13472 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 199 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@2409db39
13470 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 2)
13471 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[1, int, true] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull1 = i.isNullAt(1);
/* 030 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 031 */     long value = -9223372036854775808L;
/* 032 */     boolean isNull = false;
/* 033 */     if (!isNull1) {
/* 034 */       value = (long) value1;
/* 035 */     }
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */     return result;
/* 042 */   }
/* 043 */ }

13472 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 200 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2cb7f082
13473 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (65536 bytes)
13473 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 1, partitions 198-199
13473 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
13473 [Executor task launch worker-4] INFO  org.apache.spark.internal.Logging$class  - Getting 1 non-empty blocks out of 1 blocks
13473 [Executor task launch worker-4] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 0 ms
13472 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13473 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  0 ms
13473 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13473 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13474 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13474 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[0, int, false] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       int value = i.getInt(0);
/* 025 */       isNullA = false;
/* 026 */       primitiveA = value;
/* 027 */     }
/* 028 */     i = b;
/* 029 */     boolean isNullB;
/* 030 */     int primitiveB;
/* 031 */     {
/* 032 */
/* 033 */       int value = i.getInt(0);
/* 034 */       isNullB = false;
/* 035 */       primitiveB = value;
/* 036 */     }
/* 037 */     if (isNullA && isNullB) {
/* 038 */       // Nothing
/* 039 */     } else if (isNullA) {
/* 040 */       return -1;
/* 041 */     } else if (isNullB) {
/* 042 */       return 1;
/* 043 */     } else {
/* 044 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 045 */       if (comp != 0) {
/* 046 */         return comp;
/* 047 */       }
/* 048 */     }
/* 049 */
/* 050 */     return 0;
/* 051 */   }
/* 052 */ }

13474 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13475 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13474 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13475 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 2)
13475 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 2)
13475 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[0, int, false] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     int value1 = i.getInt(0);
/* 030 */     long value = -9223372036854775808L;
/* 031 */     boolean isNull = false;
/* 032 */     if (!false) {
/* 033 */       value = (long) value1;
/* 034 */     }
/* 035 */     if (isNull) {
/* 036 */       rowWriter.setNullAt(0);
/* 037 */     } else {
/* 038 */       rowWriter.write(0, value);
/* 039 */     }
/* 040 */     return result;
/* 041 */   }
/* 042 */ }

13475 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 200 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@43c08246
13475 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 1 (65536 bytes)
13476 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13476 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13476 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for createexternalrow(input[0, int, true], input[1, int, true], input[2, string, true].toString, StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private MutableRow mutableRow;
/* 009 */   private Object[] values;
/* 010 */   private org.apache.spark.sql.types.StructType schema;
/* 011 */
/* 012 */
/* 013 */   public SpecificSafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     mutableRow = (MutableRow) references[references.length - 1];
/* 016 */
/* 017 */     this.schema = (org.apache.spark.sql.types.StructType) references[0];
/* 018 */   }
/* 019 */
/* 020 */   public java.lang.Object apply(java.lang.Object _i) {
/* 021 */     InternalRow i = (InternalRow) _i;
/* 022 */
/* 023 */     values = new Object[3];
/* 024 */
/* 025 */     boolean isNull1 = i.isNullAt(0);
/* 026 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 027 */     if (isNull1) {
/* 028 */       values[0] = null;
/* 029 */     } else {
/* 030 */       values[0] = value1;
/* 031 */     }
/* 032 */
/* 033 */     boolean isNull2 = i.isNullAt(1);
/* 034 */     int value2 = isNull2 ? -1 : (i.getInt(1));
/* 035 */     if (isNull2) {
/* 036 */       values[1] = null;
/* 037 */     } else {
/* 038 */       values[1] = value2;
/* 039 */     }
/* 040 */
/* 041 */     boolean isNull4 = i.isNullAt(2);
/* 042 */     UTF8String value4 = isNull4 ? null : (i.getUTF8String(2));
/* 043 */
/* 044 */     boolean isNull3 = isNull4;
/* 045 */     final java.lang.String value3 = isNull3 ? null : (java.lang.String) value4.toString();
/* 046 */     isNull3 = value3 == null;
/* 047 */     if (isNull3) {
/* 048 */       values[2] = null;
/* 049 */     } else {
/* 050 */       values[2] = value3;
/* 051 */     }
/* 052 */
/* 053 */     final org.apache.spark.sql.Row value = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values, schema);
/* 054 */     if (false) {
/* 055 */       mutableRow.setNullAt(0);
/* 056 */     } else {
/* 057 */
/* 058 */       mutableRow.update(0, value);
/* 059 */     }
/* 060 */
/* 061 */     return mutableRow;
/* 062 */   }
/* 063 */ }

13477 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - SpecificUnsafeRowJoiner(StructType(StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)), StructType(StructField(value,IntegerType,true))):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeRowJoiner();
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeRowJoiner extends org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowJoiner {
/* 006 */   private byte[] buf = new byte[64];
/* 007 */   private UnsafeRow out = new UnsafeRow(4);
/* 008 */
/* 009 */   public UnsafeRow join(UnsafeRow row1, UnsafeRow row2) {
/* 010 */     // row1: 3 fields, 1 words in bitset
/* 011 */     // row2: 1, 1 words in bitset
/* 012 */     // output: 4 fields, 1 words in bitset
/* 013 */     final int sizeInBytes = row1.getSizeInBytes() + row2.getSizeInBytes() - 8;
/* 014 */     if (sizeInBytes > buf.length) {
/* 015 */       buf = new byte[sizeInBytes];
/* 016 */     }
/* 017 */
/* 018 */     final java.lang.Object obj1 = row1.getBaseObject();
/* 019 */     final long offset1 = row1.getBaseOffset();
/* 020 */     final java.lang.Object obj2 = row2.getBaseObject();
/* 021 */     final long offset2 = row2.getBaseOffset();
/* 022 */
/* 023 */     Platform.putLong(buf, 16, Platform.getLong(obj1, offset1 + 0) | (Platform.getLong(obj2, offset2) << 3));
/* 024 */
/* 025 */     // Copy fixed length data for row1
/* 026 */     Platform.copyMemory(
/* 027 */       obj1, offset1 + 8,
/* 028 */       buf, 24,
/* 029 */       24);
/* 030 */
/* 031 */
/* 032 */     // Copy fixed length data for row2
/* 033 */     Platform.copyMemory(
/* 034 */       obj2, offset2 + 8,
/* 035 */       buf, 48,
/* 036 */       8);
/* 037 */
/* 038 */
/* 039 */     // Copy variable length data for row1
/* 040 */     long numBytesVariableRow1 = row1.getSizeInBytes() - 32;
/* 041 */     Platform.copyMemory(
/* 042 */       obj1, offset1 + 32,
/* 043 */       buf, 56,
/* 044 */       numBytesVariableRow1);
/* 045 */
/* 046 */
/* 047 */     // Copy variable length data for row2
/* 048 */     long numBytesVariableRow2 = row2.getSizeInBytes() - 16;
/* 049 */     Platform.copyMemory(
/* 050 */       obj2, offset2 + 16,
/* 051 */       buf, 56 + numBytesVariableRow1,
/* 052 */       numBytesVariableRow2);
/* 053 */
/* 054 */
/* 055 */
/* 056 */
/* 057 */     Platform.putLong(buf, 40, Platform.getLong(buf, 40) + (8L << 32));
/* 058 */
/* 059 */
/* 060 */
/* 061 */     out.pointTo(buf, sizeInBytes);
/* 062 */
/* 063 */     return out;
/* 064 */   }
/* 065 */ }

13477 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.expressions.codegen.package$ExpressionCanonicalizer$CleanExpressions ===
!input[0, int, true] AS value#47   input[0, int, true]
!+- input[0, int, true]            
                
13477 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 2 iterations.
13477 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - 
=== Result of Batch CleanExpressions ===
!input[0, int, true] AS value#47   input[0, int, true]
!+- input[0, int, true]            
        
13478 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull = i.isNullAt(0);
/* 030 */     int value = isNull ? -1 : (i.getInt(0));
/* 031 */     if (isNull) {
/* 032 */       rowWriter.setNullAt(0);
/* 033 */     } else {
/* 034 */       rowWriter.write(0, value);
/* 035 */     }
/* 036 */     return result;
/* 037 */   }
/* 038 */ }

13478 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13478 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13479 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 5 (16777216 bytes)
13479 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for pmod(hash(input[3, int, true], 42), 200):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     boolean isNull = false;
/* 027 */
/* 028 */     int value1 = 42;
/* 029 */
/* 030 */     boolean isNull2 = i.isNullAt(3);
/* 031 */     int value2 = isNull2 ? -1 : (i.getInt(3));
/* 032 */     if (!isNull2) {
/* 033 */       value1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value2, value1);
/* 034 */     }
/* 035 */
/* 036 */     int value = -1;
/* 037 */
/* 038 */     int remainder = value1 % 200;
/* 039 */     if (remainder < 0) {
/* 040 */       value = (remainder + 200) % 200;
/* 041 */     } else {
/* 042 */       value = remainder;
/* 043 */     }
/* 044 */     rowWriter.write(0, value);
/* 045 */     return result;
/* 046 */   }
/* 047 */ }

13479 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13479 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13480 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13480 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13480 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13480 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13480 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13479 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 2 (262144 bytes)
13481 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 198 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@61f7a083
13481 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (16777216 bytes)
13481 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 198 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@61f7a083
13479 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 197 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@636a7c0b
13481 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

13483 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 200 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@7618234d
13484 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 2 (262144 bytes)
13484 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13484 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13484 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13485 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13485 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13485 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13485 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13485 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13486 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

13486 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 200 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@34da9594
13486 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 3 (262144 bytes)
13483 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 1 (65536 bytes)
13486 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 2 (262144 bytes)
13486 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13487 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 199 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@3589f0ad
13487 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (16777216 bytes)
13487 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 199 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@3589f0ad
13487 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 197 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@636a7c0b
13487 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 200 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2cb7f082
13488 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 4 (16777216 bytes)
13487 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Task 197 releasing lock for broadcast_2
13488 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 200 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@43c08246
13488 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 5 (16777216 bytes)
13488 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Finished task 195.0 in stage 2.0 (TID 197). 4221 bytes result sent to driver
13489 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 200 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@34da9594
13489 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 6 (16777216 bytes)
13489 [dispatcher-event-loop-1] DEBUG org.apache.spark.internal.Logging$class  - parentName: , name: TaskSet_2, runningTasks: 3
13489 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 5 (16777216 bytes)
13489 [dispatcher-event-loop-1] INFO  org.apache.spark.internal.Logging$class  - Starting task 199.0 in stage 2.0 (TID 201, localhost, partition 199, ANY, 5536 bytes)
13489 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 200 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@43c08246
13489 [task-result-getter-1] INFO  org.apache.spark.internal.Logging$class  - Finished task 195.0 in stage 2.0 (TID 197) in 92 ms on localhost (196/200)
13489 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13490 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 2)
13490 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13490 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13490 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13490 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 2)
13490 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 2)
13489 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Running task 199.0 in stage 2.0 (TID 201)
13489 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 1 (65536 bytes)
13490 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 200 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@43c08246
13490 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - ShuffleMapTask finished on driver
13490 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 4 (16777216 bytes)
13491 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Task 201's epoch is 2
13491 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13491 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 2)
13491 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 200 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2cb7f082
13491 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (65536 bytes)
13491 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 200 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2cb7f082
13491 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13491 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13491 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13491 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Getting local block broadcast_2
13492 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 2)
13492 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 2)
13491 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 200 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@7618234d
13492 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (16777216 bytes)
13492 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 3 (262144 bytes)
13492 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 200 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@34da9594
13492 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 6 (16777216 bytes)
13493 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 200 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@34da9594
13492 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Task 201 trying to acquire read lock for broadcast_2
13494 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Task 201 acquired read lock for broadcast_2
13495 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Level for block broadcast_2 is StorageLevel(disk, memory, deserialized, 1 replicas)
13496 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 0, partitions 199-200
13497 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
13497 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Getting 1 non-empty blocks out of 1 blocks
13497 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 0 ms
13497 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  0 ms
13497 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13498 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13498 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[1, int, true] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       boolean isNull = i.isNullAt(1);
/* 025 */       int value = isNull ? -1 : (i.getInt(1));
/* 026 */       isNullA = isNull;
/* 027 */       primitiveA = value;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB;
/* 031 */     int primitiveB;
/* 032 */     {
/* 033 */
/* 034 */       boolean isNull = i.isNullAt(1);
/* 035 */       int value = isNull ? -1 : (i.getInt(1));
/* 036 */       isNullB = isNull;
/* 037 */       primitiveB = value;
/* 038 */     }
/* 039 */     if (isNullA && isNullB) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */     return 0;
/* 053 */   }
/* 054 */ }

13498 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13498 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13499 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[1, int, true] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull1 = i.isNullAt(1);
/* 030 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 031 */     long value = -9223372036854775808L;
/* 032 */     boolean isNull = false;
/* 033 */     if (!isNull1) {
/* 034 */       value = (long) value1;
/* 035 */     }
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */     return result;
/* 042 */   }
/* 043 */ }

13499 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 201 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@3f66ebe4
13499 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (65536 bytes)
13499 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 1, partitions 199-200
13500 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
13500 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Getting 1 non-empty blocks out of 1 blocks
13500 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 0 ms
13500 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  0 ms
13500 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13501 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13501 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[0, int, false] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       int value = i.getInt(0);
/* 025 */       isNullA = false;
/* 026 */       primitiveA = value;
/* 027 */     }
/* 028 */     i = b;
/* 029 */     boolean isNullB;
/* 030 */     int primitiveB;
/* 031 */     {
/* 032 */
/* 033 */       int value = i.getInt(0);
/* 034 */       isNullB = false;
/* 035 */       primitiveB = value;
/* 036 */     }
/* 037 */     if (isNullA && isNullB) {
/* 038 */       // Nothing
/* 039 */     } else if (isNullA) {
/* 040 */       return -1;
/* 041 */     } else if (isNullB) {
/* 042 */       return 1;
/* 043 */     } else {
/* 044 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 045 */       if (comp != 0) {
/* 046 */         return comp;
/* 047 */       }
/* 048 */     }
/* 049 */
/* 050 */     return 0;
/* 051 */   }
/* 052 */ }

13501 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13501 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13502 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[0, int, false] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     int value1 = i.getInt(0);
/* 030 */     long value = -9223372036854775808L;
/* 031 */     boolean isNull = false;
/* 032 */     if (!false) {
/* 033 */       value = (long) value1;
/* 034 */     }
/* 035 */     if (isNull) {
/* 036 */       rowWriter.setNullAt(0);
/* 037 */     } else {
/* 038 */       rowWriter.write(0, value);
/* 039 */     }
/* 040 */     return result;
/* 041 */   }
/* 042 */ }

13502 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 201 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@17fa5a49
13502 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 1 (65536 bytes)
13503 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13503 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13503 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for createexternalrow(input[0, int, true], input[1, int, true], input[2, string, true].toString, StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private MutableRow mutableRow;
/* 009 */   private Object[] values;
/* 010 */   private org.apache.spark.sql.types.StructType schema;
/* 011 */
/* 012 */
/* 013 */   public SpecificSafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     mutableRow = (MutableRow) references[references.length - 1];
/* 016 */
/* 017 */     this.schema = (org.apache.spark.sql.types.StructType) references[0];
/* 018 */   }
/* 019 */
/* 020 */   public java.lang.Object apply(java.lang.Object _i) {
/* 021 */     InternalRow i = (InternalRow) _i;
/* 022 */
/* 023 */     values = new Object[3];
/* 024 */
/* 025 */     boolean isNull1 = i.isNullAt(0);
/* 026 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 027 */     if (isNull1) {
/* 028 */       values[0] = null;
/* 029 */     } else {
/* 030 */       values[0] = value1;
/* 031 */     }
/* 032 */
/* 033 */     boolean isNull2 = i.isNullAt(1);
/* 034 */     int value2 = isNull2 ? -1 : (i.getInt(1));
/* 035 */     if (isNull2) {
/* 036 */       values[1] = null;
/* 037 */     } else {
/* 038 */       values[1] = value2;
/* 039 */     }
/* 040 */
/* 041 */     boolean isNull4 = i.isNullAt(2);
/* 042 */     UTF8String value4 = isNull4 ? null : (i.getUTF8String(2));
/* 043 */
/* 044 */     boolean isNull3 = isNull4;
/* 045 */     final java.lang.String value3 = isNull3 ? null : (java.lang.String) value4.toString();
/* 046 */     isNull3 = value3 == null;
/* 047 */     if (isNull3) {
/* 048 */       values[2] = null;
/* 049 */     } else {
/* 050 */       values[2] = value3;
/* 051 */     }
/* 052 */
/* 053 */     final org.apache.spark.sql.Row value = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values, schema);
/* 054 */     if (false) {
/* 055 */       mutableRow.setNullAt(0);
/* 056 */     } else {
/* 057 */
/* 058 */       mutableRow.update(0, value);
/* 059 */     }
/* 060 */
/* 061 */     return mutableRow;
/* 062 */   }
/* 063 */ }

13504 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - SpecificUnsafeRowJoiner(StructType(StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)), StructType(StructField(value,IntegerType,true))):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeRowJoiner();
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeRowJoiner extends org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowJoiner {
/* 006 */   private byte[] buf = new byte[64];
/* 007 */   private UnsafeRow out = new UnsafeRow(4);
/* 008 */
/* 009 */   public UnsafeRow join(UnsafeRow row1, UnsafeRow row2) {
/* 010 */     // row1: 3 fields, 1 words in bitset
/* 011 */     // row2: 1, 1 words in bitset
/* 012 */     // output: 4 fields, 1 words in bitset
/* 013 */     final int sizeInBytes = row1.getSizeInBytes() + row2.getSizeInBytes() - 8;
/* 014 */     if (sizeInBytes > buf.length) {
/* 015 */       buf = new byte[sizeInBytes];
/* 016 */     }
/* 017 */
/* 018 */     final java.lang.Object obj1 = row1.getBaseObject();
/* 019 */     final long offset1 = row1.getBaseOffset();
/* 020 */     final java.lang.Object obj2 = row2.getBaseObject();
/* 021 */     final long offset2 = row2.getBaseOffset();
/* 022 */
/* 023 */     Platform.putLong(buf, 16, Platform.getLong(obj1, offset1 + 0) | (Platform.getLong(obj2, offset2) << 3));
/* 024 */
/* 025 */     // Copy fixed length data for row1
/* 026 */     Platform.copyMemory(
/* 027 */       obj1, offset1 + 8,
/* 028 */       buf, 24,
/* 029 */       24);
/* 030 */
/* 031 */
/* 032 */     // Copy fixed length data for row2
/* 033 */     Platform.copyMemory(
/* 034 */       obj2, offset2 + 8,
/* 035 */       buf, 48,
/* 036 */       8);
/* 037 */
/* 038 */
/* 039 */     // Copy variable length data for row1
/* 040 */     long numBytesVariableRow1 = row1.getSizeInBytes() - 32;
/* 041 */     Platform.copyMemory(
/* 042 */       obj1, offset1 + 32,
/* 043 */       buf, 56,
/* 044 */       numBytesVariableRow1);
/* 045 */
/* 046 */
/* 047 */     // Copy variable length data for row2
/* 048 */     long numBytesVariableRow2 = row2.getSizeInBytes() - 16;
/* 049 */     Platform.copyMemory(
/* 050 */       obj2, offset2 + 16,
/* 051 */       buf, 56 + numBytesVariableRow1,
/* 052 */       numBytesVariableRow2);
/* 053 */
/* 054 */
/* 055 */
/* 056 */
/* 057 */     Platform.putLong(buf, 40, Platform.getLong(buf, 40) + (8L << 32));
/* 058 */
/* 059 */
/* 060 */
/* 061 */     out.pointTo(buf, sizeInBytes);
/* 062 */
/* 063 */     return out;
/* 064 */   }
/* 065 */ }

13504 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.expressions.codegen.package$ExpressionCanonicalizer$CleanExpressions ===
!input[0, int, true] AS value#47   input[0, int, true]
!+- input[0, int, true]            
                
13504 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 2 iterations.
13505 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - 
=== Result of Batch CleanExpressions ===
!input[0, int, true] AS value#47   input[0, int, true]
!+- input[0, int, true]            
        
13506 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull = i.isNullAt(0);
/* 030 */     int value = isNull ? -1 : (i.getInt(0));
/* 031 */     if (isNull) {
/* 032 */       rowWriter.setNullAt(0);
/* 033 */     } else {
/* 034 */       rowWriter.write(0, value);
/* 035 */     }
/* 036 */     return result;
/* 037 */   }
/* 038 */ }

13506 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13506 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13507 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for pmod(hash(input[3, int, true], 42), 200):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     boolean isNull = false;
/* 027 */
/* 028 */     int value1 = 42;
/* 029 */
/* 030 */     boolean isNull2 = i.isNullAt(3);
/* 031 */     int value2 = isNull2 ? -1 : (i.getInt(3));
/* 032 */     if (!isNull2) {
/* 033 */       value1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value2, value1);
/* 034 */     }
/* 035 */
/* 036 */     int value = -1;
/* 037 */
/* 038 */     int remainder = value1 % 200;
/* 039 */     if (remainder < 0) {
/* 040 */       value = (remainder + 200) % 200;
/* 041 */     } else {
/* 042 */       value = remainder;
/* 043 */     }
/* 044 */     rowWriter.write(0, value);
/* 045 */     return result;
/* 046 */   }
/* 047 */ }

13507 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13507 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13507 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13507 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13508 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13508 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13508 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13509 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

13509 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 201 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@1f0269db
13507 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 5 (16777216 bytes)
13509 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 199 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@907234e
13509 [Executor task launch worker-0] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 1 (65536 bytes)
13509 [Executor task launch worker-0] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 199 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@907234e
13510 [Executor task launch worker-0] TRACE org.apache.spark.internal.Logging$class  - Task 199 releasing lock for broadcast_2
13510 [Executor task launch worker-0] INFO  org.apache.spark.internal.Logging$class  - Finished task 197.0 in stage 2.0 (TID 199). 4221 bytes result sent to driver
13510 [dispatcher-event-loop-4] DEBUG org.apache.spark.internal.Logging$class  - parentName: , name: TaskSet_2, runningTasks: 3
13509 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 2 (262144 bytes)
13510 [task-result-getter-2] INFO  org.apache.spark.internal.Logging$class  - Finished task 197.0 in stage 2.0 (TID 199) in 79 ms on localhost (197/200)
13511 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - ShuffleMapTask finished on driver
13511 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13511 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 2)
13511 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13511 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13511 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13511 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13511 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13512 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13512 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13512 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13512 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13512 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13512 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13511 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 2)
13513 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 2)
13512 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 5 (16777216 bytes)
13513 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 198 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@470069b5
13513 [Executor task launch worker-3] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 1 (65536 bytes)
13513 [Executor task launch worker-3] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 198 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@470069b5
13513 [Executor task launch worker-3] TRACE org.apache.spark.internal.Logging$class  - Task 198 releasing lock for broadcast_2
13513 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

13514 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 201 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@49623c1e
13514 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 3 (262144 bytes)
13514 [Executor task launch worker-3] INFO  org.apache.spark.internal.Logging$class  - Finished task 196.0 in stage 2.0 (TID 198). 4221 bytes result sent to driver
13514 [dispatcher-event-loop-2] DEBUG org.apache.spark.internal.Logging$class  - parentName: , name: TaskSet_2, runningTasks: 2
13514 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

13514 [task-result-getter-3] INFO  org.apache.spark.internal.Logging$class  - Finished task 196.0 in stage 2.0 (TID 198) in 83 ms on localhost (198/200)
13515 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 201 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@3f66ebe4
13515 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - ShuffleMapTask finished on driver
13515 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13515 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 2)
13515 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13515 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13515 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13515 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 2)
13515 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 2)
13515 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 4 (16777216 bytes)
13516 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 201 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@17fa5a49
13516 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 5 (16777216 bytes)
13516 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 201 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@49623c1e
13517 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 6 (16777216 bytes)
13517 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 5 (16777216 bytes)
13517 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 201 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@17fa5a49
13517 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 1 (65536 bytes)
13517 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 201 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@17fa5a49
13517 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 4 (16777216 bytes)
13517 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 201 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@3f66ebe4
13518 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (65536 bytes)
13518 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 201 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@3f66ebe4
13518 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 201 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@1f0269db
13518 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (16777216 bytes)
13518 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 3 (262144 bytes)
13518 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 201 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@49623c1e
13518 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 6 (16777216 bytes)
13518 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 201 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@49623c1e
13522 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 2 (262144 bytes)
13522 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 200 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@7618234d
13522 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (16777216 bytes)
13522 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 200 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@7618234d
13531 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 2 (262144 bytes)
13531 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 201 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@1f0269db
13532 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (16777216 bytes)
13532 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 201 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@1f0269db
13548 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Task 200 releasing lock for broadcast_2
13549 [Executor task launch worker-4] INFO  org.apache.spark.internal.Logging$class  - Finished task 198.0 in stage 2.0 (TID 200). 4221 bytes result sent to driver
13549 [dispatcher-event-loop-6] DEBUG org.apache.spark.internal.Logging$class  - parentName: , name: TaskSet_2, runningTasks: 1
13550 [task-result-getter-0] INFO  org.apache.spark.internal.Logging$class  - Finished task 198.0 in stage 2.0 (TID 200) in 86 ms on localhost (199/200)
13550 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - ShuffleMapTask finished on driver
13550 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13550 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 2)
13550 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13550 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13550 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13551 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 2)
13551 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 2)
13554 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Task 201 releasing lock for broadcast_2
13554 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Finished task 199.0 in stage 2.0 (TID 201). 4221 bytes result sent to driver
13555 [dispatcher-event-loop-5] DEBUG org.apache.spark.internal.Logging$class  - parentName: , name: TaskSet_2, runningTasks: 0
13555 [task-result-getter-1] INFO  org.apache.spark.internal.Logging$class  - Finished task 199.0 in stage 2.0 (TID 201) in 66 ms on localhost (200/200)
13555 [task-result-getter-1] INFO  org.apache.spark.internal.Logging$class  - Removed TaskSet 2.0, whose tasks have all completed, from pool 
13555 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - ShuffleMapTask finished on driver
13555 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - ShuffleMapStage 2 (show at DeduplicationF1.scala:44) finished in 6.902 s
13555 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - looking for newly runnable stages
13555 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - running: Set()
13555 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13555 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - failed: Set()
13556 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - Increasing epoch to 3
13556 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13556 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set()
13556 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 3)
13556 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13556 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 3)
13556 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List()
13556 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - Submitting ResultStage 3 (MapPartitionsRDD[19] at show at DeduplicationF1.scala:44), which has no missing parents
13557 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitMissingTasks(ResultStage 3)
13564 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 trying to put broadcast_3
13564 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 trying to acquire read lock for broadcast_3
13564 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 trying to acquire write lock for broadcast_3
13564 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 acquired write lock for broadcast_3
13565 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - Block broadcast_3 stored as values in memory (estimated size 28.8 KB, free 2004.5 MB)
13565 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - Put block broadcast_3 locally took  1 ms
13565 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 releasing lock for broadcast_3
13565 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - Putting block broadcast_3 without replication took  1 ms
13566 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 trying to put broadcast_3_piece0
13566 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 trying to acquire read lock for broadcast_3_piece0
13566 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 trying to acquire write lock for broadcast_3_piece0
13566 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 acquired write lock for broadcast_3_piece0
13567 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.1 KB, free 2004.5 MB)
13567 [dispatcher-event-loop-3] INFO  org.apache.spark.internal.Logging$class  - Added broadcast_3_piece0 in memory on 192.168.1.4:59832 (size: 12.1 KB, free: 2004.6 MB)
13567 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - Updated info of block broadcast_3_piece0
13567 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - Told master about block broadcast_3_piece0
13568 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - Put block broadcast_3_piece0 locally took  1 ms
13568 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 releasing lock for broadcast_3_piece0
13568 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - Putting block broadcast_3_piece0 without replication took  1 ms
13568 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - Created broadcast 3 from broadcast at DAGScheduler.scala:1012
13569 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[19] at show at DeduplicationF1.scala:44)
13569 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - New pending partitions: Set(0)
13570 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - Adding task set 3.0 with 1 tasks
13570 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - Epoch for TaskSet 3.0: 3
13570 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - Valid locality levels for TaskSet 3.0: ANY
13570 [dispatcher-event-loop-7] DEBUG org.apache.spark.internal.Logging$class  - parentName: , name: TaskSet_3, runningTasks: 0
13571 [dispatcher-event-loop-7] INFO  org.apache.spark.internal.Logging$class  - Starting task 0.0 in stage 3.0 (TID 202, localhost, partition 0, ANY, 5219 bytes)
13572 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13572 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Running task 0.0 in stage 3.0 (TID 202)
13572 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ResultStage 3)
13572 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set()
13572 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13572 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Task 202's epoch is 3
13573 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Getting local block broadcast_3
13573 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Task 202 trying to acquire read lock for broadcast_3
13573 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Task 202 acquired read lock for broadcast_3
13573 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Level for block broadcast_3 is StorageLevel(disk, memory, deserialized, 1 replicas)
13575 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 2, partitions 0-1
13576 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
13576 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Getting 69 non-empty blocks out of 200 blocks
13576 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 1 ms
13580 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  5 ms
13581 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13581 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13581 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[3, int, true] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       boolean isNull = i.isNullAt(3);
/* 025 */       int value = isNull ? -1 : (i.getInt(3));
/* 026 */       isNullA = isNull;
/* 027 */       primitiveA = value;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB;
/* 031 */     int primitiveB;
/* 032 */     {
/* 033 */
/* 034 */       boolean isNull = i.isNullAt(3);
/* 035 */       int value = isNull ? -1 : (i.getInt(3));
/* 036 */       isNullB = isNull;
/* 037 */       primitiveB = value;
/* 038 */     }
/* 039 */     if (isNullA && isNullB) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */     return 0;
/* 053 */   }
/* 054 */ }

13582 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - 
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       boolean isNull = i.isNullAt(3);
/* 025 */       int value = isNull ? -1 : (i.getInt(3));
/* 026 */       isNullA = isNull;
/* 027 */       primitiveA = value;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB;
/* 031 */     int primitiveB;
/* 032 */     {
/* 033 */
/* 034 */       boolean isNull = i.isNullAt(3);
/* 035 */       int value = isNull ? -1 : (i.getInt(3));
/* 036 */       isNullB = isNull;
/* 037 */       primitiveB = value;
/* 038 */     }
/* 039 */     if (isNullA && isNullB) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */     return 0;
/* 053 */   }
/* 054 */ }

13590 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Code generated in 8.593397 ms
13591 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13591 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13592 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[3, int, true] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull1 = i.isNullAt(3);
/* 030 */     int value1 = isNull1 ? -1 : (i.getInt(3));
/* 031 */     long value = -9223372036854775808L;
/* 032 */     boolean isNull = false;
/* 033 */     if (!isNull1) {
/* 034 */       value = (long) value1;
/* 035 */     }
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */     return result;
/* 042 */   }
/* 043 */ }

13592 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull1 = i.isNullAt(3);
/* 030 */     int value1 = isNull1 ? -1 : (i.getInt(3));
/* 031 */     long value = -9223372036854775808L;
/* 032 */     boolean isNull = false;
/* 033 */     if (!isNull1) {
/* 034 */       value = (long) value1;
/* 035 */     }
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */     return result;
/* 042 */   }
/* 043 */ }

13602 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Code generated in 10.23004 ms
13603 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 202 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@7d9cad15
13604 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (65536 bytes)
13605 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 202 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@7d9cad15
13605 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 1 (16777216 bytes)
13615 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13615 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13615 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[3, int, true] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       boolean isNull = i.isNullAt(3);
/* 025 */       int value = isNull ? -1 : (i.getInt(3));
/* 026 */       isNullA = isNull;
/* 027 */       primitiveA = value;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB;
/* 031 */     int primitiveB;
/* 032 */     {
/* 033 */
/* 034 */       boolean isNull = i.isNullAt(3);
/* 035 */       int value = isNull ? -1 : (i.getInt(3));
/* 036 */       isNullB = isNull;
/* 037 */       primitiveB = value;
/* 038 */     }
/* 039 */     if (isNullA && isNullB) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */     return 0;
/* 053 */   }
/* 054 */ }

13616 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13616 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13616 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for input[3, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull = i.isNullAt(3);
/* 030 */     int value = isNull ? -1 : (i.getInt(3));
/* 031 */     if (isNull) {
/* 032 */       rowWriter.setNullAt(0);
/* 033 */     } else {
/* 034 */       rowWriter.write(0, value);
/* 035 */     }
/* 036 */     return result;
/* 037 */   }
/* 038 */ }

13617 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull = i.isNullAt(3);
/* 030 */     int value = isNull ? -1 : (i.getInt(3));
/* 031 */     if (isNull) {
/* 032 */       rowWriter.setNullAt(0);
/* 033 */     } else {
/* 034 */       rowWriter.write(0, value);
/* 035 */     }
/* 036 */     return result;
/* 037 */   }
/* 038 */ }

13624 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Code generated in 7.439166 ms
13626 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13627 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13627 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private MutableRow mutableRow;
/* 009 */
/* 010 */
/* 011 */
/* 012 */   public SpecificSafeProjection(Object[] references) {
/* 013 */     this.references = references;
/* 014 */     mutableRow = (MutableRow) references[references.length - 1];
/* 015 */
/* 016 */   }
/* 017 */
/* 018 */   public java.lang.Object apply(java.lang.Object _i) {
/* 019 */     InternalRow i = (InternalRow) _i;
/* 020 */
/* 021 */     boolean isNull = i.isNullAt(0);
/* 022 */     int value = isNull ? -1 : (i.getInt(0));
/* 023 */     if (isNull) {
/* 024 */       mutableRow.setNullAt(0);
/* 025 */     } else {
/* 026 */
/* 027 */       mutableRow.setInt(0, value);
/* 028 */     }
/* 029 */
/* 030 */     return mutableRow;
/* 031 */   }
/* 032 */ }

13627 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private MutableRow mutableRow;
/* 009 */
/* 010 */
/* 011 */
/* 012 */   public SpecificSafeProjection(Object[] references) {
/* 013 */     this.references = references;
/* 014 */     mutableRow = (MutableRow) references[references.length - 1];
/* 015 */
/* 016 */   }
/* 017 */
/* 018 */   public java.lang.Object apply(java.lang.Object _i) {
/* 019 */     InternalRow i = (InternalRow) _i;
/* 020 */
/* 021 */     boolean isNull = i.isNullAt(0);
/* 022 */     int value = isNull ? -1 : (i.getInt(0));
/* 023 */     if (isNull) {
/* 024 */       mutableRow.setNullAt(0);
/* 025 */     } else {
/* 026 */
/* 027 */       mutableRow.setInt(0, value);
/* 028 */     }
/* 029 */
/* 030 */     return mutableRow;
/* 031 */   }
/* 032 */ }

13632 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Code generated in 4.649193 ms
13633 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13633 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13634 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for createexternalrow(input[0, int, true], input[1, int, true], input[2, string, true].toString, StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private MutableRow mutableRow;
/* 009 */   private Object[] values;
/* 010 */   private org.apache.spark.sql.types.StructType schema;
/* 011 */
/* 012 */
/* 013 */   public SpecificSafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     mutableRow = (MutableRow) references[references.length - 1];
/* 016 */
/* 017 */     this.schema = (org.apache.spark.sql.types.StructType) references[0];
/* 018 */   }
/* 019 */
/* 020 */   public java.lang.Object apply(java.lang.Object _i) {
/* 021 */     InternalRow i = (InternalRow) _i;
/* 022 */
/* 023 */     values = new Object[3];
/* 024 */
/* 025 */     boolean isNull1 = i.isNullAt(0);
/* 026 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 027 */     if (isNull1) {
/* 028 */       values[0] = null;
/* 029 */     } else {
/* 030 */       values[0] = value1;
/* 031 */     }
/* 032 */
/* 033 */     boolean isNull2 = i.isNullAt(1);
/* 034 */     int value2 = isNull2 ? -1 : (i.getInt(1));
/* 035 */     if (isNull2) {
/* 036 */       values[1] = null;
/* 037 */     } else {
/* 038 */       values[1] = value2;
/* 039 */     }
/* 040 */
/* 041 */     boolean isNull4 = i.isNullAt(2);
/* 042 */     UTF8String value4 = isNull4 ? null : (i.getUTF8String(2));
/* 043 */
/* 044 */     boolean isNull3 = isNull4;
/* 045 */     final java.lang.String value3 = isNull3 ? null : (java.lang.String) value4.toString();
/* 046 */     isNull3 = value3 == null;
/* 047 */     if (isNull3) {
/* 048 */       values[2] = null;
/* 049 */     } else {
/* 050 */       values[2] = value3;
/* 051 */     }
/* 052 */
/* 053 */     final org.apache.spark.sql.Row value = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values, schema);
/* 054 */     if (false) {
/* 055 */       mutableRow.setNullAt(0);
/* 056 */     } else {
/* 057 */
/* 058 */       mutableRow.update(0, value);
/* 059 */     }
/* 060 */
/* 061 */     return mutableRow;
/* 062 */   }
/* 063 */ }

13638 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 1 (16777216 bytes)
13638 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 202 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@7d9cad15
13638 [Executor task launch worker-1] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (65536 bytes)
13638 [Executor task launch worker-1] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 202 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@7d9cad15
13638 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Task 202 releasing lock for broadcast_3
13639 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Finished task 0.0 in stage 3.0 (TID 202). 4328 bytes result sent to driver
13639 [dispatcher-event-loop-0] DEBUG org.apache.spark.internal.Logging$class  - parentName: , name: TaskSet_3, runningTasks: 0
13639 [task-result-getter-2] INFO  org.apache.spark.internal.Logging$class  - Finished task 0.0 in stage 3.0 (TID 202) in 69 ms on localhost (1/1)
13639 [task-result-getter-2] INFO  org.apache.spark.internal.Logging$class  - Removed TaskSet 3.0, whose tasks have all completed, from pool 
13639 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - ResultStage 3 (show at DeduplicationF1.scala:44) finished in 0.069 s
13644 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - After removal of stage 2, remaining stages = 3
13644 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - After removal of stage 1, remaining stages = 2
13644 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - After removal of stage 3, remaining stages = 1
13644 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - After removal of stage 0, remaining stages = 0
13645 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13645 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set()
13645 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set()
13645 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13646 [main] INFO  org.apache.spark.internal.Logging$class  - Job 0 finished: show at DeduplicationF1.scala:44, took 8.165170 s
13652 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13652 [main] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13653 [main] DEBUG org.apache.spark.internal.Logging$class  - code for createexternalrow(staticinvoke(class scala.collection.mutable.WrappedArray$, ObjectType(interface scala.collection.Seq), make, mapobjects(MapObjects_loopValue6, MapObjects_loopIsNull7, StringType, lambdavariable(MapObjects_loopValue6, MapObjects_loopIsNull7, StringType).toString, input[0, array<string>, true]).array, true), StructField(value,ArrayType(StringType,true),true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private MutableRow mutableRow;
/* 009 */   private Object[] values;
/* 010 */   private boolean MapObjects_loopIsNull7;
/* 011 */   private UTF8String MapObjects_loopValue6;
/* 012 */   private org.apache.spark.sql.types.StructType schema;
/* 013 */
/* 014 */
/* 015 */   public SpecificSafeProjection(Object[] references) {
/* 016 */     this.references = references;
/* 017 */     mutableRow = (MutableRow) references[references.length - 1];
/* 018 */
/* 019 */
/* 020 */
/* 021 */     this.schema = (org.apache.spark.sql.types.StructType) references[0];
/* 022 */   }
/* 023 */
/* 024 */   public java.lang.Object apply(java.lang.Object _i) {
/* 025 */     InternalRow i = (InternalRow) _i;
/* 026 */
/* 027 */     values = new Object[1];
/* 028 */
/* 029 */     boolean isNull4 = i.isNullAt(0);
/* 030 */     ArrayData value4 = isNull4 ? null : (i.getArray(0));
/* 031 */     ArrayData value3 = null;
/* 032 */
/* 033 */     if (!isNull4) {
/* 034 */
/* 035 */       java.lang.String[] convertedArray = null;
/* 036 */       int dataLength = value4.numElements();
/* 037 */       convertedArray = new java.lang.String[dataLength];
/* 038 */
/* 039 */       int loopIndex = 0;
/* 040 */       while (loopIndex < dataLength) {
/* 041 */         MapObjects_loopValue6 = (UTF8String) (value4.getUTF8String(loopIndex));
/* 042 */         MapObjects_loopIsNull7 = value4.isNullAt(loopIndex);
/* 043 */
/* 044 */
/* 045 */         boolean isNull5 = MapObjects_loopIsNull7;
/* 046 */         final java.lang.String value5 = isNull5 ? null : (java.lang.String) MapObjects_loopValue6.toString();
/* 047 */         isNull5 = value5 == null;
/* 048 */         if (isNull5) {
/* 049 */           convertedArray[loopIndex] = null;
/* 050 */         } else {
/* 051 */           convertedArray[loopIndex] = value5;
/* 052 */         }
/* 053 */
/* 054 */         loopIndex += 1;
/* 055 */       }
/* 056 */
/* 057 */       value3 = new org.apache.spark.sql.catalyst.util.GenericArrayData(convertedArray);
/* 058 */     }
/* 059 */
/* 060 */     boolean isNull2 = isNull4;
/* 061 */     final java.lang.Object value2 = isNull2 ? null : (java.lang.Object) value3.array();
/* 062 */     isNull2 = value2 == null;
/* 063 */     boolean isNull1 = isNull2;
/* 064 */     final scala.collection.Seq value1 = isNull1 ? null : scala.collection.mutable.WrappedArray.make(value2);
/* 065 */     isNull1 = value1 == null;
/* 066 */     if (isNull1) {
/* 067 */       values[0] = null;
/* 068 */     } else {
/* 069 */       values[0] = value1;
/* 070 */     }
/* 071 */
/* 072 */     final org.apache.spark.sql.Row value = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values, schema);
/* 073 */     if (false) {
/* 074 */       mutableRow.setNullAt(0);
/* 075 */     } else {
/* 076 */
/* 077 */       mutableRow.update(0, value);
/* 078 */     }
/* 079 */
/* 080 */     return mutableRow;
/* 081 */   }
/* 082 */ }

13653 [main] DEBUG org.apache.spark.internal.Logging$class  - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private MutableRow mutableRow;
/* 009 */   private Object[] values;
/* 010 */   private boolean MapObjects_loopIsNull7;
/* 011 */   private UTF8String MapObjects_loopValue6;
/* 012 */   private org.apache.spark.sql.types.StructType schema;
/* 013 */
/* 014 */
/* 015 */   public SpecificSafeProjection(Object[] references) {
/* 016 */     this.references = references;
/* 017 */     mutableRow = (MutableRow) references[references.length - 1];
/* 018 */
/* 019 */
/* 020 */
/* 021 */     this.schema = (org.apache.spark.sql.types.StructType) references[0];
/* 022 */   }
/* 023 */
/* 024 */   public java.lang.Object apply(java.lang.Object _i) {
/* 025 */     InternalRow i = (InternalRow) _i;
/* 026 */
/* 027 */     values = new Object[1];
/* 028 */
/* 029 */     boolean isNull4 = i.isNullAt(0);
/* 030 */     ArrayData value4 = isNull4 ? null : (i.getArray(0));
/* 031 */     ArrayData value3 = null;
/* 032 */
/* 033 */     if (!isNull4) {
/* 034 */
/* 035 */       java.lang.String[] convertedArray = null;
/* 036 */       int dataLength = value4.numElements();
/* 037 */       convertedArray = new java.lang.String[dataLength];
/* 038 */
/* 039 */       int loopIndex = 0;
/* 040 */       while (loopIndex < dataLength) {
/* 041 */         MapObjects_loopValue6 = (UTF8String) (value4.getUTF8String(loopIndex));
/* 042 */         MapObjects_loopIsNull7 = value4.isNullAt(loopIndex);
/* 043 */
/* 044 */
/* 045 */         boolean isNull5 = MapObjects_loopIsNull7;
/* 046 */         final java.lang.String value5 = isNull5 ? null : (java.lang.String) MapObjects_loopValue6.toString();
/* 047 */         isNull5 = value5 == null;
/* 048 */         if (isNull5) {
/* 049 */           convertedArray[loopIndex] = null;
/* 050 */         } else {
/* 051 */           convertedArray[loopIndex] = value5;
/* 052 */         }
/* 053 */
/* 054 */         loopIndex += 1;
/* 055 */       }
/* 056 */
/* 057 */       value3 = new org.apache.spark.sql.catalyst.util.GenericArrayData(convertedArray);
/* 058 */     }
/* 059 */
/* 060 */     boolean isNull2 = isNull4;
/* 061 */     final java.lang.Object value2 = isNull2 ? null : (java.lang.Object) value3.array();
/* 062 */     isNull2 = value2 == null;
/* 063 */     boolean isNull1 = isNull2;
/* 064 */     final scala.collection.Seq value1 = isNull1 ? null : scala.collection.mutable.WrappedArray.make(value2);
/* 065 */     isNull1 = value1 == null;
/* 066 */     if (isNull1) {
/* 067 */       values[0] = null;
/* 068 */     } else {
/* 069 */       values[0] = value1;
/* 070 */     }
/* 071 */
/* 072 */     final org.apache.spark.sql.Row value = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values, schema);
/* 073 */     if (false) {
/* 074 */       mutableRow.setNullAt(0);
/* 075 */     } else {
/* 076 */
/* 077 */       mutableRow.update(0, value);
/* 078 */     }
/* 079 */
/* 080 */     return mutableRow;
/* 081 */   }
/* 082 */ }

13665 [main] INFO  org.apache.spark.internal.Logging$class  - Code generated in 11.618953 ms
13676 [main] TRACE org.apache.spark.internal.Logging$class  - Wrapper for org.postgresql.Driver already exists
13677 [main] TRACE org.apache.spark.internal.Logging$class  - Wrapper for org.postgresql.Driver already exists
13695 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Substitution after 1 iterations.
13695 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Substitution has no effect.
13696 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve Relation[tid#55,recid#56,firstname#57,middlename#58,lastname#59,address#60,city#61,state#62,zip#63,pobox#64,pocitystatezip#65,ssn#66,dob#67] JDBCRelation(tb_grounddb)
13696 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Resolution after 1 iterations.
13696 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Resolution has no effect.
13696 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Nondeterministic after 1 iterations.
13696 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Nondeterministic has no effect.
13696 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch UDF after 1 iterations.
13696 [main] TRACE org.apache.spark.internal.Logging$class  - Batch UDF has no effect.
13697 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch FixNullability after 1 iterations.
13697 [main] TRACE org.apache.spark.internal.Logging$class  - Batch FixNullability has no effect.
13697 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Cleanup after 1 iterations.
13697 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Cleanup has no effect.
13698 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Substitution after 1 iterations.
13698 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Substitution has no effect.
13699 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Resolution after 1 iterations.
13699 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Resolution has no effect.
13699 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Nondeterministic after 1 iterations.
13699 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Nondeterministic has no effect.
13699 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch UDF after 1 iterations.
13699 [main] TRACE org.apache.spark.internal.Logging$class  - Batch UDF has no effect.
13699 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch FixNullability after 1 iterations.
13699 [main] TRACE org.apache.spark.internal.Logging$class  - Batch FixNullability has no effect.
13699 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Cleanup after 1 iterations.
13700 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Cleanup has no effect.
13700 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Substitution after 1 iterations.
13700 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Substitution has no effect.
13701 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve LocalRelation <empty>, [tid#55, recid#56, firstname#57, middlename#58, lastname#59, address#60, city#61, state#62, zip#63, pobox#64, pocitystatezip#65, ssn#66, dob#67]
13704 [main] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer ===
!'DeserializeToObject unresolveddeserializer(createexternalrow(getcolumnbyordinal(0, IntegerType), getcolumnbyordinal(1, StringType).toString, getcolumnbyordinal(2, StringType).toString, getcolumnbyordinal(3, StringType).toString, getcolumnbyordinal(4, StringType).toString, getcolumnbyordinal(5, StringType).toString, getcolumnbyordinal(6, StringType).toString, getcolumnbyordinal(7, StringType).toString, getcolumnbyordinal(8, StringType).toString, getcolumnbyordinal(9, StringType).toString, getcolumnbyordinal(10, StringType).toString, getcolumnbyordinal(11, StringType).toString, getcolumnbyordinal(12, StringType).toString, StructField(tid,IntegerType,false), StructField(recid,StringType,true), StructField(firstname,StringType,true), StructField(middlename,StringType,true), StructField(lastname,StringType,true), StructField(address,StringType,true), StructField(city,StringType,true), StructField(state,StringType,true), StructField(zip,StringType,true), StructField(pobox,StringType,true), StructField(pocitystatezip,StringType,true), ... 2 more fields)), obj#81: org.apache.spark.sql.Row   DeserializeToObject createexternalrow(tid#55, recid#56.toString, firstname#57.toString, middlename#58.toString, lastname#59.toString, address#60.toString, city#61.toString, state#62.toString, zip#63.toString, pobox#64.toString, pocitystatezip#65.toString, ssn#66.toString, dob#67.toString, StructField(tid,IntegerType,false), StructField(recid,StringType,true), StructField(firstname,StringType,true), StructField(middlename,StringType,true), StructField(lastname,StringType,true), StructField(address,StringType,true), StructField(city,StringType,true), StructField(state,StringType,true), StructField(zip,StringType,true), StructField(pobox,StringType,true), StructField(pocitystatezip,StringType,true), ... 2 more fields), obj#81: org.apache.spark.sql.Row
 +- LocalRelation <empty>, [tid#55, recid#56, firstname#57, middlename#58, lastname#59, address#60, city#61, state#62, zip#63, pobox#64, pocitystatezip#65, ssn#66, dob#67]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   +- LocalRelation <empty>, [tid#55, recid#56, firstname#57, middlename#58, lastname#59, address#60, city#61, state#62, zip#63, pobox#64, pocitystatezip#65, ssn#66, dob#67]
                
13706 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve LocalRelation <empty>, [tid#55, recid#56, firstname#57, middlename#58, lastname#59, address#60, city#61, state#62, zip#63, pobox#64, pocitystatezip#65, ssn#66, dob#67]
13707 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve DeserializeToObject createexternalrow(tid#55, recid#56.toString, firstname#57.toString, middlename#58.toString, lastname#59.toString, address#60.toString, city#61.toString, state#62.toString, zip#63.toString, pobox#64.toString, pocitystatezip#65.toString, ssn#66.toString, dob#67.toString, StructField(tid,IntegerType,false), StructField(recid,StringType,true), StructField(firstname,StringType,true), StructField(middlename,StringType,true), StructField(lastname,StringType,true), StructField(address,StringType,true), StructField(city,StringType,true), StructField(state,StringType,true), StructField(zip,StringType,true), StructField(pobox,StringType,true), StructField(pocitystatezip,StringType,true), ... 2 more fields), obj#81: org.apache.spark.sql.Row
13709 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Resolution after 2 iterations.
13710 [main] DEBUG org.apache.spark.internal.Logging$class  - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(createexternalrow(getcolumnbyordinal(0, IntegerType), getcolumnbyordinal(1, StringType).toString, getcolumnbyordinal(2, StringType).toString, getcolumnbyordinal(3, StringType).toString, getcolumnbyordinal(4, StringType).toString, getcolumnbyordinal(5, StringType).toString, getcolumnbyordinal(6, StringType).toString, getcolumnbyordinal(7, StringType).toString, getcolumnbyordinal(8, StringType).toString, getcolumnbyordinal(9, StringType).toString, getcolumnbyordinal(10, StringType).toString, getcolumnbyordinal(11, StringType).toString, getcolumnbyordinal(12, StringType).toString, StructField(tid,IntegerType,false), StructField(recid,StringType,true), StructField(firstname,StringType,true), StructField(middlename,StringType,true), StructField(lastname,StringType,true), StructField(address,StringType,true), StructField(city,StringType,true), StructField(state,StringType,true), StructField(zip,StringType,true), StructField(pobox,StringType,true), StructField(pocitystatezip,StringType,true), ... 2 more fields)), obj#81: org.apache.spark.sql.Row   DeserializeToObject createexternalrow(tid#55, recid#56.toString, firstname#57.toString, middlename#58.toString, lastname#59.toString, address#60.toString, city#61.toString, state#62.toString, zip#63.toString, pobox#64.toString, pocitystatezip#65.toString, ssn#66.toString, dob#67.toString, StructField(tid,IntegerType,false), StructField(recid,StringType,true), StructField(firstname,StringType,true), StructField(middlename,StringType,true), StructField(lastname,StringType,true), StructField(address,StringType,true), StructField(city,StringType,true), StructField(state,StringType,true), StructField(zip,StringType,true), StructField(pobox,StringType,true), StructField(pocitystatezip,StringType,true), ... 2 more fields), obj#81: org.apache.spark.sql.Row
 +- LocalRelation <empty>, [tid#55, recid#56, firstname#57, middlename#58, lastname#59, address#60, city#61, state#62, zip#63, pobox#64, pocitystatezip#65, ssn#66, dob#67]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   +- LocalRelation <empty>, [tid#55, recid#56, firstname#57, middlename#58, lastname#59, address#60, city#61, state#62, zip#63, pobox#64, pocitystatezip#65, ssn#66, dob#67]
        
13710 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Nondeterministic after 1 iterations.
13710 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Nondeterministic has no effect.
13710 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch UDF after 1 iterations.
13711 [main] TRACE org.apache.spark.internal.Logging$class  - Batch UDF has no effect.
13711 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch FixNullability after 1 iterations.
13711 [main] TRACE org.apache.spark.internal.Logging$class  - Batch FixNullability has no effect.
13711 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Cleanup after 1 iterations.
13711 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Cleanup has no effect.
13713 [main] INFO  org.apache.spark.internal.Logging$class  - Parsing command: tb_grounddb
13714 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Substitution after 1 iterations.
13714 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Substitution has no effect.
13714 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve CreateViewCommand CatalogTable(
	Table: `tb_grounddb`
	Created: Wed Dec 07 15:37:03 CET 2016
	Last Access: Thu Jan 01 00:59:59 CET 1970
	Type: VIEW
	Storage()), false, true, true
13715 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Resolution after 1 iterations.
13715 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Resolution has no effect.
13715 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Nondeterministic after 1 iterations.
13715 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Nondeterministic has no effect.
13715 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch UDF after 1 iterations.
13715 [main] TRACE org.apache.spark.internal.Logging$class  - Batch UDF has no effect.
13715 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch FixNullability after 1 iterations.
13715 [main] TRACE org.apache.spark.internal.Logging$class  - Batch FixNullability has no effect.
13715 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Cleanup after 1 iterations.
13715 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Cleanup has no effect.
13716 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Substitution after 1 iterations.
13716 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Substitution has no effect.
13716 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Resolution after 1 iterations.
13716 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Resolution has no effect.
13716 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Nondeterministic after 1 iterations.
13716 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Nondeterministic has no effect.
13716 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch UDF after 1 iterations.
13716 [main] TRACE org.apache.spark.internal.Logging$class  - Batch UDF has no effect.
13716 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch FixNullability after 1 iterations.
13717 [main] TRACE org.apache.spark.internal.Logging$class  - Batch FixNullability has no effect.
13717 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Cleanup after 1 iterations.
13717 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Cleanup has no effect.
13717 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Finish Analysis after 1 iterations.
13717 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Finish Analysis has no effect.
13717 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Union after 1 iterations.
13717 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Union has no effect.
13717 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Subquery after 1 iterations.
13717 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Subquery has no effect.
13717 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Replace Operators after 1 iterations.
13718 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Replace Operators has no effect.
13718 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Aggregate after 1 iterations.
13718 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Aggregate has no effect.
13718 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Operator Optimizations after 1 iterations.
13718 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Operator Optimizations has no effect.
13718 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Decimal Optimizations after 1 iterations.
13718 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Decimal Optimizations has no effect.
13718 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Typed Filter Optimization after 1 iterations.
13718 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Typed Filter Optimization has no effect.
13719 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch LocalRelation after 1 iterations.
13719 [main] TRACE org.apache.spark.internal.Logging$class  - Batch LocalRelation has no effect.
13719 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch OptimizeCodegen after 1 iterations.
13719 [main] TRACE org.apache.spark.internal.Logging$class  - Batch OptimizeCodegen has no effect.
13719 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch RewriteSubquery after 1 iterations.
13719 [main] TRACE org.apache.spark.internal.Logging$class  - Batch RewriteSubquery has no effect.
13719 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Extract Python UDF from Aggregate after 1 iterations.
13719 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Extract Python UDF from Aggregate has no effect.
13719 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch User Provided Optimizers after 1 iterations.
13719 [main] TRACE org.apache.spark.internal.Logging$class  - Batch User Provided Optimizers has no effect.
13720 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Substitution after 1 iterations.
13720 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Substitution has no effect.
13720 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Resolution after 1 iterations.
13720 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Resolution has no effect.
13720 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Nondeterministic after 1 iterations.
13720 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Nondeterministic has no effect.
13720 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch UDF after 1 iterations.
13720 [main] TRACE org.apache.spark.internal.Logging$class  - Batch UDF has no effect.
13720 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch FixNullability after 1 iterations.
13720 [main] TRACE org.apache.spark.internal.Logging$class  - Batch FixNullability has no effect.
13721 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Cleanup after 1 iterations.
13721 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Cleanup has no effect.
13721 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Substitution after 1 iterations.
13721 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Substitution has no effect.
13721 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve LocalRelation <empty>
13722 [main] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer ===
!'DeserializeToObject unresolveddeserializer(createexternalrow()), obj#82: org.apache.spark.sql.Row   DeserializeToObject createexternalrow(), obj#82: org.apache.spark.sql.Row
 +- LocalRelation <empty>                                                                             +- LocalRelation <empty>
                
13722 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve LocalRelation <empty>
13722 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve DeserializeToObject createexternalrow(), obj#82: org.apache.spark.sql.Row
13723 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Resolution after 2 iterations.
13723 [main] DEBUG org.apache.spark.internal.Logging$class  - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(createexternalrow()), obj#82: org.apache.spark.sql.Row   DeserializeToObject createexternalrow(), obj#82: org.apache.spark.sql.Row
 +- LocalRelation <empty>                                                                             +- LocalRelation <empty>
        
13723 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Nondeterministic after 1 iterations.
13723 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Nondeterministic has no effect.
13723 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch UDF after 1 iterations.
13724 [main] TRACE org.apache.spark.internal.Logging$class  - Batch UDF has no effect.
13724 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch FixNullability after 1 iterations.
13724 [main] TRACE org.apache.spark.internal.Logging$class  - Batch FixNullability has no effect.
13724 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Cleanup after 1 iterations.
13724 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Cleanup has no effect.
13724 [main] INFO  org.apache.spark.internal.Logging$class  - Parsing command: 
SELECT v.vid, v.tupleid, i.recid
FROM violation as v
  JOIN tb_grounddb as i on v.tupleid = i.tid
WHERE v.rid='dedupBlackOakGold'
GROUP BY v.vid, v.tupleid, i.recid
         
13727 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Substitution after 1 iterations.
13728 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Substitution has no effect.
13729 [main] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations ===
 'Aggregate ['v.vid, 'v.tupleid, 'i.recid], ['v.vid, 'v.tupleid, 'i.recid]   'Aggregate ['v.vid, 'v.tupleid, 'i.recid], ['v.vid, 'v.tupleid, 'i.recid]
 +- 'Filter ('v.rid = dedupBlackOakGold)                                     +- 'Filter ('v.rid = dedupBlackOakGold)
    +- 'Join Inner, ('v.tupleid = 'i.tid)                                       +- 'Join Inner, ('v.tupleid = 'i.tid)
!      :- 'UnresolvedRelation `violation`, v                                       :- SubqueryAlias v
!      +- 'UnresolvedRelation `tb_grounddb`, i                                     :  +- SubqueryAlias violation
!                                                                                  :     +- Relation[vid#0,rid#1,tablename#2,tupleid#3,attribute#4,value#5] JDBCRelation(violation)
!                                                                                  +- SubqueryAlias i
!                                                                                     +- SubqueryAlias tb_grounddb
!                                                                                        +- Relation[tid#55,recid#56,firstname#57,middlename#58,lastname#59,address#60,city#61,state#62,zip#63,pobox#64,pocitystatezip#65,ssn#66,dob#67] JDBCRelation(tb_grounddb)
                
13729 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve SubqueryAlias violation
13729 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve SubqueryAlias v
13730 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve SubqueryAlias tb_grounddb
13730 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve SubqueryAlias i
13730 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve 'Join Inner, ('v.tupleid = 'i.tid)
13731 [main] DEBUG org.apache.spark.internal.Logging$class  - Resolving 'v.tupleid to tupleid#3
13731 [main] DEBUG org.apache.spark.internal.Logging$class  - Resolving 'i.tid to tid#55
13731 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve 'Filter ('v.rid = dedupBlackOakGold)
13732 [main] DEBUG org.apache.spark.internal.Logging$class  - Resolving 'v.rid to rid#1
13732 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve 'Aggregate ['v.vid, 'v.tupleid, 'i.recid], ['v.vid, 'v.tupleid, 'i.recid]
13733 [main] DEBUG org.apache.spark.internal.Logging$class  - Resolving 'v.vid to vid#0
13733 [main] DEBUG org.apache.spark.internal.Logging$class  - Resolving 'v.tupleid to tupleid#3
13733 [main] DEBUG org.apache.spark.internal.Logging$class  - Resolving 'i.recid to recid#56
13734 [main] DEBUG org.apache.spark.internal.Logging$class  - Resolving 'v.vid to vid#0
13734 [main] DEBUG org.apache.spark.internal.Logging$class  - Resolving 'v.tupleid to tupleid#3
13734 [main] DEBUG org.apache.spark.internal.Logging$class  - Resolving 'i.recid to recid#56
13737 [main] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences ===
!'Aggregate ['v.vid, 'v.tupleid, 'i.recid], ['v.vid, 'v.tupleid, 'i.recid]                                                                                                               Aggregate [vid#0, tupleid#3, recid#56], [vid#0, tupleid#3, recid#56]
!+- 'Filter ('v.rid = dedupBlackOakGold)                                                                                                                                                 +- Filter (rid#1 = dedupBlackOakGold)
!   +- 'Join Inner, ('v.tupleid = 'i.tid)                                                                                                                                                   +- Join Inner, (tupleid#3 = tid#55)
       :- SubqueryAlias v                                                                                                                                                                      :- SubqueryAlias v
       :  +- SubqueryAlias violation                                                                                                                                                           :  +- SubqueryAlias violation
       :     +- Relation[vid#0,rid#1,tablename#2,tupleid#3,attribute#4,value#5] JDBCRelation(violation)                                                                                        :     +- Relation[vid#0,rid#1,tablename#2,tupleid#3,attribute#4,value#5] JDBCRelation(violation)
       +- SubqueryAlias i                                                                                                                                                                      +- SubqueryAlias i
          +- SubqueryAlias tb_grounddb                                                                                                                                                            +- SubqueryAlias tb_grounddb
             +- Relation[tid#55,recid#56,firstname#57,middlename#58,lastname#59,address#60,city#61,state#62,zip#63,pobox#64,pocitystatezip#65,ssn#66,dob#67] JDBCRelation(tb_grounddb)               +- Relation[tid#55,recid#56,firstname#57,middlename#58,lastname#59,address#60,city#61,state#62,zip#63,pobox#64,pocitystatezip#65,ssn#66,dob#67] JDBCRelation(tb_grounddb)
                
13740 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve SubqueryAlias violation
13741 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve SubqueryAlias v
13741 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve SubqueryAlias tb_grounddb
13741 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve SubqueryAlias i
13742 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve Join Inner, (tupleid#3 = tid#55)
13742 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve Filter (rid#1 = dedupBlackOakGold)
13742 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve Aggregate [vid#0, tupleid#3, recid#56], [vid#0, tupleid#3, recid#56]
13744 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Resolution after 2 iterations.
13745 [main] DEBUG org.apache.spark.internal.Logging$class  - 
=== Result of Batch Resolution ===
!'Aggregate ['v.vid, 'v.tupleid, 'i.recid], ['v.vid, 'v.tupleid, 'i.recid]   Aggregate [vid#0, tupleid#3, recid#56], [vid#0, tupleid#3, recid#56]
!+- 'Filter ('v.rid = dedupBlackOakGold)                                     +- Filter (rid#1 = dedupBlackOakGold)
!   +- 'Join Inner, ('v.tupleid = 'i.tid)                                       +- Join Inner, (tupleid#3 = tid#55)
!      :- 'UnresolvedRelation `violation`, v                                       :- SubqueryAlias v
!      +- 'UnresolvedRelation `tb_grounddb`, i                                     :  +- SubqueryAlias violation
!                                                                                  :     +- Relation[vid#0,rid#1,tablename#2,tupleid#3,attribute#4,value#5] JDBCRelation(violation)
!                                                                                  +- SubqueryAlias i
!                                                                                     +- SubqueryAlias tb_grounddb
!                                                                                        +- Relation[tid#55,recid#56,firstname#57,middlename#58,lastname#59,address#60,city#61,state#62,zip#63,pobox#64,pocitystatezip#65,ssn#66,dob#67] JDBCRelation(tb_grounddb)
        
13745 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Nondeterministic after 1 iterations.
13745 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Nondeterministic has no effect.
13746 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch UDF after 1 iterations.
13746 [main] TRACE org.apache.spark.internal.Logging$class  - Batch UDF has no effect.
13746 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch FixNullability after 1 iterations.
13747 [main] TRACE org.apache.spark.internal.Logging$class  - Batch FixNullability has no effect.
13747 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Cleanup after 1 iterations.
13747 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Cleanup has no effect.
13748 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Substitution after 1 iterations.
13748 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Substitution has no effect.
13749 [main] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations ===
 'Aggregate ['v.vid, 'v.tupleid, 'i.recid], ['v.vid, 'v.tupleid, 'i.recid]   'Aggregate ['v.vid, 'v.tupleid, 'i.recid], ['v.vid, 'v.tupleid, 'i.recid]
 +- 'Filter ('v.rid = dedupBlackOakGold)                                     +- 'Filter ('v.rid = dedupBlackOakGold)
    +- 'Join Inner, ('v.tupleid = 'i.tid)                                       +- 'Join Inner, ('v.tupleid = 'i.tid)
!      :- 'UnresolvedRelation `violation`, v                                       :- SubqueryAlias v
!      +- 'UnresolvedRelation `tb_grounddb`, i                                     :  +- SubqueryAlias violation
!                                                                                  :     +- Relation[vid#0,rid#1,tablename#2,tupleid#3,attribute#4,value#5] JDBCRelation(violation)
!                                                                                  +- SubqueryAlias i
!                                                                                     +- SubqueryAlias tb_grounddb
!                                                                                        +- Relation[tid#55,recid#56,firstname#57,middlename#58,lastname#59,address#60,city#61,state#62,zip#63,pobox#64,pocitystatezip#65,ssn#66,dob#67] JDBCRelation(tb_grounddb)
                
13749 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve SubqueryAlias violation
13749 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve SubqueryAlias v
13749 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve SubqueryAlias tb_grounddb
13750 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve SubqueryAlias i
13750 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve 'Join Inner, ('v.tupleid = 'i.tid)
13750 [main] DEBUG org.apache.spark.internal.Logging$class  - Resolving 'v.tupleid to tupleid#3
13750 [main] DEBUG org.apache.spark.internal.Logging$class  - Resolving 'i.tid to tid#55
13751 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve 'Filter ('v.rid = dedupBlackOakGold)
13751 [main] DEBUG org.apache.spark.internal.Logging$class  - Resolving 'v.rid to rid#1
13751 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve 'Aggregate ['v.vid, 'v.tupleid, 'i.recid], ['v.vid, 'v.tupleid, 'i.recid]
13752 [main] DEBUG org.apache.spark.internal.Logging$class  - Resolving 'v.vid to vid#0
13752 [main] DEBUG org.apache.spark.internal.Logging$class  - Resolving 'v.tupleid to tupleid#3
13752 [main] DEBUG org.apache.spark.internal.Logging$class  - Resolving 'i.recid to recid#56
13752 [main] DEBUG org.apache.spark.internal.Logging$class  - Resolving 'v.vid to vid#0
13752 [main] DEBUG org.apache.spark.internal.Logging$class  - Resolving 'v.tupleid to tupleid#3
13753 [main] DEBUG org.apache.spark.internal.Logging$class  - Resolving 'i.recid to recid#56
13754 [main] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences ===
!'Aggregate ['v.vid, 'v.tupleid, 'i.recid], ['v.vid, 'v.tupleid, 'i.recid]                                                                                                               Aggregate [vid#0, tupleid#3, recid#56], [vid#0, tupleid#3, recid#56]
!+- 'Filter ('v.rid = dedupBlackOakGold)                                                                                                                                                 +- Filter (rid#1 = dedupBlackOakGold)
!   +- 'Join Inner, ('v.tupleid = 'i.tid)                                                                                                                                                   +- Join Inner, (tupleid#3 = tid#55)
       :- SubqueryAlias v                                                                                                                                                                      :- SubqueryAlias v
       :  +- SubqueryAlias violation                                                                                                                                                           :  +- SubqueryAlias violation
       :     +- Relation[vid#0,rid#1,tablename#2,tupleid#3,attribute#4,value#5] JDBCRelation(violation)                                                                                        :     +- Relation[vid#0,rid#1,tablename#2,tupleid#3,attribute#4,value#5] JDBCRelation(violation)
       +- SubqueryAlias i                                                                                                                                                                      +- SubqueryAlias i
          +- SubqueryAlias tb_grounddb                                                                                                                                                            +- SubqueryAlias tb_grounddb
             +- Relation[tid#55,recid#56,firstname#57,middlename#58,lastname#59,address#60,city#61,state#62,zip#63,pobox#64,pocitystatezip#65,ssn#66,dob#67] JDBCRelation(tb_grounddb)               +- Relation[tid#55,recid#56,firstname#57,middlename#58,lastname#59,address#60,city#61,state#62,zip#63,pobox#64,pocitystatezip#65,ssn#66,dob#67] JDBCRelation(tb_grounddb)
                
13757 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve SubqueryAlias violation
13757 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve SubqueryAlias v
13757 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve SubqueryAlias tb_grounddb
13758 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve SubqueryAlias i
13758 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve Join Inner, (tupleid#3 = tid#55)
13758 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve Filter (rid#1 = dedupBlackOakGold)
13759 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve Aggregate [vid#0, tupleid#3, recid#56], [vid#0, tupleid#3, recid#56]
13761 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Resolution after 2 iterations.
13762 [main] DEBUG org.apache.spark.internal.Logging$class  - 
=== Result of Batch Resolution ===
!'Aggregate ['v.vid, 'v.tupleid, 'i.recid], ['v.vid, 'v.tupleid, 'i.recid]   Aggregate [vid#0, tupleid#3, recid#56], [vid#0, tupleid#3, recid#56]
!+- 'Filter ('v.rid = dedupBlackOakGold)                                     +- Filter (rid#1 = dedupBlackOakGold)
!   +- 'Join Inner, ('v.tupleid = 'i.tid)                                       +- Join Inner, (tupleid#3 = tid#55)
!      :- 'UnresolvedRelation `violation`, v                                       :- SubqueryAlias v
!      +- 'UnresolvedRelation `tb_grounddb`, i                                     :  +- SubqueryAlias violation
!                                                                                  :     +- Relation[vid#0,rid#1,tablename#2,tupleid#3,attribute#4,value#5] JDBCRelation(violation)
!                                                                                  +- SubqueryAlias i
!                                                                                     +- SubqueryAlias tb_grounddb
!                                                                                        +- Relation[tid#55,recid#56,firstname#57,middlename#58,lastname#59,address#60,city#61,state#62,zip#63,pobox#64,pocitystatezip#65,ssn#66,dob#67] JDBCRelation(tb_grounddb)
        
13762 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Nondeterministic after 1 iterations.
13762 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Nondeterministic has no effect.
13763 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch UDF after 1 iterations.
13763 [main] TRACE org.apache.spark.internal.Logging$class  - Batch UDF has no effect.
13764 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch FixNullability after 1 iterations.
13764 [main] TRACE org.apache.spark.internal.Logging$class  - Batch FixNullability has no effect.
13764 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Cleanup after 1 iterations.
13764 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Cleanup has no effect.
13765 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Substitution after 1 iterations.
13765 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Substitution has no effect.
13765 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve LocalRelation <empty>, [vid#0, tupleid#3, recid#56]
13766 [main] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer ===
!'DeserializeToObject unresolveddeserializer(createexternalrow(getcolumnbyordinal(0, IntegerType), getcolumnbyordinal(1, IntegerType), getcolumnbyordinal(2, StringType).toString, StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true))), obj#86: org.apache.spark.sql.Row   DeserializeToObject createexternalrow(vid#0, tupleid#3, recid#56.toString, StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)), obj#86: org.apache.spark.sql.Row
 +- LocalRelation <empty>, [vid#0, tupleid#3, recid#56]                                                                                                                                                                                                                                                                               +- LocalRelation <empty>, [vid#0, tupleid#3, recid#56]
                
13767 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve LocalRelation <empty>, [vid#0, tupleid#3, recid#56]
13767 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve DeserializeToObject createexternalrow(vid#0, tupleid#3, recid#56.toString, StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)), obj#86: org.apache.spark.sql.Row
13768 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Resolution after 2 iterations.
13769 [main] DEBUG org.apache.spark.internal.Logging$class  - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(createexternalrow(getcolumnbyordinal(0, IntegerType), getcolumnbyordinal(1, IntegerType), getcolumnbyordinal(2, StringType).toString, StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true))), obj#86: org.apache.spark.sql.Row   DeserializeToObject createexternalrow(vid#0, tupleid#3, recid#56.toString, StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)), obj#86: org.apache.spark.sql.Row
 +- LocalRelation <empty>, [vid#0, tupleid#3, recid#56]                                                                                                                                                                                                                                                                               +- LocalRelation <empty>, [vid#0, tupleid#3, recid#56]
        
13769 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Nondeterministic after 1 iterations.
13769 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Nondeterministic has no effect.
13769 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch UDF after 1 iterations.
13769 [main] TRACE org.apache.spark.internal.Logging$class  - Batch UDF has no effect.
13769 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch FixNullability after 1 iterations.
13769 [main] TRACE org.apache.spark.internal.Logging$class  - Batch FixNullability has no effect.
13769 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Cleanup after 1 iterations.
13769 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Cleanup has no effect.
13771 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Substitution after 1 iterations.
13771 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Substitution has no effect.
13771 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve LocalRelation <empty>, [vid#0, tupleid#3, recid#56]
13772 [main] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer ===
!'DeserializeToObject unresolveddeserializer(createexternalrow(getcolumnbyordinal(0, IntegerType), getcolumnbyordinal(1, IntegerType), getcolumnbyordinal(2, StringType).toString, StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true))), obj#90: org.apache.spark.sql.Row   DeserializeToObject createexternalrow(vid#0, tupleid#3, recid#56.toString, StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)), obj#90: org.apache.spark.sql.Row
 +- LocalRelation <empty>, [vid#0, tupleid#3, recid#56]                                                                                                                                                                                                                                                                               +- LocalRelation <empty>, [vid#0, tupleid#3, recid#56]
                
13772 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve LocalRelation <empty>, [vid#0, tupleid#3, recid#56]
13773 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve DeserializeToObject createexternalrow(vid#0, tupleid#3, recid#56.toString, StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)), obj#90: org.apache.spark.sql.Row
13773 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Resolution after 2 iterations.
13774 [main] DEBUG org.apache.spark.internal.Logging$class  - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(createexternalrow(getcolumnbyordinal(0, IntegerType), getcolumnbyordinal(1, IntegerType), getcolumnbyordinal(2, StringType).toString, StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true))), obj#90: org.apache.spark.sql.Row   DeserializeToObject createexternalrow(vid#0, tupleid#3, recid#56.toString, StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)), obj#90: org.apache.spark.sql.Row
 +- LocalRelation <empty>, [vid#0, tupleid#3, recid#56]                                                                                                                                                                                                                                                                               +- LocalRelation <empty>, [vid#0, tupleid#3, recid#56]
        
13774 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Nondeterministic after 1 iterations.
13774 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Nondeterministic has no effect.
13774 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch UDF after 1 iterations.
13774 [main] TRACE org.apache.spark.internal.Logging$class  - Batch UDF has no effect.
13774 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch FixNullability after 1 iterations.
13774 [main] TRACE org.apache.spark.internal.Logging$class  - Batch FixNullability has no effect.
13775 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Cleanup after 1 iterations.
13775 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Cleanup has no effect.
13775 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Substitution after 1 iterations.
13776 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Substitution has no effect.
13776 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve LocalLimit 7
13776 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve GlobalLimit 7
13777 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Resolution after 1 iterations.
13777 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Resolution has no effect.
13777 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Nondeterministic after 1 iterations.
13777 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Nondeterministic has no effect.
13777 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch UDF after 1 iterations.
13777 [main] TRACE org.apache.spark.internal.Logging$class  - Batch UDF has no effect.
13778 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch FixNullability after 1 iterations.
13778 [main] TRACE org.apache.spark.internal.Logging$class  - Batch FixNullability has no effect.
13778 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Cleanup after 1 iterations.
13779 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Cleanup has no effect.
13779 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Substitution after 1 iterations.
13779 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Substitution has no effect.
13779 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve LocalRelation <empty>, [vid#0, tupleid#3, recid#56]
13780 [main] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer ===
!'DeserializeToObject unresolveddeserializer(createexternalrow(getcolumnbyordinal(0, IntegerType), getcolumnbyordinal(1, IntegerType), getcolumnbyordinal(2, StringType).toString, StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true))), obj#91: org.apache.spark.sql.Row   DeserializeToObject createexternalrow(vid#0, tupleid#3, recid#56.toString, StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)), obj#91: org.apache.spark.sql.Row
 +- LocalRelation <empty>, [vid#0, tupleid#3, recid#56]                                                                                                                                                                                                                                                                               +- LocalRelation <empty>, [vid#0, tupleid#3, recid#56]
                
13781 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve LocalRelation <empty>, [vid#0, tupleid#3, recid#56]
13781 [main] TRACE org.apache.spark.internal.Logging$class  - Attempting to resolve DeserializeToObject createexternalrow(vid#0, tupleid#3, recid#56.toString, StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)), obj#91: org.apache.spark.sql.Row
13782 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Resolution after 2 iterations.
13782 [main] DEBUG org.apache.spark.internal.Logging$class  - 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(createexternalrow(getcolumnbyordinal(0, IntegerType), getcolumnbyordinal(1, IntegerType), getcolumnbyordinal(2, StringType).toString, StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true))), obj#91: org.apache.spark.sql.Row   DeserializeToObject createexternalrow(vid#0, tupleid#3, recid#56.toString, StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)), obj#91: org.apache.spark.sql.Row
 +- LocalRelation <empty>, [vid#0, tupleid#3, recid#56]                                                                                                                                                                                                                                                                               +- LocalRelation <empty>, [vid#0, tupleid#3, recid#56]
        
13783 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Nondeterministic after 1 iterations.
13783 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Nondeterministic has no effect.
13783 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch UDF after 1 iterations.
13783 [main] TRACE org.apache.spark.internal.Logging$class  - Batch UDF has no effect.
13783 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch FixNullability after 1 iterations.
13783 [main] TRACE org.apache.spark.internal.Logging$class  - Batch FixNullability has no effect.
13783 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Cleanup after 1 iterations.
13783 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Cleanup has no effect.
13785 [main] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.EliminateSubqueryAliases ===
 GlobalLimit 7                                                                                                                                                                                 GlobalLimit 7
 +- LocalLimit 7                                                                                                                                                                               +- LocalLimit 7
    +- Aggregate [vid#0, tupleid#3, recid#56], [vid#0, tupleid#3, recid#56]                                                                                                                       +- Aggregate [vid#0, tupleid#3, recid#56], [vid#0, tupleid#3, recid#56]
       +- Filter (rid#1 = dedupBlackOakGold)                                                                                                                                                         +- Filter (rid#1 = dedupBlackOakGold)
          +- Join Inner, (tupleid#3 = tid#55)                                                                                                                                                           +- Join Inner, (tupleid#3 = tid#55)
!            :- SubqueryAlias v                                                                                                                                                                            :- Relation[vid#0,rid#1,tablename#2,tupleid#3,attribute#4,value#5] JDBCRelation(violation)
!            :  +- SubqueryAlias violation                                                                                                                                                                 +- Relation[tid#55,recid#56,firstname#57,middlename#58,lastname#59,address#60,city#61,state#62,zip#63,pobox#64,pocitystatezip#65,ssn#66,dob#67] JDBCRelation(tb_grounddb)
!            :     +- Relation[vid#0,rid#1,tablename#2,tupleid#3,attribute#4,value#5] JDBCRelation(violation)                                                                                  
!            +- SubqueryAlias i                                                                                                                                                                
!               +- SubqueryAlias tb_grounddb                                                                                                                                                   
!                  +- Relation[tid#55,recid#56,firstname#57,middlename#58,lastname#59,address#60,city#61,state#62,zip#63,pobox#64,pocitystatezip#65,ssn#66,dob#67] JDBCRelation(tb_grounddb)   
                
13786 [main] DEBUG org.apache.spark.internal.Logging$class  - 
=== Result of Batch Finish Analysis ===
 GlobalLimit 7                                                                                                                                                                                 GlobalLimit 7
 +- LocalLimit 7                                                                                                                                                                               +- LocalLimit 7
    +- Aggregate [vid#0, tupleid#3, recid#56], [vid#0, tupleid#3, recid#56]                                                                                                                       +- Aggregate [vid#0, tupleid#3, recid#56], [vid#0, tupleid#3, recid#56]
       +- Filter (rid#1 = dedupBlackOakGold)                                                                                                                                                         +- Filter (rid#1 = dedupBlackOakGold)
          +- Join Inner, (tupleid#3 = tid#55)                                                                                                                                                           +- Join Inner, (tupleid#3 = tid#55)
!            :- SubqueryAlias v                                                                                                                                                                            :- Relation[vid#0,rid#1,tablename#2,tupleid#3,attribute#4,value#5] JDBCRelation(violation)
!            :  +- SubqueryAlias violation                                                                                                                                                                 +- Relation[tid#55,recid#56,firstname#57,middlename#58,lastname#59,address#60,city#61,state#62,zip#63,pobox#64,pocitystatezip#65,ssn#66,dob#67] JDBCRelation(tb_grounddb)
!            :     +- Relation[vid#0,rid#1,tablename#2,tupleid#3,attribute#4,value#5] JDBCRelation(violation)                                                                                  
!            +- SubqueryAlias i                                                                                                                                                                
!               +- SubqueryAlias tb_grounddb                                                                                                                                                   
!                  +- Relation[tid#55,recid#56,firstname#57,middlename#58,lastname#59,address#60,city#61,state#62,zip#63,pobox#64,pocitystatezip#65,ssn#66,dob#67] JDBCRelation(tb_grounddb)   
        
13786 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Union after 1 iterations.
13786 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Union has no effect.
13787 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Subquery after 1 iterations.
13787 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Subquery has no effect.
13787 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Replace Operators after 1 iterations.
13787 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Replace Operators has no effect.
13787 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Aggregate after 1 iterations.
13787 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Aggregate has no effect.
13789 [main] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.PushPredicateThroughJoin ===
 GlobalLimit 7                                                                                                                                                                           GlobalLimit 7
 +- LocalLimit 7                                                                                                                                                                         +- LocalLimit 7
    +- Aggregate [vid#0, tupleid#3, recid#56], [vid#0, tupleid#3, recid#56]                                                                                                                 +- Aggregate [vid#0, tupleid#3, recid#56], [vid#0, tupleid#3, recid#56]
!      +- Filter (rid#1 = dedupBlackOakGold)                                                                                                                                                   +- Join Inner, (tupleid#3 = tid#55)
!         +- Join Inner, (tupleid#3 = tid#55)                                                                                                                                                     :- Filter (rid#1 = dedupBlackOakGold)
!            :- Relation[vid#0,rid#1,tablename#2,tupleid#3,attribute#4,value#5] JDBCRelation(violation)                                                                                           :  +- Relation[vid#0,rid#1,tablename#2,tupleid#3,attribute#4,value#5] JDBCRelation(violation)
!            +- Relation[tid#55,recid#56,firstname#57,middlename#58,lastname#59,address#60,city#61,state#62,zip#63,pobox#64,pocitystatezip#65,ssn#66,dob#67] JDBCRelation(tb_grounddb)            +- Relation[tid#55,recid#56,firstname#57,middlename#58,lastname#59,address#60,city#61,state#62,zip#63,pobox#64,pocitystatezip#65,ssn#66,dob#67] JDBCRelation(tb_grounddb)
                
13791 [main] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.ColumnPruning ===
 GlobalLimit 7                                                                                                                                                                        GlobalLimit 7
 +- LocalLimit 7                                                                                                                                                                      +- LocalLimit 7
    +- Aggregate [vid#0, tupleid#3, recid#56], [vid#0, tupleid#3, recid#56]                                                                                                              +- Aggregate [vid#0, tupleid#3, recid#56], [vid#0, tupleid#3, recid#56]
!      +- Join Inner, (tupleid#3 = tid#55)                                                                                                                                                  +- Project [vid#0, tupleid#3, recid#56]
!         :- Filter (rid#1 = dedupBlackOakGold)                                                                                                                                                +- Join Inner, (tupleid#3 = tid#55)
!         :  +- Relation[vid#0,rid#1,tablename#2,tupleid#3,attribute#4,value#5] JDBCRelation(violation)                                                                                           :- Project [vid#0, tupleid#3]
!         +- Relation[tid#55,recid#56,firstname#57,middlename#58,lastname#59,address#60,city#61,state#62,zip#63,pobox#64,pocitystatezip#65,ssn#66,dob#67] JDBCRelation(tb_grounddb)               :  +- Filter (rid#1 = dedupBlackOakGold)
!                                                                                                                                                                                                 :     +- Relation[vid#0,rid#1,tablename#2,tupleid#3,attribute#4,value#5] JDBCRelation(violation)
!                                                                                                                                                                                                 +- Project [tid#55, recid#56]
!                                                                                                                                                                                                    +- Relation[tid#55,recid#56,firstname#57,middlename#58,lastname#59,address#60,city#61,state#62,zip#63,pobox#64,pocitystatezip#65,ssn#66,dob#67] JDBCRelation(tb_grounddb)
                
13795 [main] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraints ===
 GlobalLimit 7                                                                                                                                                                              GlobalLimit 7
 +- LocalLimit 7                                                                                                                                                                            +- LocalLimit 7
    +- Aggregate [vid#0, tupleid#3, recid#56], [vid#0, tupleid#3, recid#56]                                                                                                                    +- Aggregate [vid#0, tupleid#3, recid#56], [vid#0, tupleid#3, recid#56]
       +- Project [vid#0, tupleid#3, recid#56]                                                                                                                                                    +- Project [vid#0, tupleid#3, recid#56]
!         +- Join Inner, (tupleid#3 = tid#55)                                                                                                                                                        +- Join Inner, (isnotnull(tupleid#3) && (tupleid#3 = tid#55))
             :- Project [vid#0, tupleid#3]                                                                                                                                                              :- Project [vid#0, tupleid#3]
!            :  +- Filter (rid#1 = dedupBlackOakGold)                                                                                                                                                   :  +- Filter (isnotnull(rid#1) && (rid#1 = dedupBlackOakGold))
             :     +- Relation[vid#0,rid#1,tablename#2,tupleid#3,attribute#4,value#5] JDBCRelation(violation)                                                                                           :     +- Relation[vid#0,rid#1,tablename#2,tupleid#3,attribute#4,value#5] JDBCRelation(violation)
             +- Project [tid#55, recid#56]                                                                                                                                                              +- Project [tid#55, recid#56]
                +- Relation[tid#55,recid#56,firstname#57,middlename#58,lastname#59,address#60,city#61,state#62,zip#63,pobox#64,pocitystatezip#65,ssn#66,dob#67] JDBCRelation(tb_grounddb)                  +- Relation[tid#55,recid#56,firstname#57,middlename#58,lastname#59,address#60,city#61,state#62,zip#63,pobox#64,pocitystatezip#65,ssn#66,dob#67] JDBCRelation(tb_grounddb)
                
13798 [main] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.PushPredicateThroughJoin ===
 GlobalLimit 7                                                                                                                                                                              GlobalLimit 7
 +- LocalLimit 7                                                                                                                                                                            +- LocalLimit 7
    +- Aggregate [vid#0, tupleid#3, recid#56], [vid#0, tupleid#3, recid#56]                                                                                                                    +- Aggregate [vid#0, tupleid#3, recid#56], [vid#0, tupleid#3, recid#56]
       +- Project [vid#0, tupleid#3, recid#56]                                                                                                                                                    +- Project [vid#0, tupleid#3, recid#56]
!         +- Join Inner, (isnotnull(tupleid#3) && (tupleid#3 = tid#55))                                                                                                                              +- Join Inner, (tupleid#3 = tid#55)
!            :- Project [vid#0, tupleid#3]                                                                                                                                                              :- Filter isnotnull(tupleid#3)
!            :  +- Filter (isnotnull(rid#1) && (rid#1 = dedupBlackOakGold))                                                                                                                             :  +- Project [vid#0, tupleid#3]
!            :     +- Relation[vid#0,rid#1,tablename#2,tupleid#3,attribute#4,value#5] JDBCRelation(violation)                                                                                           :     +- Filter (isnotnull(rid#1) && (rid#1 = dedupBlackOakGold))
!            +- Project [tid#55, recid#56]                                                                                                                                                              :        +- Relation[vid#0,rid#1,tablename#2,tupleid#3,attribute#4,value#5] JDBCRelation(violation)
!               +- Relation[tid#55,recid#56,firstname#57,middlename#58,lastname#59,address#60,city#61,state#62,zip#63,pobox#64,pocitystatezip#65,ssn#66,dob#67] JDBCRelation(tb_grounddb)               +- Project [tid#55, recid#56]
!                                                                                                                                                                                                          +- Relation[tid#55,recid#56,firstname#57,middlename#58,lastname#59,address#60,city#61,state#62,zip#63,pobox#64,pocitystatezip#65,ssn#66,dob#67] JDBCRelation(tb_grounddb)
                
13800 [main] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.PushDownPredicate ===
 GlobalLimit 7                                                                                                                                                                              GlobalLimit 7
 +- LocalLimit 7                                                                                                                                                                            +- LocalLimit 7
    +- Aggregate [vid#0, tupleid#3, recid#56], [vid#0, tupleid#3, recid#56]                                                                                                                    +- Aggregate [vid#0, tupleid#3, recid#56], [vid#0, tupleid#3, recid#56]
       +- Project [vid#0, tupleid#3, recid#56]                                                                                                                                                    +- Project [vid#0, tupleid#3, recid#56]
          +- Join Inner, (tupleid#3 = tid#55)                                                                                                                                                        +- Join Inner, (tupleid#3 = tid#55)
!            :- Filter isnotnull(tupleid#3)                                                                                                                                                             :- Project [vid#0, tupleid#3]
!            :  +- Project [vid#0, tupleid#3]                                                                                                                                                           :  +- Filter isnotnull(tupleid#3)
             :     +- Filter (isnotnull(rid#1) && (rid#1 = dedupBlackOakGold))                                                                                                                          :     +- Filter (isnotnull(rid#1) && (rid#1 = dedupBlackOakGold))
             :        +- Relation[vid#0,rid#1,tablename#2,tupleid#3,attribute#4,value#5] JDBCRelation(violation)                                                                                        :        +- Relation[vid#0,rid#1,tablename#2,tupleid#3,attribute#4,value#5] JDBCRelation(violation)
             +- Project [tid#55, recid#56]                                                                                                                                                              +- Project [tid#55, recid#56]
                +- Relation[tid#55,recid#56,firstname#57,middlename#58,lastname#59,address#60,city#61,state#62,zip#63,pobox#64,pocitystatezip#65,ssn#66,dob#67] JDBCRelation(tb_grounddb)                  +- Relation[tid#55,recid#56,firstname#57,middlename#58,lastname#59,address#60,city#61,state#62,zip#63,pobox#64,pocitystatezip#65,ssn#66,dob#67] JDBCRelation(tb_grounddb)
                
13804 [main] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.ColumnPruning ===
 GlobalLimit 7                                                                                                                                                                              GlobalLimit 7
 +- LocalLimit 7                                                                                                                                                                            +- LocalLimit 7
    +- Aggregate [vid#0, tupleid#3, recid#56], [vid#0, tupleid#3, recid#56]                                                                                                                    +- Aggregate [vid#0, tupleid#3, recid#56], [vid#0, tupleid#3, recid#56]
       +- Project [vid#0, tupleid#3, recid#56]                                                                                                                                                    +- Project [vid#0, tupleid#3, recid#56]
          +- Join Inner, (tupleid#3 = tid#55)                                                                                                                                                        +- Join Inner, (tupleid#3 = tid#55)
             :- Project [vid#0, tupleid#3]                                                                                                                                                              :- Project [vid#0, tupleid#3]
             :  +- Filter isnotnull(tupleid#3)                                                                                                                                                          :  +- Filter isnotnull(tupleid#3)
             :     +- Filter (isnotnull(rid#1) && (rid#1 = dedupBlackOakGold))                                                                                                                          :     +- Filter (isnotnull(rid#1) && (rid#1 = dedupBlackOakGold))
!            :        +- Relation[vid#0,rid#1,tablename#2,tupleid#3,attribute#4,value#5] JDBCRelation(violation)                                                                                        :        +- Project [vid#0, rid#1, tupleid#3]
!            +- Project [tid#55, recid#56]                                                                                                                                                              :           +- Relation[vid#0,rid#1,tablename#2,tupleid#3,attribute#4,value#5] JDBCRelation(violation)
!               +- Relation[tid#55,recid#56,firstname#57,middlename#58,lastname#59,address#60,city#61,state#62,zip#63,pobox#64,pocitystatezip#65,ssn#66,dob#67] JDBCRelation(tb_grounddb)               +- Project [tid#55, recid#56]
!                                                                                                                                                                                                          +- Relation[tid#55,recid#56,firstname#57,middlename#58,lastname#59,address#60,city#61,state#62,zip#63,pobox#64,pocitystatezip#65,ssn#66,dob#67] JDBCRelation(tb_grounddb)
                
13808 [main] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.CombineFilters ===
 GlobalLimit 7                                                                                                                                                                              GlobalLimit 7
 +- LocalLimit 7                                                                                                                                                                            +- LocalLimit 7
    +- Aggregate [vid#0, tupleid#3, recid#56], [vid#0, tupleid#3, recid#56]                                                                                                                    +- Aggregate [vid#0, tupleid#3, recid#56], [vid#0, tupleid#3, recid#56]
       +- Project [vid#0, tupleid#3, recid#56]                                                                                                                                                    +- Project [vid#0, tupleid#3, recid#56]
          +- Join Inner, (tupleid#3 = tid#55)                                                                                                                                                        +- Join Inner, (tupleid#3 = tid#55)
             :- Project [vid#0, tupleid#3]                                                                                                                                                              :- Project [vid#0, tupleid#3]
!            :  +- Filter isnotnull(tupleid#3)                                                                                                                                                          :  +- Filter ((isnotnull(rid#1) && (rid#1 = dedupBlackOakGold)) && isnotnull(tupleid#3))
!            :     +- Filter (isnotnull(rid#1) && (rid#1 = dedupBlackOakGold))                                                                                                                          :     +- Project [vid#0, rid#1, tupleid#3]
!            :        +- Project [vid#0, rid#1, tupleid#3]                                                                                                                                              :        +- Relation[vid#0,rid#1,tablename#2,tupleid#3,attribute#4,value#5] JDBCRelation(violation)
!            :           +- Relation[vid#0,rid#1,tablename#2,tupleid#3,attribute#4,value#5] JDBCRelation(violation)                                                                                     +- Project [tid#55, recid#56]
!            +- Project [tid#55, recid#56]                                                                                                                                                                 +- Relation[tid#55,recid#56,firstname#57,middlename#58,lastname#59,address#60,city#61,state#62,zip#63,pobox#64,pocitystatezip#65,ssn#66,dob#67] JDBCRelation(tb_grounddb)
!               +- Relation[tid#55,recid#56,firstname#57,middlename#58,lastname#59,address#60,city#61,state#62,zip#63,pobox#64,pocitystatezip#65,ssn#66,dob#67] JDBCRelation(tb_grounddb)   
                
13811 [main] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.PushDownPredicate ===
 GlobalLimit 7                                                                                                                                                                              GlobalLimit 7
 +- LocalLimit 7                                                                                                                                                                            +- LocalLimit 7
    +- Aggregate [vid#0, tupleid#3, recid#56], [vid#0, tupleid#3, recid#56]                                                                                                                    +- Aggregate [vid#0, tupleid#3, recid#56], [vid#0, tupleid#3, recid#56]
       +- Project [vid#0, tupleid#3, recid#56]                                                                                                                                                    +- Project [vid#0, tupleid#3, recid#56]
          +- Join Inner, (tupleid#3 = tid#55)                                                                                                                                                        +- Join Inner, (tupleid#3 = tid#55)
             :- Project [vid#0, tupleid#3]                                                                                                                                                              :- Project [vid#0, tupleid#3]
!            :  +- Filter ((isnotnull(rid#1) && (rid#1 = dedupBlackOakGold)) && isnotnull(tupleid#3))                                                                                                   :  +- Project [vid#0, rid#1, tupleid#3]
!            :     +- Project [vid#0, rid#1, tupleid#3]                                                                                                                                                 :     +- Filter ((isnotnull(rid#1) && (rid#1 = dedupBlackOakGold)) && isnotnull(tupleid#3))
             :        +- Relation[vid#0,rid#1,tablename#2,tupleid#3,attribute#4,value#5] JDBCRelation(violation)                                                                                        :        +- Relation[vid#0,rid#1,tablename#2,tupleid#3,attribute#4,value#5] JDBCRelation(violation)
             +- Project [tid#55, recid#56]                                                                                                                                                              +- Project [tid#55, recid#56]
                +- Relation[tid#55,recid#56,firstname#57,middlename#58,lastname#59,address#60,city#61,state#62,zip#63,pobox#64,pocitystatezip#65,ssn#66,dob#67] JDBCRelation(tb_grounddb)                  +- Relation[tid#55,recid#56,firstname#57,middlename#58,lastname#59,address#60,city#61,state#62,zip#63,pobox#64,pocitystatezip#65,ssn#66,dob#67] JDBCRelation(tb_grounddb)
                
13814 [main] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.ColumnPruning ===
 GlobalLimit 7                                                                                                                                                                              GlobalLimit 7
 +- LocalLimit 7                                                                                                                                                                            +- LocalLimit 7
    +- Aggregate [vid#0, tupleid#3, recid#56], [vid#0, tupleid#3, recid#56]                                                                                                                    +- Aggregate [vid#0, tupleid#3, recid#56], [vid#0, tupleid#3, recid#56]
       +- Project [vid#0, tupleid#3, recid#56]                                                                                                                                                    +- Project [vid#0, tupleid#3, recid#56]
          +- Join Inner, (tupleid#3 = tid#55)                                                                                                                                                        +- Join Inner, (tupleid#3 = tid#55)
             :- Project [vid#0, tupleid#3]                                                                                                                                                              :- Project [vid#0, tupleid#3]
!            :  +- Project [vid#0, rid#1, tupleid#3]                                                                                                                                                    :  +- Project [vid#0, tupleid#3]
             :     +- Filter ((isnotnull(rid#1) && (rid#1 = dedupBlackOakGold)) && isnotnull(tupleid#3))                                                                                                :     +- Filter ((isnotnull(rid#1) && (rid#1 = dedupBlackOakGold)) && isnotnull(tupleid#3))
             :        +- Relation[vid#0,rid#1,tablename#2,tupleid#3,attribute#4,value#5] JDBCRelation(violation)                                                                                        :        +- Relation[vid#0,rid#1,tablename#2,tupleid#3,attribute#4,value#5] JDBCRelation(violation)
             +- Project [tid#55, recid#56]                                                                                                                                                              +- Project [tid#55, recid#56]
                +- Relation[tid#55,recid#56,firstname#57,middlename#58,lastname#59,address#60,city#61,state#62,zip#63,pobox#64,pocitystatezip#65,ssn#66,dob#67] JDBCRelation(tb_grounddb)                  +- Relation[tid#55,recid#56,firstname#57,middlename#58,lastname#59,address#60,city#61,state#62,zip#63,pobox#64,pocitystatezip#65,ssn#66,dob#67] JDBCRelation(tb_grounddb)
                
13818 [main] TRACE org.apache.spark.internal.Logging$class  - 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.CollapseProject ===
 GlobalLimit 7                                                                                                                                                                              GlobalLimit 7
 +- LocalLimit 7                                                                                                                                                                            +- LocalLimit 7
    +- Aggregate [vid#0, tupleid#3, recid#56], [vid#0, tupleid#3, recid#56]                                                                                                                    +- Aggregate [vid#0, tupleid#3, recid#56], [vid#0, tupleid#3, recid#56]
       +- Project [vid#0, tupleid#3, recid#56]                                                                                                                                                    +- Project [vid#0, tupleid#3, recid#56]
          +- Join Inner, (tupleid#3 = tid#55)                                                                                                                                                        +- Join Inner, (tupleid#3 = tid#55)
             :- Project [vid#0, tupleid#3]                                                                                                                                                              :- Project [vid#0, tupleid#3]
!            :  +- Project [vid#0, tupleid#3]                                                                                                                                                           :  +- Filter ((isnotnull(rid#1) && (rid#1 = dedupBlackOakGold)) && isnotnull(tupleid#3))
!            :     +- Filter ((isnotnull(rid#1) && (rid#1 = dedupBlackOakGold)) && isnotnull(tupleid#3))                                                                                                :     +- Relation[vid#0,rid#1,tablename#2,tupleid#3,attribute#4,value#5] JDBCRelation(violation)
!            :        +- Relation[vid#0,rid#1,tablename#2,tupleid#3,attribute#4,value#5] JDBCRelation(violation)                                                                                        +- Project [tid#55, recid#56]
!            +- Project [tid#55, recid#56]                                                                                                                                                                 +- Relation[tid#55,recid#56,firstname#57,middlename#58,lastname#59,address#60,city#61,state#62,zip#63,pobox#64,pocitystatezip#65,ssn#66,dob#67] JDBCRelation(tb_grounddb)
!               +- Relation[tid#55,recid#56,firstname#57,middlename#58,lastname#59,address#60,city#61,state#62,zip#63,pobox#64,pocitystatezip#65,ssn#66,dob#67] JDBCRelation(tb_grounddb)   
                
13822 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Operator Optimizations after 4 iterations.
13824 [main] DEBUG org.apache.spark.internal.Logging$class  - 
=== Result of Batch Operator Optimizations ===
 GlobalLimit 7                                                                                                                                                                                 GlobalLimit 7
 +- LocalLimit 7                                                                                                                                                                               +- LocalLimit 7
    +- Aggregate [vid#0, tupleid#3, recid#56], [vid#0, tupleid#3, recid#56]                                                                                                                       +- Aggregate [vid#0, tupleid#3, recid#56], [vid#0, tupleid#3, recid#56]
!      +- Filter (rid#1 = dedupBlackOakGold)                                                                                                                                                         +- Project [vid#0, tupleid#3, recid#56]
          +- Join Inner, (tupleid#3 = tid#55)                                                                                                                                                           +- Join Inner, (tupleid#3 = tid#55)
!            :- SubqueryAlias v                                                                                                                                                                            :- Project [vid#0, tupleid#3]
!            :  +- SubqueryAlias violation                                                                                                                                                                 :  +- Filter ((isnotnull(rid#1) && (rid#1 = dedupBlackOakGold)) && isnotnull(tupleid#3))
             :     +- Relation[vid#0,rid#1,tablename#2,tupleid#3,attribute#4,value#5] JDBCRelation(violation)                                                                                              :     +- Relation[vid#0,rid#1,tablename#2,tupleid#3,attribute#4,value#5] JDBCRelation(violation)
!            +- SubqueryAlias i                                                                                                                                                                            +- Project [tid#55, recid#56]
!               +- SubqueryAlias tb_grounddb                                                                                                                                                                  +- Relation[tid#55,recid#56,firstname#57,middlename#58,lastname#59,address#60,city#61,state#62,zip#63,pobox#64,pocitystatezip#65,ssn#66,dob#67] JDBCRelation(tb_grounddb)
!                  +- Relation[tid#55,recid#56,firstname#57,middlename#58,lastname#59,address#60,city#61,state#62,zip#63,pobox#64,pocitystatezip#65,ssn#66,dob#67] JDBCRelation(tb_grounddb)   
        
13824 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Decimal Optimizations after 1 iterations.
13824 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Decimal Optimizations has no effect.
13825 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Typed Filter Optimization after 1 iterations.
13825 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Typed Filter Optimization has no effect.
13825 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch LocalRelation after 1 iterations.
13825 [main] TRACE org.apache.spark.internal.Logging$class  - Batch LocalRelation has no effect.
13825 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch OptimizeCodegen after 1 iterations.
13825 [main] TRACE org.apache.spark.internal.Logging$class  - Batch OptimizeCodegen has no effect.
13825 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch RewriteSubquery after 1 iterations.
13825 [main] TRACE org.apache.spark.internal.Logging$class  - Batch RewriteSubquery has no effect.
13825 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch Extract Python UDF from Aggregate after 1 iterations.
13826 [main] TRACE org.apache.spark.internal.Logging$class  - Batch Extract Python UDF from Aggregate has no effect.
13826 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch User Provided Optimizers after 1 iterations.
13826 [main] TRACE org.apache.spark.internal.Logging$class  - Batch User Provided Optimizers has no effect.
13827 [main] DEBUG org.apache.spark.internal.Logging$class  - Considering join on: Some((tupleid#3 = tid#55))
13827 [main] DEBUG org.apache.spark.internal.Logging$class  - leftKeys:List(tupleid#3) | rightKeys:List(tid#55)
13827 [main] DEBUG org.apache.spark.internal.Logging$class  - Considering join on: Some((tupleid#3 = tid#55))
13828 [main] DEBUG org.apache.spark.internal.Logging$class  - leftKeys:List(tupleid#3) | rightKeys:List(tid#55)
13828 [main] DEBUG org.apache.spark.internal.Logging$class  - Considering join on: Some((tupleid#3 = tid#55))
13828 [main] DEBUG org.apache.spark.internal.Logging$class  - leftKeys:List(tupleid#3) | rightKeys:List(tid#55)
13829 [main] DEBUG org.apache.spark.internal.Logging$class  - Considering join on: Some((tupleid#3 = tid#55))
13829 [main] DEBUG org.apache.spark.internal.Logging$class  - leftKeys:List(tupleid#3) | rightKeys:List(tid#55)
13829 [main] DEBUG org.apache.spark.internal.Logging$class  - Considering join on: Some((tupleid#3 = tid#55))
13829 [main] DEBUG org.apache.spark.internal.Logging$class  - leftKeys:List(tupleid#3) | rightKeys:List(tid#55)
13830 [main] TRACE org.apache.spark.internal.Logging$class  - Wrapper for org.postgresql.Driver already exists
13832 [main] TRACE org.apache.spark.internal.Logging$class  - Wrapper for org.postgresql.Driver already exists
13844 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13844 [main] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13844 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13845 [main] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13845 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13845 [main] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13850 [main] DEBUG org.apache.spark.internal.Logging$class  - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIterator(references);
/* 003 */ }
/* 004 */
/* 005 */ final class GeneratedIterator extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 006 */   private Object[] references;
/* 007 */   private boolean agg_initAgg;
/* 008 */   private org.apache.spark.sql.execution.aggregate.HashAggregateExec agg_plan;
/* 009 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap agg_hashMap;
/* 010 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter agg_sorter;
/* 011 */   private org.apache.spark.unsafe.KVIterator agg_mapIter;
/* 012 */   private org.apache.spark.sql.execution.metric.SQLMetric agg_peakMemory;
/* 013 */   private org.apache.spark.sql.execution.metric.SQLMetric agg_spillSize;
/* 014 */   private boolean agg_initAgg1;
/* 015 */   private org.apache.spark.sql.execution.aggregate.HashAggregateExec agg_plan1;
/* 016 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap agg_hashMap1;
/* 017 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter agg_sorter1;
/* 018 */   private org.apache.spark.unsafe.KVIterator agg_mapIter1;
/* 019 */   private org.apache.spark.sql.execution.metric.SQLMetric agg_peakMemory1;
/* 020 */   private org.apache.spark.sql.execution.metric.SQLMetric agg_spillSize1;
/* 021 */   private scala.collection.Iterator smj_leftInput;
/* 022 */   private scala.collection.Iterator smj_rightInput;
/* 023 */   private InternalRow smj_leftRow;
/* 024 */   private InternalRow smj_rightRow;
/* 025 */   private int smj_value2;
/* 026 */   private java.util.ArrayList smj_matches;
/* 027 */   private int smj_value3;
/* 028 */   private int smj_value4;
/* 029 */   private boolean smj_isNull2;
/* 030 */   private int smj_value5;
/* 031 */   private boolean smj_isNull3;
/* 032 */   private org.apache.spark.sql.execution.metric.SQLMetric smj_numOutputRows;
/* 033 */   private UnsafeRow smj_result;
/* 034 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder smj_holder;
/* 035 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter smj_rowWriter;
/* 036 */   private UnsafeRow project_result;
/* 037 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder project_holder;
/* 038 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter project_rowWriter;
/* 039 */   private UnsafeRow agg_result;
/* 040 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder agg_holder;
/* 041 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter agg_rowWriter;
/* 042 */   private UnsafeRow agg_result1;
/* 043 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder agg_holder1;
/* 044 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter agg_rowWriter1;
/* 045 */   private UnsafeRow agg_result2;
/* 046 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder agg_holder2;
/* 047 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter agg_rowWriter2;
/* 048 */   private org.apache.spark.sql.execution.metric.SQLMetric agg_numOutputRows;
/* 049 */   private org.apache.spark.sql.execution.metric.SQLMetric agg_aggTime;
/* 050 */   private UnsafeRow agg_result3;
/* 051 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder agg_holder3;
/* 052 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter agg_rowWriter3;
/* 053 */   private org.apache.spark.sql.execution.metric.SQLMetric wholestagecodegen_numOutputRows;
/* 054 */   private org.apache.spark.sql.execution.metric.SQLMetric wholestagecodegen_aggTime;
/* 055 */
/* 056 */   public GeneratedIterator(Object[] references) {
/* 057 */     this.references = references;
/* 058 */   }
/* 059 */
/* 060 */   public void init(int index, scala.collection.Iterator inputs[]) {
/* 061 */     partitionIndex = index;
/* 062 */     agg_initAgg = false;
/* 063 */     this.agg_plan = (org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0];
/* 064 */
/* 065 */     this.agg_peakMemory = (org.apache.spark.sql.execution.metric.SQLMetric) references[1];
/* 066 */     this.agg_spillSize = (org.apache.spark.sql.execution.metric.SQLMetric) references[2];
/* 067 */     agg_initAgg1 = false;
/* 068 */     this.agg_plan1 = (org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[3];
/* 069 */
/* 070 */     this.agg_peakMemory1 = (org.apache.spark.sql.execution.metric.SQLMetric) references[4];
/* 071 */     this.agg_spillSize1 = (org.apache.spark.sql.execution.metric.SQLMetric) references[5];
/* 072 */     smj_leftInput = inputs[0];
/* 073 */     smj_rightInput = inputs[1];
/* 074 */
/* 075 */     smj_rightRow = null;
/* 076 */
/* 077 */     smj_matches = new java.util.ArrayList();
/* 078 */
/* 079 */     this.smj_numOutputRows = (org.apache.spark.sql.execution.metric.SQLMetric) references[6];
/* 080 */     smj_result = new UnsafeRow(4);
/* 081 */     this.smj_holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(smj_result, 32);
/* 082 */     this.smj_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(smj_holder, 4);
/* 083 */     project_result = new UnsafeRow(3);
/* 084 */     this.project_holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(project_result, 32);
/* 085 */     this.project_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(project_holder, 3);
/* 086 */     agg_result = new UnsafeRow(3);
/* 087 */     this.agg_holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(agg_result, 32);
/* 088 */     this.agg_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(agg_holder, 3);
/* 089 */     agg_result1 = new UnsafeRow(3);
/* 090 */     this.agg_holder1 = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(agg_result1, 32);
/* 091 */     this.agg_rowWriter1 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(agg_holder1, 3);
/* 092 */     agg_result2 = new UnsafeRow(3);
/* 093 */     this.agg_holder2 = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(agg_result2, 32);
/* 094 */     this.agg_rowWriter2 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(agg_holder2, 3);
/* 095 */     this.agg_numOutputRows = (org.apache.spark.sql.execution.metric.SQLMetric) references[7];
/* 096 */     this.agg_aggTime = (org.apache.spark.sql.execution.metric.SQLMetric) references[8];
/* 097 */     agg_result3 = new UnsafeRow(3);
/* 098 */     this.agg_holder3 = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(agg_result3, 32);
/* 099 */     this.agg_rowWriter3 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(agg_holder3, 3);
/* 100 */     this.wholestagecodegen_numOutputRows = (org.apache.spark.sql.execution.metric.SQLMetric) references[9];
/* 101 */     this.wholestagecodegen_aggTime = (org.apache.spark.sql.execution.metric.SQLMetric) references[10];
/* 102 */   }
/* 103 */
/* 104 */   private void agg_doAggregateWithKeys() throws java.io.IOException {
/* 105 */     agg_hashMap = agg_plan.createHashMap();
/* 106 */
/* 107 */     if (!agg_initAgg1) {
/* 108 */       agg_initAgg1 = true;
/* 109 */       long agg_beforeAgg = System.nanoTime();
/* 110 */       agg_doAggregateWithKeys1();
/* 111 */       agg_aggTime.add((System.nanoTime() - agg_beforeAgg) / 1000000);
/* 112 */     }
/* 113 */
/* 114 */     // output the result
/* 115 */
/* 116 */     while (agg_mapIter1.next()) {
/* 117 */       agg_numOutputRows.add(1);
/* 118 */       UnsafeRow agg_aggKey = (UnsafeRow) agg_mapIter1.getKey();
/* 119 */       UnsafeRow agg_aggBuffer = (UnsafeRow) agg_mapIter1.getValue();
/* 120 */
/* 121 */       boolean agg_isNull10 = agg_aggKey.isNullAt(0);
/* 122 */       int agg_value10 = agg_isNull10 ? -1 : (agg_aggKey.getInt(0));
/* 123 */       boolean agg_isNull11 = agg_aggKey.isNullAt(1);
/* 124 */       int agg_value11 = agg_isNull11 ? -1 : (agg_aggKey.getInt(1));
/* 125 */       boolean agg_isNull12 = agg_aggKey.isNullAt(2);
/* 126 */       UTF8String agg_value12 = agg_isNull12 ? null : (agg_aggKey.getUTF8String(2));
/* 127 */
/* 128 */       UnsafeRow agg_unsafeRowAggBuffer1 = null;
/* 129 */       org.apache.spark.sql.execution.vectorized.ColumnarBatch.Row agg_vectorizedAggBuffer1 = null;
/* 130 */
/* 131 */       if (agg_vectorizedAggBuffer1 == null) {
/* 132 */         // generate grouping key
/* 133 */         agg_holder2.reset();
/* 134 */
/* 135 */         agg_rowWriter2.zeroOutNullBytes();
/* 136 */
/* 137 */         if (agg_isNull10) {
/* 138 */           agg_rowWriter2.setNullAt(0);
/* 139 */         } else {
/* 140 */           agg_rowWriter2.write(0, agg_value10);
/* 141 */         }
/* 142 */
/* 143 */         if (agg_isNull11) {
/* 144 */           agg_rowWriter2.setNullAt(1);
/* 145 */         } else {
/* 146 */           agg_rowWriter2.write(1, agg_value11);
/* 147 */         }
/* 148 */
/* 149 */         if (agg_isNull12) {
/* 150 */           agg_rowWriter2.setNullAt(2);
/* 151 */         } else {
/* 152 */           agg_rowWriter2.write(2, agg_value12);
/* 153 */         }
/* 154 */         agg_result2.setTotalSize(agg_holder2.totalSize());
/* 155 */         int agg_value22 = 42;
/* 156 */
/* 157 */         if (!agg_isNull10) {
/* 158 */           agg_value22 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(agg_value10, agg_value22);
/* 159 */         }
/* 160 */
/* 161 */         if (!agg_isNull11) {
/* 162 */           agg_value22 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(agg_value11, agg_value22);
/* 163 */         }
/* 164 */
/* 165 */         if (!agg_isNull12) {
/* 166 */           agg_value22 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashUnsafeBytes(agg_value12.getBaseObject(), agg_value12.getBaseOffset(), agg_value12.numBytes(), agg_value22);
/* 167 */         }
/* 168 */         if (true) {
/* 169 */           // try to get the buffer from hash map
/* 170 */           agg_unsafeRowAggBuffer1 =
/* 171 */           agg_hashMap.getAggregationBufferFromUnsafeRow(agg_result2, agg_value22);
/* 172 */         }
/* 173 */         if (agg_unsafeRowAggBuffer1 == null) {
/* 174 */           if (agg_sorter == null) {
/* 175 */             agg_sorter = agg_hashMap.destructAndCreateExternalSorter();
/* 176 */           } else {
/* 177 */             agg_sorter.merge(agg_hashMap.destructAndCreateExternalSorter());
/* 178 */           }
/* 179 */
/* 180 */           // the hash map had be spilled, it should have enough memory now,
/* 181 */           // try  to allocate buffer again.
/* 182 */           agg_unsafeRowAggBuffer1 =
/* 183 */           agg_hashMap.getAggregationBufferFromUnsafeRow(agg_result2, agg_value22);
/* 184 */           if (agg_unsafeRowAggBuffer1 == null) {
/* 185 */             // failed to allocate the first page
/* 186 */             throw new OutOfMemoryError("No enough memory for aggregation");
/* 187 */           }
/* 188 */         }
/* 189 */       }
/* 190 */
/* 191 */       if (agg_vectorizedAggBuffer1 != null) {
/* 192 */         // update vectorized row
/* 193 */
/* 194 */       } else {
/* 195 */         // update unsafe row
/* 196 */
/* 197 */         // common sub-expressions
/* 198 */
/* 199 */         // evaluate aggregate function
/* 200 */
/* 201 */         // update unsafe row buffer
/* 202 */
/* 203 */       }
/* 204 */
/* 205 */       if (shouldStop()) return;
/* 206 */     }
/* 207 */
/* 208 */     agg_mapIter1.close();
/* 209 */     if (agg_sorter1 == null) {
/* 210 */       agg_hashMap1.free();
/* 211 */     }
/* 212 */
/* 213 */     agg_mapIter = agg_plan.finishAggregate(agg_hashMap, agg_sorter, agg_peakMemory, agg_spillSize);
/* 214 */   }
/* 215 */
/* 216 */   private void agg_doAggregateWithKeys1() throws java.io.IOException {
/* 217 */     agg_hashMap1 = agg_plan1.createHashMap();
/* 218 */
/* 219 */     while (findNextInnerJoinRows(smj_leftInput, smj_rightInput)) {
/* 220 */       int smj_size = smj_matches.size();
/* 221 */       smj_isNull2 = smj_leftRow.isNullAt(0);
/* 222 */       smj_value4 = smj_isNull2 ? -1 : (smj_leftRow.getInt(0));
/* 223 */       smj_isNull3 = smj_leftRow.isNullAt(1);
/* 224 */       smj_value5 = smj_isNull3 ? -1 : (smj_leftRow.getInt(1));
/* 225 */       for (int smj_i = 0; smj_i < smj_size; smj_i ++) {
/* 226 */         InternalRow smj_rightRow1 = (InternalRow) smj_matches.get(smj_i);
/* 227 */
/* 228 */         smj_numOutputRows.add(1);
/* 229 */
/* 230 */         boolean smj_isNull5 = smj_rightRow1.isNullAt(1);
/* 231 */         UTF8String smj_value7 = smj_isNull5 ? null : (smj_rightRow1.getUTF8String(1));
/* 232 */
/* 233 */         UnsafeRow agg_unsafeRowAggBuffer = null;
/* 234 */         org.apache.spark.sql.execution.vectorized.ColumnarBatch.Row agg_vectorizedAggBuffer = null;
/* 235 */
/* 236 */         if (agg_vectorizedAggBuffer == null) {
/* 237 */           // generate grouping key
/* 238 */           agg_holder.reset();
/* 239 */
/* 240 */           agg_rowWriter.zeroOutNullBytes();
/* 241 */
/* 242 */           if (smj_isNull2) {
/* 243 */             agg_rowWriter.setNullAt(0);
/* 244 */           } else {
/* 245 */             agg_rowWriter.write(0, smj_value4);
/* 246 */           }
/* 247 */
/* 248 */           if (smj_isNull3) {
/* 249 */             agg_rowWriter.setNullAt(1);
/* 250 */           } else {
/* 251 */             agg_rowWriter.write(1, smj_value5);
/* 252 */           }
/* 253 */
/* 254 */           if (smj_isNull5) {
/* 255 */             agg_rowWriter.setNullAt(2);
/* 256 */           } else {
/* 257 */             agg_rowWriter.write(2, smj_value7);
/* 258 */           }
/* 259 */           agg_result.setTotalSize(agg_holder.totalSize());
/* 260 */           int agg_value6 = 42;
/* 261 */
/* 262 */           if (!smj_isNull2) {
/* 263 */             agg_value6 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(smj_value4, agg_value6);
/* 264 */           }
/* 265 */
/* 266 */           if (!smj_isNull3) {
/* 267 */             agg_value6 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(smj_value5, agg_value6);
/* 268 */           }
/* 269 */
/* 270 */           if (!smj_isNull5) {
/* 271 */             agg_value6 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashUnsafeBytes(smj_value7.getBaseObject(), smj_value7.getBaseOffset(), smj_value7.numBytes(), agg_value6);
/* 272 */           }
/* 273 */           if (true) {
/* 274 */             // try to get the buffer from hash map
/* 275 */             agg_unsafeRowAggBuffer =
/* 276 */             agg_hashMap1.getAggregationBufferFromUnsafeRow(agg_result, agg_value6);
/* 277 */           }
/* 278 */           if (agg_unsafeRowAggBuffer == null) {
/* 279 */             if (agg_sorter1 == null) {
/* 280 */               agg_sorter1 = agg_hashMap1.destructAndCreateExternalSorter();
/* 281 */             } else {
/* 282 */               agg_sorter1.merge(agg_hashMap1.destructAndCreateExternalSorter());
/* 283 */             }
/* 284 */
/* 285 */             // the hash map had be spilled, it should have enough memory now,
/* 286 */             // try  to allocate buffer again.
/* 287 */             agg_unsafeRowAggBuffer =
/* 288 */             agg_hashMap1.getAggregationBufferFromUnsafeRow(agg_result, agg_value6);
/* 289 */             if (agg_unsafeRowAggBuffer == null) {
/* 290 */               // failed to allocate the first page
/* 291 */               throw new OutOfMemoryError("No enough memory for aggregation");
/* 292 */             }
/* 293 */           }
/* 294 */         }
/* 295 */
/* 296 */         if (agg_vectorizedAggBuffer != null) {
/* 297 */           // update vectorized row
/* 298 */
/* 299 */         } else {
/* 300 */           // update unsafe row
/* 301 */
/* 302 */           // common sub-expressions
/* 303 */
/* 304 */           // evaluate aggregate function
/* 305 */
/* 306 */           // update unsafe row buffer
/* 307 */
/* 308 */         }
/* 309 */
/* 310 */       }
/* 311 */       if (shouldStop()) return;
/* 312 */     }
/* 313 */
/* 314 */     agg_mapIter1 = agg_plan1.finishAggregate(agg_hashMap1, agg_sorter1, agg_peakMemory1, agg_spillSize1);
/* 315 */   }
/* 316 */
/* 317 */   private boolean findNextInnerJoinRows(
/* 318 */     scala.collection.Iterator leftIter,
/* 319 */     scala.collection.Iterator rightIter) {
/* 320 */     smj_leftRow = null;
/* 321 */     int comp = 0;
/* 322 */     while (smj_leftRow == null) {
/* 323 */       if (!leftIter.hasNext()) return false;
/* 324 */       smj_leftRow = (InternalRow) leftIter.next();
/* 325 */
/* 326 */       boolean smj_isNull = smj_leftRow.isNullAt(1);
/* 327 */       int smj_value = smj_isNull ? -1 : (smj_leftRow.getInt(1));
/* 328 */       if (smj_isNull) {
/* 329 */         smj_leftRow = null;
/* 330 */         continue;
/* 331 */       }
/* 332 */       if (!smj_matches.isEmpty()) {
/* 333 */         comp = 0;
/* 334 */         if (comp == 0) {
/* 335 */           comp = (smj_value > smj_value3 ? 1 : smj_value < smj_value3 ? -1 : 0);
/* 336 */         }
/* 337 */
/* 338 */         if (comp == 0) {
/* 339 */           return true;
/* 340 */         }
/* 341 */         smj_matches.clear();
/* 342 */       }
/* 343 */
/* 344 */       do {
/* 345 */         if (smj_rightRow == null) {
/* 346 */           if (!rightIter.hasNext()) {
/* 347 */             smj_value3 = smj_value;
/* 348 */             return !smj_matches.isEmpty();
/* 349 */           }
/* 350 */           smj_rightRow = (InternalRow) rightIter.next();
/* 351 */
/* 352 */           int smj_value1 = smj_rightRow.getInt(0);
/* 353 */           if (false) {
/* 354 */             smj_rightRow = null;
/* 355 */             continue;
/* 356 */           }
/* 357 */           smj_value2 = smj_value1;
/* 358 */         }
/* 359 */
/* 360 */         comp = 0;
/* 361 */         if (comp == 0) {
/* 362 */           comp = (smj_value > smj_value2 ? 1 : smj_value < smj_value2 ? -1 : 0);
/* 363 */         }
/* 364 */
/* 365 */         if (comp > 0) {
/* 366 */           smj_rightRow = null;
/* 367 */         } else if (comp < 0) {
/* 368 */           if (!smj_matches.isEmpty()) {
/* 369 */             smj_value3 = smj_value;
/* 370 */             return true;
/* 371 */           }
/* 372 */           smj_leftRow = null;
/* 373 */         } else {
/* 374 */           smj_matches.add(smj_rightRow.copy());
/* 375 */           smj_rightRow = null;;
/* 376 */         }
/* 377 */       } while (smj_leftRow != null);
/* 378 */     }
/* 379 */     return false; // unreachable
/* 380 */   }
/* 381 */
/* 382 */   protected void processNext() throws java.io.IOException {
/* 383 */     if (!agg_initAgg) {
/* 384 */       agg_initAgg = true;
/* 385 */       long wholestagecodegen_beforeAgg = System.nanoTime();
/* 386 */       agg_doAggregateWithKeys();
/* 387 */       wholestagecodegen_aggTime.add((System.nanoTime() - wholestagecodegen_beforeAgg) / 1000000);
/* 388 */     }
/* 389 */
/* 390 */     // output the result
/* 391 */
/* 392 */     while (agg_mapIter.next()) {
/* 393 */       wholestagecodegen_numOutputRows.add(1);
/* 394 */       UnsafeRow agg_aggKey1 = (UnsafeRow) agg_mapIter.getKey();
/* 395 */       UnsafeRow agg_aggBuffer1 = (UnsafeRow) agg_mapIter.getValue();
/* 396 */
/* 397 */       boolean agg_isNull26 = agg_aggKey1.isNullAt(0);
/* 398 */       int agg_value26 = agg_isNull26 ? -1 : (agg_aggKey1.getInt(0));
/* 399 */       boolean agg_isNull27 = agg_aggKey1.isNullAt(1);
/* 400 */       int agg_value27 = agg_isNull27 ? -1 : (agg_aggKey1.getInt(1));
/* 401 */       boolean agg_isNull28 = agg_aggKey1.isNullAt(2);
/* 402 */       UTF8String agg_value28 = agg_isNull28 ? null : (agg_aggKey1.getUTF8String(2));
/* 403 */       agg_holder3.reset();
/* 404 */
/* 405 */       agg_rowWriter3.zeroOutNullBytes();
/* 406 */
/* 407 */       if (agg_isNull26) {
/* 408 */         agg_rowWriter3.setNullAt(0);
/* 409 */       } else {
/* 410 */         agg_rowWriter3.write(0, agg_value26);
/* 411 */       }
/* 412 */
/* 413 */       if (agg_isNull27) {
/* 414 */         agg_rowWriter3.setNullAt(1);
/* 415 */       } else {
/* 416 */         agg_rowWriter3.write(1, agg_value27);
/* 417 */       }
/* 418 */
/* 419 */       if (agg_isNull28) {
/* 420 */         agg_rowWriter3.setNullAt(2);
/* 421 */       } else {
/* 422 */         agg_rowWriter3.write(2, agg_value28);
/* 423 */       }
/* 424 */       agg_result3.setTotalSize(agg_holder3.totalSize());
/* 425 */       append(agg_result3);
/* 426 */
/* 427 */       if (shouldStop()) return;
/* 428 */     }
/* 429 */
/* 430 */     agg_mapIter.close();
/* 431 */     if (agg_sorter == null) {
/* 432 */       agg_hashMap.free();
/* 433 */     }
/* 434 */   }
/* 435 */ }

13852 [main] DEBUG org.apache.spark.internal.Logging$class  - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIterator(references);
/* 003 */ }
/* 004 */
/* 005 */ final class GeneratedIterator extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 006 */   private Object[] references;
/* 007 */   private boolean sort_needToSort;
/* 008 */   private org.apache.spark.sql.execution.SortExec sort_plan;
/* 009 */   private org.apache.spark.sql.execution.UnsafeExternalRowSorter sort_sorter;
/* 010 */   private org.apache.spark.executor.TaskMetrics sort_metrics;
/* 011 */   private scala.collection.Iterator<UnsafeRow> sort_sortedIter;
/* 012 */   private scala.collection.Iterator inputadapter_input;
/* 013 */   private org.apache.spark.sql.execution.metric.SQLMetric sort_peakMemory;
/* 014 */   private org.apache.spark.sql.execution.metric.SQLMetric sort_spillSize;
/* 015 */   private org.apache.spark.sql.execution.metric.SQLMetric sort_sortTime;
/* 016 */
/* 017 */   public GeneratedIterator(Object[] references) {
/* 018 */     this.references = references;
/* 019 */   }
/* 020 */
/* 021 */   public void init(int index, scala.collection.Iterator inputs[]) {
/* 022 */     partitionIndex = index;
/* 023 */     sort_needToSort = true;
/* 024 */     this.sort_plan = (org.apache.spark.sql.execution.SortExec) references[0];
/* 025 */     sort_sorter = sort_plan.createSorter();
/* 026 */     sort_metrics = org.apache.spark.TaskContext.get().taskMetrics();
/* 027 */
/* 028 */     inputadapter_input = inputs[0];
/* 029 */     this.sort_peakMemory = (org.apache.spark.sql.execution.metric.SQLMetric) references[1];
/* 030 */     this.sort_spillSize = (org.apache.spark.sql.execution.metric.SQLMetric) references[2];
/* 031 */     this.sort_sortTime = (org.apache.spark.sql.execution.metric.SQLMetric) references[3];
/* 032 */   }
/* 033 */
/* 034 */   private void sort_addToSorter() throws java.io.IOException {
/* 035 */     while (inputadapter_input.hasNext()) {
/* 036 */       InternalRow inputadapter_row = (InternalRow) inputadapter_input.next();
/* 037 */       sort_sorter.insertRow((UnsafeRow)inputadapter_row);
/* 038 */       if (shouldStop()) return;
/* 039 */     }
/* 040 */
/* 041 */   }
/* 042 */
/* 043 */   protected void processNext() throws java.io.IOException {
/* 044 */     if (sort_needToSort) {
/* 045 */       long sort_spillSizeBefore = sort_metrics.memoryBytesSpilled();
/* 046 */       sort_addToSorter();
/* 047 */       sort_sortedIter = sort_sorter.sort();
/* 048 */       sort_sortTime.add(sort_sorter.getSortTimeNanos() / 1000000);
/* 049 */       sort_peakMemory.add(sort_sorter.getPeakMemoryUsage());
/* 050 */       sort_spillSize.add(sort_metrics.memoryBytesSpilled() - sort_spillSizeBefore);
/* 051 */       sort_metrics.incPeakExecutionMemory(sort_sorter.getPeakMemoryUsage());
/* 052 */       sort_needToSort = false;
/* 053 */     }
/* 054 */
/* 055 */     while (sort_sortedIter.hasNext()) {
/* 056 */       UnsafeRow sort_outputRow = (UnsafeRow)sort_sortedIter.next();
/* 057 */
/* 058 */       append(sort_outputRow);
/* 059 */
/* 060 */       if (shouldStop()) return;
/* 061 */     }
/* 062 */   }
/* 063 */ }

13854 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13854 [main] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13855 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
13855 [main] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
13856 [main] DEBUG org.apache.spark.internal.Logging$class  - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIterator(references);
/* 003 */ }
/* 004 */
/* 005 */ final class GeneratedIterator extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 006 */   private Object[] references;
/* 007 */   private org.apache.spark.sql.execution.metric.SQLMetric scan_numOutputRows;
/* 008 */   private scala.collection.Iterator scan_input;
/* 009 */   private UnsafeRow scan_result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder scan_holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter scan_rowWriter;
/* 012 */   private UnsafeRow project_result;
/* 013 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder project_holder;
/* 014 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter project_rowWriter;
/* 015 */
/* 016 */   public GeneratedIterator(Object[] references) {
/* 017 */     this.references = references;
/* 018 */   }
/* 019 */
/* 020 */   public void init(int index, scala.collection.Iterator inputs[]) {
/* 021 */     partitionIndex = index;
/* 022 */     this.scan_numOutputRows = (org.apache.spark.sql.execution.metric.SQLMetric) references[0];
/* 023 */     scan_input = inputs[0];
/* 024 */     scan_result = new UnsafeRow(2);
/* 025 */     this.scan_holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(scan_result, 0);
/* 026 */     this.scan_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(scan_holder, 2);
/* 027 */     project_result = new UnsafeRow(2);
/* 028 */     this.project_holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(project_result, 0);
/* 029 */     this.project_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(project_holder, 2);
/* 030 */   }
/* 031 */
/* 032 */   protected void processNext() throws java.io.IOException {
/* 033 */     while (scan_input.hasNext()) {
/* 034 */       InternalRow scan_row = (InternalRow) scan_input.next();
/* 035 */       scan_numOutputRows.add(1);
/* 036 */       boolean scan_isNull = scan_row.isNullAt(0);
/* 037 */       int scan_value = scan_isNull ? -1 : (scan_row.getInt(0));
/* 038 */       boolean scan_isNull1 = scan_row.isNullAt(1);
/* 039 */       int scan_value1 = scan_isNull1 ? -1 : (scan_row.getInt(1));
/* 040 */       project_rowWriter.zeroOutNullBytes();
/* 041 */
/* 042 */       if (scan_isNull) {
/* 043 */         project_rowWriter.setNullAt(0);
/* 044 */       } else {
/* 045 */         project_rowWriter.write(0, scan_value);
/* 046 */       }
/* 047 */
/* 048 */       if (scan_isNull1) {
/* 049 */         project_rowWriter.setNullAt(1);
/* 050 */       } else {
/* 051 */         project_rowWriter.write(1, scan_value1);
/* 052 */       }
/* 053 */       append(project_result);
/* 054 */       if (shouldStop()) return;
/* 055 */     }
/* 056 */   }
/* 057 */ }

13857 [main] DEBUG org.apache.spark.internal.Logging$class  - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8) +++
13859 [main] DEBUG org.apache.spark.internal.Logging$class  -  + declared fields: 4
13860 [main] DEBUG org.apache.spark.internal.Logging$class  -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8.serialVersionUID
13860 [main] DEBUG org.apache.spark.internal.Logging$class  -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8.cleanedSource$2
13860 [main] DEBUG org.apache.spark.internal.Logging$class  -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8.references$1
13861 [main] DEBUG org.apache.spark.internal.Logging$class  -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8.durationMs$1
13861 [main] DEBUG org.apache.spark.internal.Logging$class  -  + declared methods: 2
13861 [main] DEBUG org.apache.spark.internal.Logging$class  -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8.apply(java.lang.Object,java.lang.Object)
13861 [main] DEBUG org.apache.spark.internal.Logging$class  -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8.apply(int,scala.collection.Iterator)
13862 [main] DEBUG org.apache.spark.internal.Logging$class  -  + inner classes: 1
13862 [main] DEBUG org.apache.spark.internal.Logging$class  -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1
13862 [main] DEBUG org.apache.spark.internal.Logging$class  -  + outer classes: 0
13862 [main] DEBUG org.apache.spark.internal.Logging$class  -  + outer objects: 0
13863 [main] DEBUG org.apache.spark.internal.Logging$class  -  + populating accessed fields because this is the starting closure
13865 [main] DEBUG org.apache.spark.internal.Logging$class  -  + fields accessed by starting closure: 0
13865 [main] DEBUG org.apache.spark.internal.Logging$class  -  + there are no enclosing objects!
13865 [main] DEBUG org.apache.spark.internal.Logging$class  -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8) is now cleaned +++
13868 [main] DEBUG org.apache.spark.internal.Logging$class  - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8) +++
13870 [main] DEBUG org.apache.spark.internal.Logging$class  -  + declared fields: 4
13870 [main] DEBUG org.apache.spark.internal.Logging$class  -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8.serialVersionUID
13870 [main] DEBUG org.apache.spark.internal.Logging$class  -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8.cleanedSource$2
13870 [main] DEBUG org.apache.spark.internal.Logging$class  -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8.references$1
13870 [main] DEBUG org.apache.spark.internal.Logging$class  -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8.durationMs$1
13870 [main] DEBUG org.apache.spark.internal.Logging$class  -  + declared methods: 2
13871 [main] DEBUG org.apache.spark.internal.Logging$class  -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8.apply(java.lang.Object,java.lang.Object)
13871 [main] DEBUG org.apache.spark.internal.Logging$class  -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8.apply(int,scala.collection.Iterator)
13871 [main] DEBUG org.apache.spark.internal.Logging$class  -  + inner classes: 1
13871 [main] DEBUG org.apache.spark.internal.Logging$class  -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1
13871 [main] DEBUG org.apache.spark.internal.Logging$class  -  + outer classes: 0
13871 [main] DEBUG org.apache.spark.internal.Logging$class  -  + outer objects: 0
13872 [main] DEBUG org.apache.spark.internal.Logging$class  -  + populating accessed fields because this is the starting closure
13873 [main] DEBUG org.apache.spark.internal.Logging$class  -  + fields accessed by starting closure: 0
13873 [main] DEBUG org.apache.spark.internal.Logging$class  -  + there are no enclosing objects!
13873 [main] DEBUG org.apache.spark.internal.Logging$class  -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8) is now cleaned +++
13877 [main] DEBUG org.apache.spark.internal.Logging$class  - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIterator(references);
/* 003 */ }
/* 004 */
/* 005 */ final class GeneratedIterator extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 006 */   private Object[] references;
/* 007 */   private boolean sort_needToSort;
/* 008 */   private org.apache.spark.sql.execution.SortExec sort_plan;
/* 009 */   private org.apache.spark.sql.execution.UnsafeExternalRowSorter sort_sorter;
/* 010 */   private org.apache.spark.executor.TaskMetrics sort_metrics;
/* 011 */   private scala.collection.Iterator<UnsafeRow> sort_sortedIter;
/* 012 */   private scala.collection.Iterator inputadapter_input;
/* 013 */   private org.apache.spark.sql.execution.metric.SQLMetric sort_peakMemory;
/* 014 */   private org.apache.spark.sql.execution.metric.SQLMetric sort_spillSize;
/* 015 */   private org.apache.spark.sql.execution.metric.SQLMetric sort_sortTime;
/* 016 */
/* 017 */   public GeneratedIterator(Object[] references) {
/* 018 */     this.references = references;
/* 019 */   }
/* 020 */
/* 021 */   public void init(int index, scala.collection.Iterator inputs[]) {
/* 022 */     partitionIndex = index;
/* 023 */     sort_needToSort = true;
/* 024 */     this.sort_plan = (org.apache.spark.sql.execution.SortExec) references[0];
/* 025 */     sort_sorter = sort_plan.createSorter();
/* 026 */     sort_metrics = org.apache.spark.TaskContext.get().taskMetrics();
/* 027 */
/* 028 */     inputadapter_input = inputs[0];
/* 029 */     this.sort_peakMemory = (org.apache.spark.sql.execution.metric.SQLMetric) references[1];
/* 030 */     this.sort_spillSize = (org.apache.spark.sql.execution.metric.SQLMetric) references[2];
/* 031 */     this.sort_sortTime = (org.apache.spark.sql.execution.metric.SQLMetric) references[3];
/* 032 */   }
/* 033 */
/* 034 */   private void sort_addToSorter() throws java.io.IOException {
/* 035 */     while (inputadapter_input.hasNext()) {
/* 036 */       InternalRow inputadapter_row = (InternalRow) inputadapter_input.next();
/* 037 */       sort_sorter.insertRow((UnsafeRow)inputadapter_row);
/* 038 */       if (shouldStop()) return;
/* 039 */     }
/* 040 */
/* 041 */   }
/* 042 */
/* 043 */   protected void processNext() throws java.io.IOException {
/* 044 */     if (sort_needToSort) {
/* 045 */       long sort_spillSizeBefore = sort_metrics.memoryBytesSpilled();
/* 046 */       sort_addToSorter();
/* 047 */       sort_sortedIter = sort_sorter.sort();
/* 048 */       sort_sortTime.add(sort_sorter.getSortTimeNanos() / 1000000);
/* 049 */       sort_peakMemory.add(sort_sorter.getPeakMemoryUsage());
/* 050 */       sort_spillSize.add(sort_metrics.memoryBytesSpilled() - sort_spillSizeBefore);
/* 051 */       sort_metrics.incPeakExecutionMemory(sort_sorter.getPeakMemoryUsage());
/* 052 */       sort_needToSort = false;
/* 053 */     }
/* 054 */
/* 055 */     while (sort_sortedIter.hasNext()) {
/* 056 */       UnsafeRow sort_outputRow = (UnsafeRow)sort_sortedIter.next();
/* 057 */
/* 058 */       append(sort_outputRow);
/* 059 */
/* 060 */       if (shouldStop()) return;
/* 061 */     }
/* 062 */   }
/* 063 */ }

13878 [main] DEBUG org.apache.spark.internal.Logging$class  - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIterator(references);
/* 003 */ }
/* 004 */
/* 005 */ final class GeneratedIterator extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 006 */   private Object[] references;
/* 007 */   private org.apache.spark.sql.execution.metric.SQLMetric scan_numOutputRows;
/* 008 */   private scala.collection.Iterator scan_input;
/* 009 */   private UnsafeRow scan_result;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder scan_holder;
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter scan_rowWriter;
/* 012 */
/* 013 */   public GeneratedIterator(Object[] references) {
/* 014 */     this.references = references;
/* 015 */   }
/* 016 */
/* 017 */   public void init(int index, scala.collection.Iterator inputs[]) {
/* 018 */     partitionIndex = index;
/* 019 */     this.scan_numOutputRows = (org.apache.spark.sql.execution.metric.SQLMetric) references[0];
/* 020 */     scan_input = inputs[0];
/* 021 */     scan_result = new UnsafeRow(2);
/* 022 */     this.scan_holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(scan_result, 32);
/* 023 */     this.scan_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(scan_holder, 2);
/* 024 */   }
/* 025 */
/* 026 */   protected void processNext() throws java.io.IOException {
/* 027 */     while (scan_input.hasNext()) {
/* 028 */       InternalRow scan_row = (InternalRow) scan_input.next();
/* 029 */       scan_numOutputRows.add(1);
/* 030 */       int scan_value = scan_row.getInt(0);
/* 031 */       boolean scan_isNull1 = scan_row.isNullAt(1);
/* 032 */       UTF8String scan_value1 = scan_isNull1 ? null : (scan_row.getUTF8String(1));
/* 033 */       scan_holder.reset();
/* 034 */
/* 035 */       scan_rowWriter.zeroOutNullBytes();
/* 036 */
/* 037 */       scan_rowWriter.write(0, scan_value);
/* 038 */
/* 039 */       if (scan_isNull1) {
/* 040 */         scan_rowWriter.setNullAt(1);
/* 041 */       } else {
/* 042 */         scan_rowWriter.write(1, scan_value1);
/* 043 */       }
/* 044 */       scan_result.setTotalSize(scan_holder.totalSize());
/* 045 */       append(scan_result);
/* 046 */       if (shouldStop()) return;
/* 047 */     }
/* 048 */   }
/* 049 */ }

13879 [main] DEBUG org.apache.spark.internal.Logging$class  - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8) +++
13880 [main] DEBUG org.apache.spark.internal.Logging$class  -  + declared fields: 4
13881 [main] DEBUG org.apache.spark.internal.Logging$class  -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8.serialVersionUID
13881 [main] DEBUG org.apache.spark.internal.Logging$class  -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8.cleanedSource$2
13881 [main] DEBUG org.apache.spark.internal.Logging$class  -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8.references$1
13881 [main] DEBUG org.apache.spark.internal.Logging$class  -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8.durationMs$1
13881 [main] DEBUG org.apache.spark.internal.Logging$class  -  + declared methods: 2
13882 [main] DEBUG org.apache.spark.internal.Logging$class  -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8.apply(java.lang.Object,java.lang.Object)
13882 [main] DEBUG org.apache.spark.internal.Logging$class  -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8.apply(int,scala.collection.Iterator)
13882 [main] DEBUG org.apache.spark.internal.Logging$class  -  + inner classes: 1
13882 [main] DEBUG org.apache.spark.internal.Logging$class  -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1
13882 [main] DEBUG org.apache.spark.internal.Logging$class  -  + outer classes: 0
13883 [main] DEBUG org.apache.spark.internal.Logging$class  -  + outer objects: 0
13883 [main] DEBUG org.apache.spark.internal.Logging$class  -  + populating accessed fields because this is the starting closure
13885 [main] DEBUG org.apache.spark.internal.Logging$class  -  + fields accessed by starting closure: 0
13885 [main] DEBUG org.apache.spark.internal.Logging$class  -  + there are no enclosing objects!
13885 [main] DEBUG org.apache.spark.internal.Logging$class  -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8) is now cleaned +++
13888 [main] DEBUG org.apache.spark.internal.Logging$class  - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8) +++
13889 [main] DEBUG org.apache.spark.internal.Logging$class  -  + declared fields: 4
13889 [main] DEBUG org.apache.spark.internal.Logging$class  -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8.serialVersionUID
13889 [main] DEBUG org.apache.spark.internal.Logging$class  -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8.cleanedSource$2
13889 [main] DEBUG org.apache.spark.internal.Logging$class  -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8.references$1
13889 [main] DEBUG org.apache.spark.internal.Logging$class  -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8.durationMs$1
13890 [main] DEBUG org.apache.spark.internal.Logging$class  -  + declared methods: 2
13890 [main] DEBUG org.apache.spark.internal.Logging$class  -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8.apply(java.lang.Object,java.lang.Object)
13890 [main] DEBUG org.apache.spark.internal.Logging$class  -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8.apply(int,scala.collection.Iterator)
13890 [main] DEBUG org.apache.spark.internal.Logging$class  -  + inner classes: 1
13890 [main] DEBUG org.apache.spark.internal.Logging$class  -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1
13890 [main] DEBUG org.apache.spark.internal.Logging$class  -  + outer classes: 0
13891 [main] DEBUG org.apache.spark.internal.Logging$class  -  + outer objects: 0
13891 [main] DEBUG org.apache.spark.internal.Logging$class  -  + populating accessed fields because this is the starting closure
13892 [main] DEBUG org.apache.spark.internal.Logging$class  -  + fields accessed by starting closure: 0
13893 [main] DEBUG org.apache.spark.internal.Logging$class  -  + there are no enclosing objects!
13893 [main] DEBUG org.apache.spark.internal.Logging$class  -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8) is now cleaned +++
13894 [main] DEBUG org.apache.spark.internal.Logging$class  - +++ Cleaning closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$doExecute$3) +++
13896 [main] DEBUG org.apache.spark.internal.Logging$class  -  + declared fields: 4
13896 [main] DEBUG org.apache.spark.internal.Logging$class  -      public static final long org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$doExecute$3.serialVersionUID
13896 [main] DEBUG org.apache.spark.internal.Logging$class  -      private final org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$doExecute$3.cleanedSource$2
13896 [main] DEBUG org.apache.spark.internal.Logging$class  -      private final java.lang.Object[] org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$doExecute$3.references$1
13896 [main] DEBUG org.apache.spark.internal.Logging$class  -      public final org.apache.spark.sql.execution.metric.SQLMetric org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$doExecute$3.durationMs$1
13896 [main] DEBUG org.apache.spark.internal.Logging$class  -  + declared methods: 2
13896 [main] DEBUG org.apache.spark.internal.Logging$class  -      public final java.lang.Object org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$doExecute$3.apply(java.lang.Object,java.lang.Object)
13897 [main] DEBUG org.apache.spark.internal.Logging$class  -      public final scala.collection.Iterator org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$doExecute$3.apply(scala.collection.Iterator,scala.collection.Iterator)
13897 [main] DEBUG org.apache.spark.internal.Logging$class  -  + inner classes: 1
13897 [main] DEBUG org.apache.spark.internal.Logging$class  -      org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$doExecute$3$$anon$2
13897 [main] DEBUG org.apache.spark.internal.Logging$class  -  + outer classes: 0
13897 [main] DEBUG org.apache.spark.internal.Logging$class  -  + outer objects: 0
13898 [main] DEBUG org.apache.spark.internal.Logging$class  -  + populating accessed fields because this is the starting closure
13899 [main] DEBUG org.apache.spark.internal.Logging$class  -  + fields accessed by starting closure: 0
13899 [main] DEBUG org.apache.spark.internal.Logging$class  -  + there are no enclosing objects!
13899 [main] DEBUG org.apache.spark.internal.Logging$class  -  +++ closure <function2> (org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$doExecute$3) is now cleaned +++
13904 [main] DEBUG org.apache.spark.internal.Logging$class  - +++ Cleaning closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) +++
13905 [main] DEBUG org.apache.spark.internal.Logging$class  -  + declared fields: 1
13905 [main] DEBUG org.apache.spark.internal.Logging$class  -      public static final long org.apache.spark.sql.execution.SparkPlan$$anonfun$5.serialVersionUID
13905 [main] DEBUG org.apache.spark.internal.Logging$class  -  + declared methods: 2
13905 [main] DEBUG org.apache.spark.internal.Logging$class  -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(java.lang.Object)
13905 [main] DEBUG org.apache.spark.internal.Logging$class  -      public final java.lang.Object org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(scala.collection.Iterator)
13906 [main] DEBUG org.apache.spark.internal.Logging$class  -  + inner classes: 0
13906 [main] DEBUG org.apache.spark.internal.Logging$class  -  + outer classes: 0
13906 [main] DEBUG org.apache.spark.internal.Logging$class  -  + outer objects: 0
13906 [main] DEBUG org.apache.spark.internal.Logging$class  -  + populating accessed fields because this is the starting closure
13907 [main] DEBUG org.apache.spark.internal.Logging$class  -  + fields accessed by starting closure: 0
13907 [main] DEBUG org.apache.spark.internal.Logging$class  -  + there are no enclosing objects!
13907 [main] DEBUG org.apache.spark.internal.Logging$class  -  +++ closure <function1> (org.apache.spark.sql.execution.SparkPlan$$anonfun$5) is now cleaned +++
13907 [main] DEBUG org.apache.spark.internal.Logging$class  - +++ Cleaning closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
13908 [main] DEBUG org.apache.spark.internal.Logging$class  -  + declared fields: 2
13908 [main] DEBUG org.apache.spark.internal.Logging$class  -      public static final long org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
13908 [main] DEBUG org.apache.spark.internal.Logging$class  -      private final scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
13908 [main] DEBUG org.apache.spark.internal.Logging$class  -  + declared methods: 2
13908 [main] DEBUG org.apache.spark.internal.Logging$class  -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.Object,java.lang.Object)
13908 [main] DEBUG org.apache.spark.internal.Logging$class  -      public final java.lang.Object org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
13909 [main] DEBUG org.apache.spark.internal.Logging$class  -  + inner classes: 0
13909 [main] DEBUG org.apache.spark.internal.Logging$class  -  + outer classes: 0
13909 [main] DEBUG org.apache.spark.internal.Logging$class  -  + outer objects: 0
13909 [main] DEBUG org.apache.spark.internal.Logging$class  -  + populating accessed fields because this is the starting closure
13909 [main] DEBUG org.apache.spark.internal.Logging$class  -  + fields accessed by starting closure: 0
13910 [main] DEBUG org.apache.spark.internal.Logging$class  -  + there are no enclosing objects!
13910 [main] DEBUG org.apache.spark.internal.Logging$class  -  +++ closure <function2> (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
13910 [main] INFO  org.apache.spark.internal.Logging$class  - Starting job: show at DeduplicationF1.scala:65
13910 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - Registering RDD 28 (show at DeduplicationF1.scala:65)
13911 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - Registering RDD 24 (show at DeduplicationF1.scala:65)
13911 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - Got job 1 (show at DeduplicationF1.scala:65) with 1 output partitions
13911 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - Final stage: ResultStage 6 (show at DeduplicationF1.scala:65)
13911 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - Parents of final stage: List(ShuffleMapStage 5, ShuffleMapStage 4)
13911 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - Missing parents: List(ShuffleMapStage 5, ShuffleMapStage 4)
13911 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 6)
13912 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 4, ShuffleMapStage 5)
13912 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 4)
13912 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List()
13912 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - Submitting ShuffleMapStage 4 (MapPartitionsRDD[28] at show at DeduplicationF1.scala:65), which has no missing parents
13912 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitMissingTasks(ShuffleMapStage 4)
13913 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 trying to put broadcast_4
13913 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 trying to acquire read lock for broadcast_4
13913 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 trying to acquire write lock for broadcast_4
13913 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 acquired write lock for broadcast_4
13914 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - Block broadcast_4 stored as values in memory (estimated size 9.8 KB, free 2004.5 MB)
13914 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - Put block broadcast_4 locally took  1 ms
13914 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 releasing lock for broadcast_4
13914 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - Putting block broadcast_4 without replication took  1 ms
13915 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 trying to put broadcast_4_piece0
13915 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 trying to acquire read lock for broadcast_4_piece0
13915 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 trying to acquire write lock for broadcast_4_piece0
13915 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 acquired write lock for broadcast_4_piece0
13915 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.0 KB, free 2004.5 MB)
13916 [dispatcher-event-loop-4] INFO  org.apache.spark.internal.Logging$class  - Added broadcast_4_piece0 in memory on 192.168.1.4:59832 (size: 5.0 KB, free: 2004.6 MB)
13916 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - Updated info of block broadcast_4_piece0
13916 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - Told master about block broadcast_4_piece0
13916 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - Put block broadcast_4_piece0 locally took  1 ms
13916 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 releasing lock for broadcast_4_piece0
13916 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - Putting block broadcast_4_piece0 without replication took  1 ms
13917 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - Created broadcast 4 from broadcast at DAGScheduler.scala:1012
13917 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[28] at show at DeduplicationF1.scala:65)
13917 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - New pending partitions: Set(0)
13917 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - Adding task set 4.0 with 1 tasks
13917 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - Epoch for TaskSet 4.0: 3
13917 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - Valid locality levels for TaskSet 4.0: NO_PREF, ANY
13918 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 5)
13918 [dispatcher-event-loop-2] DEBUG org.apache.spark.internal.Logging$class  - parentName: , name: TaskSet_4, runningTasks: 0
13918 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List()
13918 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - Submitting ShuffleMapStage 5 (MapPartitionsRDD[24] at show at DeduplicationF1.scala:65), which has no missing parents
13918 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitMissingTasks(ShuffleMapStage 5)
13919 [dispatcher-event-loop-2] INFO  org.apache.spark.internal.Logging$class  - Starting task 0.0 in stage 4.0 (TID 203, localhost, partition 0, PROCESS_LOCAL, 5131 bytes)
13919 [dispatcher-event-loop-2] DEBUG org.apache.spark.internal.Logging$class  - No tasks for locality level NO_PREF, so moving to locality level ANY
13919 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Running task 0.0 in stage 4.0 (TID 203)
13919 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 trying to put broadcast_5
13919 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 trying to acquire read lock for broadcast_5
13919 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Task 203's epoch is 3
13919 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 trying to acquire write lock for broadcast_5
13920 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 acquired write lock for broadcast_5
13920 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Getting local block broadcast_4
13920 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Task 203 trying to acquire read lock for broadcast_4
13920 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Task 203 acquired read lock for broadcast_4
13920 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - Block broadcast_5 stored as values in memory (estimated size 10.4 KB, free 2004.5 MB)
13920 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - Put block broadcast_5 locally took  0 ms
13921 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 releasing lock for broadcast_5
13920 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - Level for block broadcast_4 is StorageLevel(disk, memory, deserialized, 1 replicas)
13921 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - Putting block broadcast_5 without replication took  1 ms
13922 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 trying to put broadcast_5_piece0
13922 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 trying to acquire read lock for broadcast_5_piece0
13922 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 trying to acquire write lock for broadcast_5_piece0
13922 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 acquired write lock for broadcast_5_piece0
13922 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.1 KB, free 2004.5 MB)
13922 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Wrapper for org.postgresql.Driver already exists
13923 [dispatcher-event-loop-3] INFO  org.apache.spark.internal.Logging$class  - Added broadcast_5_piece0 in memory on 192.168.1.4:59832 (size: 5.1 KB, free: 2004.6 MB)
13923 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - Updated info of block broadcast_5_piece0
13923 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - Told master about block broadcast_5_piece0
13923 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - Put block broadcast_5_piece0 locally took  1 ms
13923 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 releasing lock for broadcast_5_piece0
13923 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - Putting block broadcast_5_piece0 without replication took  1 ms
13924 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - Created broadcast 5 from broadcast at DAGScheduler.scala:1012
13924 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[24] at show at DeduplicationF1.scala:65)
13924 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - New pending partitions: Set(0)
13924 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - Adding task set 5.0 with 1 tasks
13924 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - Epoch for TaskSet 5.0: 3
13924 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - Valid locality levels for TaskSet 5.0: NO_PREF, ANY
13924 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13925 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 5, ShuffleMapStage 4)
13925 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 6)
13925 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13925 [dispatcher-event-loop-1] DEBUG org.apache.spark.internal.Logging$class  - parentName: , name: TaskSet_4, runningTasks: 1
13925 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 6)
13925 [dispatcher-event-loop-1] DEBUG org.apache.spark.internal.Logging$class  - parentName: , name: TaskSet_5, runningTasks: 0
13925 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 4, ShuffleMapStage 5)
13925 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 4)
13926 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 5)
13926 [dispatcher-event-loop-1] INFO  org.apache.spark.internal.Logging$class  - Starting task 0.0 in stage 5.0 (TID 204, localhost, partition 0, PROCESS_LOCAL, 5131 bytes)
13926 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13926 [dispatcher-event-loop-1] DEBUG org.apache.spark.internal.Logging$class  - No tasks for locality level NO_PREF, so moving to locality level ANY
13926 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 5, ShuffleMapStage 4)
13926 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 6)
13926 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13926 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 6)
13926 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 4, ShuffleMapStage 5)
13926 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 4)
13926 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 5)
13927 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
13927 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 5, ShuffleMapStage 4)
13927 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 6)
13927 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
13927 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 6)
13927 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 4, ShuffleMapStage 5)
13927 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 4)
13927 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 5)
13926 [Executor task launch worker-4] INFO  org.apache.spark.internal.Logging$class  - Running task 0.0 in stage 5.0 (TID 204)
13929 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Task 204's epoch is 3
13929 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Getting local block broadcast_5
13929 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Task 204 trying to acquire read lock for broadcast_5
13929 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Task 204 acquired read lock for broadcast_5
13929 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Level for block broadcast_5 is StorageLevel(disk, memory, deserialized, 1 replicas)
13931 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Wrapper for org.postgresql.Driver already exists
14101 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
14101 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
14102 [Executor task launch worker-1] DEBUG org.apache.spark.internal.Logging$class  - code for pmod(hash(input[0, int, false], 42), 200):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     boolean isNull = false;
/* 027 */
/* 028 */     int value1 = 42;
/* 029 */
/* 030 */     int value2 = i.getInt(0);
/* 031 */     value1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value2, value1);
/* 032 */
/* 033 */     int value = -1;
/* 034 */
/* 035 */     int remainder = value1 % 200;
/* 036 */     if (remainder < 0) {
/* 037 */       value = (remainder + 200) % 200;
/* 038 */     } else {
/* 039 */       value = remainder;
/* 040 */     }
/* 041 */     rowWriter.write(0, value);
/* 042 */     return result;
/* 043 */   }
/* 044 */ }

14249 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - closed connection
14330 [Executor task launch worker-1] TRACE org.apache.spark.internal.Logging$class  - Task 203 releasing lock for broadcast_4
14332 [Executor task launch worker-1] INFO  org.apache.spark.internal.Logging$class  - Finished task 0.0 in stage 4.0 (TID 203). 1850 bytes result sent to driver
14332 [dispatcher-event-loop-4] DEBUG org.apache.spark.internal.Logging$class  - parentName: , name: TaskSet_4, runningTasks: 0
14332 [dispatcher-event-loop-4] DEBUG org.apache.spark.internal.Logging$class  - parentName: , name: TaskSet_5, runningTasks: 1
14332 [task-result-getter-3] INFO  org.apache.spark.internal.Logging$class  - Finished task 0.0 in stage 4.0 (TID 203) in 414 ms on localhost (1/1)
14333 [task-result-getter-3] INFO  org.apache.spark.internal.Logging$class  - Removed TaskSet 4.0, whose tasks have all completed, from pool 
14333 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - ShuffleMapTask finished on driver
14333 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - ShuffleMapStage 4 (show at DeduplicationF1.scala:65) finished in 0.416 s
14333 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - looking for newly runnable stages
14333 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 5)
14333 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 6)
14333 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - failed: Set()
14333 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - Increasing epoch to 4
14333 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
14333 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ShuffleMapStage 5)
14334 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 6)
14334 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
14334 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 6)
14334 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List(ShuffleMapStage 5)
14334 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ShuffleMapStage 5)
15087 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
15087 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
15087 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for pmod(hash(input[1, int, true], 42), 200):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     boolean isNull = false;
/* 027 */
/* 028 */     int value1 = 42;
/* 029 */
/* 030 */     boolean isNull2 = i.isNullAt(1);
/* 031 */     int value2 = isNull2 ? -1 : (i.getInt(1));
/* 032 */     if (!isNull2) {
/* 033 */       value1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value2, value1);
/* 034 */     }
/* 035 */
/* 036 */     int value = -1;
/* 037 */
/* 038 */     int remainder = value1 % 200;
/* 039 */     if (remainder < 0) {
/* 040 */       value = (remainder + 200) % 200;
/* 041 */     } else {
/* 042 */       value = remainder;
/* 043 */     }
/* 044 */     rowWriter.write(0, value);
/* 045 */     return result;
/* 046 */   }
/* 047 */ }

15286 [Executor task launch worker-4] INFO  org.apache.spark.internal.Logging$class  - closed connection
15355 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Task 204 releasing lock for broadcast_5
15356 [Executor task launch worker-4] INFO  org.apache.spark.internal.Logging$class  - Finished task 0.0 in stage 5.0 (TID 204). 1763 bytes result sent to driver
15356 [dispatcher-event-loop-2] DEBUG org.apache.spark.internal.Logging$class  - parentName: , name: TaskSet_5, runningTasks: 0
15357 [task-result-getter-0] INFO  org.apache.spark.internal.Logging$class  - Finished task 0.0 in stage 5.0 (TID 204) in 1431 ms on localhost (1/1)
15357 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - ShuffleMapTask finished on driver
15357 [task-result-getter-0] INFO  org.apache.spark.internal.Logging$class  - Removed TaskSet 5.0, whose tasks have all completed, from pool 
15357 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - ShuffleMapStage 5 (show at DeduplicationF1.scala:65) finished in 1.433 s
15357 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - looking for newly runnable stages
15357 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - running: Set()
15357 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 6)
15357 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - failed: Set()
15357 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - Increasing epoch to 5
15358 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
15358 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set()
15358 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set(ResultStage 6)
15358 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
15358 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitStage(ResultStage 6)
15358 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - missing: List()
15358 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - Submitting ResultStage 6 (MapPartitionsRDD[32] at show at DeduplicationF1.scala:65), which has no missing parents
15358 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - submitMissingTasks(ResultStage 6)
15360 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 trying to put broadcast_6
15360 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 trying to acquire read lock for broadcast_6
15360 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 trying to acquire write lock for broadcast_6
15360 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 acquired write lock for broadcast_6
15361 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - Block broadcast_6 stored as values in memory (estimated size 37.0 KB, free 2004.4 MB)
15361 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - Put block broadcast_6 locally took  1 ms
15361 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 releasing lock for broadcast_6
15361 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - Putting block broadcast_6 without replication took  1 ms
15362 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 trying to put broadcast_6_piece0
15363 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 trying to acquire read lock for broadcast_6_piece0
15363 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 trying to acquire write lock for broadcast_6_piece0
15363 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 acquired write lock for broadcast_6_piece0
15363 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.1 KB, free 2004.4 MB)
15363 [dispatcher-event-loop-6] INFO  org.apache.spark.internal.Logging$class  - Added broadcast_6_piece0 in memory on 192.168.1.4:59832 (size: 14.1 KB, free: 2004.5 MB)
15364 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - Updated info of block broadcast_6_piece0
15364 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - Told master about block broadcast_6_piece0
15364 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - Put block broadcast_6_piece0 locally took  1 ms
15364 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Task -1024 releasing lock for broadcast_6_piece0
15364 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - Putting block broadcast_6_piece0 without replication took  1 ms
15364 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - Created broadcast 6 from broadcast at DAGScheduler.scala:1012
15365 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[32] at show at DeduplicationF1.scala:65)
15365 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - New pending partitions: Set(0)
15365 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - Adding task set 6.0 with 1 tasks
15365 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - Epoch for TaskSet 6.0: 5
15365 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - Valid locality levels for TaskSet 6.0: ANY
15366 [dispatcher-event-loop-5] DEBUG org.apache.spark.internal.Logging$class  - parentName: , name: TaskSet_6, runningTasks: 0
15366 [dispatcher-event-loop-5] INFO  org.apache.spark.internal.Logging$class  - Starting task 0.0 in stage 6.0 (TID 205, localhost, partition 0, ANY, 5547 bytes)
15366 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
15367 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set(ResultStage 6)
15367 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set()
15367 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
15367 [Executor task launch worker-4] INFO  org.apache.spark.internal.Logging$class  - Running task 0.0 in stage 6.0 (TID 205)
15368 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Task 205's epoch is 5
15369 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Getting local block broadcast_6
15369 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Task 205 trying to acquire read lock for broadcast_6
15369 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Task 205 acquired read lock for broadcast_6
15369 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Level for block broadcast_6 is StorageLevel(disk, memory, deserialized, 1 replicas)
15371 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 3, partitions 0-1
15371 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
15371 [Executor task launch worker-4] INFO  org.apache.spark.internal.Logging$class  - Getting 1 non-empty blocks out of 1 blocks
15371 [Executor task launch worker-4] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 0 ms
15372 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  1 ms
15372 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
15372 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
15373 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[1, int, true] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       boolean isNull = i.isNullAt(1);
/* 025 */       int value = isNull ? -1 : (i.getInt(1));
/* 026 */       isNullA = isNull;
/* 027 */       primitiveA = value;
/* 028 */     }
/* 029 */     i = b;
/* 030 */     boolean isNullB;
/* 031 */     int primitiveB;
/* 032 */     {
/* 033 */
/* 034 */       boolean isNull = i.isNullAt(1);
/* 035 */       int value = isNull ? -1 : (i.getInt(1));
/* 036 */       isNullB = isNull;
/* 037 */       primitiveB = value;
/* 038 */     }
/* 039 */     if (isNullA && isNullB) {
/* 040 */       // Nothing
/* 041 */     } else if (isNullA) {
/* 042 */       return -1;
/* 043 */     } else if (isNullB) {
/* 044 */       return 1;
/* 045 */     } else {
/* 046 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 047 */       if (comp != 0) {
/* 048 */         return comp;
/* 049 */       }
/* 050 */     }
/* 051 */
/* 052 */     return 0;
/* 053 */   }
/* 054 */ }

15373 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
15373 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
15374 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[1, int, true] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     boolean isNull1 = i.isNullAt(1);
/* 030 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 031 */     long value = -9223372036854775808L;
/* 032 */     boolean isNull = false;
/* 033 */     if (!isNull1) {
/* 034 */       value = (long) value1;
/* 035 */     }
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */     return result;
/* 042 */   }
/* 043 */ }

15374 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 205 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@7e33b76d
15374 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (65536 bytes)
15374 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Fetching outputs for shuffle 4, partitions 0-1
15374 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - maxBytesInFlight: 50331648, targetRequestSize: 10066329
15374 [Executor task launch worker-4] INFO  org.apache.spark.internal.Logging$class  - Getting 1 non-empty blocks out of 1 blocks
15374 [Executor task launch worker-4] INFO  org.apache.spark.internal.Logging$class  - Started 0 remote fetches in 0 ms
15374 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Got local blocks in  0 ms
15375 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
15375 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
15375 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - Generated Ordering by input[0, int, false] ASC:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */
/* 011 */   public SpecificOrdering(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public int compare(InternalRow a, InternalRow b) {
/* 017 */     InternalRow i = null;  // Holds current row being evaluated.
/* 018 */
/* 019 */     i = a;
/* 020 */     boolean isNullA;
/* 021 */     int primitiveA;
/* 022 */     {
/* 023 */
/* 024 */       int value = i.getInt(0);
/* 025 */       isNullA = false;
/* 026 */       primitiveA = value;
/* 027 */     }
/* 028 */     i = b;
/* 029 */     boolean isNullB;
/* 030 */     int primitiveB;
/* 031 */     {
/* 032 */
/* 033 */       int value = i.getInt(0);
/* 034 */       isNullB = false;
/* 035 */       primitiveB = value;
/* 036 */     }
/* 037 */     if (isNullA && isNullB) {
/* 038 */       // Nothing
/* 039 */     } else if (isNullA) {
/* 040 */       return -1;
/* 041 */     } else if (isNullB) {
/* 042 */       return 1;
/* 043 */     } else {
/* 044 */       int comp = (primitiveA > primitiveB ? 1 : primitiveA < primitiveB ? -1 : 0);
/* 045 */       if (comp != 0) {
/* 046 */         return comp;
/* 047 */       }
/* 048 */     }
/* 049 */
/* 050 */     return 0;
/* 051 */   }
/* 052 */ }

15376 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
15376 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
15376 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for sortprefix(input[0, int, false] ASC):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(1);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 1);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     rowWriter.zeroOutNullBytes();
/* 027 */
/* 028 */
/* 029 */     int value1 = i.getInt(0);
/* 030 */     long value = -9223372036854775808L;
/* 031 */     boolean isNull = false;
/* 032 */     if (!false) {
/* 033 */       value = (long) value1;
/* 034 */     }
/* 035 */     if (isNull) {
/* 036 */       rowWriter.setNullAt(0);
/* 037 */     } else {
/* 038 */       rowWriter.write(0, value);
/* 039 */     }
/* 040 */     return result;
/* 041 */   }
/* 042 */ }

15376 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 205 acquired 64.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@4d4b953c
15377 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 1 (65536 bytes)
15378 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

15378 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
15378 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
15378 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
15379 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
15379 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
15379 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
15379 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

15380 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 205 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@6141e54c
15380 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 2 (262144 bytes)
15380 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

15381 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

15381 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
15381 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
15381 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
15381 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
15381 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
15381 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
15382 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for input[0, int, true],input[1, int, true],input[2, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull = i.isNullAt(0);
/* 032 */     int value = isNull ? -1 : (i.getInt(0));
/* 033 */     if (isNull) {
/* 034 */       rowWriter.setNullAt(0);
/* 035 */     } else {
/* 036 */       rowWriter.write(0, value);
/* 037 */     }
/* 038 */
/* 039 */
/* 040 */     boolean isNull1 = i.isNullAt(1);
/* 041 */     int value1 = isNull1 ? -1 : (i.getInt(1));
/* 042 */     if (isNull1) {
/* 043 */       rowWriter.setNullAt(1);
/* 044 */     } else {
/* 045 */       rowWriter.write(1, value1);
/* 046 */     }
/* 047 */
/* 048 */
/* 049 */     boolean isNull2 = i.isNullAt(2);
/* 050 */     UTF8String value2 = isNull2 ? null : (i.getUTF8String(2));
/* 051 */     if (isNull2) {
/* 052 */       rowWriter.setNullAt(2);
/* 053 */     } else {
/* 054 */       rowWriter.write(2, value2);
/* 055 */     }
/* 056 */     result.setTotalSize(holder.totalSize());
/* 057 */     return result;
/* 058 */   }
/* 059 */ }

15382 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 205 acquired 256.0 KB for org.apache.spark.unsafe.map.BytesToBytesMap@c779fd1
15382 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 3 (262144 bytes)
15383 [Executor task launch worker-4] DEBUG org.apache.spark.internal.Logging$class  - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(0);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 0);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 0);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */
/* 027 */     return result;
/* 028 */   }
/* 029 */ }

15383 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 205 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@7e33b76d
15383 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 4 (16777216 bytes)
15385 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 205 acquired 128.0 KB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@7e33b76d
15385 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 5 (131072 bytes)
15385 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (65536 bytes)
15385 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 205 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@7e33b76d
15386 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 205 acquired 16.0 MB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@4d4b953c
15386 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (16777216 bytes)
15387 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 205 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@c779fd1
15387 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 6 (16777216 bytes)
15388 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 0 (16777216 bytes)
15388 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 205 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@4d4b953c
15388 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 1 (65536 bytes)
15388 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 205 release 64.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@4d4b953c
15388 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 4 (16777216 bytes)
15388 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 205 release 16.0 MB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@7e33b76d
15388 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 5 (131072 bytes)
15389 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 205 release 128.0 KB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@7e33b76d
15389 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 205 acquired 16.0 MB for org.apache.spark.unsafe.map.BytesToBytesMap@6141e54c
15389 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Allocate page number 0 (16777216 bytes)
15389 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 3 (262144 bytes)
15389 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 205 release 256.0 KB from org.apache.spark.unsafe.map.BytesToBytesMap@c779fd1
15389 [Executor task launch worker-4] TRACE org.apache.spark.memory.TaskMemoryManager  - Freed page number 6 (16777216 bytes)
15389 [Executor task launch worker-4] DEBUG org.apache.spark.memory.TaskMemoryManager  - Task 205 release 16.0 MB from org.apache.spark.unsafe.map.BytesToBytesMap@c779fd1
15389 [Executor task launch worker-4] TRACE org.apache.spark.internal.Logging$class  - Task 205 releasing lock for broadcast_6
15390 [Executor task launch worker-4] WARN  org.apache.spark.memory.TaskMemoryManager  - leak 16.3 MB memory from org.apache.spark.unsafe.map.BytesToBytesMap@6141e54c
15390 [Executor task launch worker-4] WARN  org.apache.spark.memory.TaskMemoryManager  - leak a page: org.apache.spark.unsafe.memory.MemoryBlock@6f1cc9ed in task 205
15390 [Executor task launch worker-4] WARN  org.apache.spark.memory.TaskMemoryManager  - leak a page: org.apache.spark.unsafe.memory.MemoryBlock@7eef4347 in task 205
15392 [Executor task launch worker-4] WARN  org.apache.spark.internal.Logging$class  - Managed memory leak detected; size = 17039360 bytes, TID = 205
15393 [Executor task launch worker-4] INFO  org.apache.spark.internal.Logging$class  - Finished task 0.0 in stage 6.0 (TID 205). 3796 bytes result sent to driver
15393 [dispatcher-event-loop-7] DEBUG org.apache.spark.internal.Logging$class  - parentName: , name: TaskSet_6, runningTasks: 0
15393 [task-result-getter-1] INFO  org.apache.spark.internal.Logging$class  - Finished task 0.0 in stage 6.0 (TID 205) in 27 ms on localhost (1/1)
15393 [task-result-getter-1] INFO  org.apache.spark.internal.Logging$class  - Removed TaskSet 6.0, whose tasks have all completed, from pool 
15393 [dag-scheduler-event-loop] INFO  org.apache.spark.internal.Logging$class  - ResultStage 6 (show at DeduplicationF1.scala:65) finished in 0.028 s
15394 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - After removal of stage 5, remaining stages = 2
15394 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - After removal of stage 4, remaining stages = 1
15394 [dag-scheduler-event-loop] DEBUG org.apache.spark.internal.Logging$class  - After removal of stage 6, remaining stages = 0
15394 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - Checking for newly runnable parent stages
15394 [main] INFO  org.apache.spark.internal.Logging$class  - Job 1 finished: show at DeduplicationF1.scala:65, took 1.484035 s
15394 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - running: Set()
15394 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - waiting: Set()
15394 [dag-scheduler-event-loop] TRACE org.apache.spark.internal.Logging$class  - failed: Set()
15396 [main] TRACE org.apache.spark.internal.Logging$class  - Fixed point reached for batch CleanExpressions after 1 iterations.
15396 [main] TRACE org.apache.spark.internal.Logging$class  - Batch CleanExpressions has no effect.
15397 [main] DEBUG org.apache.spark.internal.Logging$class  - code for createexternalrow(input[0, int, true], input[1, int, true], input[2, string, true].toString, StructField(vid,IntegerType,true), StructField(tupleid,IntegerType,true), StructField(recid,StringType,true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private MutableRow mutableRow;
/* 009 */   private Object[] values;
/* 010 */   private org.apache.spark.sql.types.StructType schema;
/* 011 */
/* 012 */
/* 013 */   public SpecificSafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     mutableRow = (MutableRow) references[references.length - 1];
/* 016 */
/* 017 */     this.schema = (org.apache.spark.sql.types.StructType) references[0];
/* 018 */   }
/* 019 */
/* 020 */   public java.lang.Object apply(java.lang.Object _i) {
/* 021 */     InternalRow i = (InternalRow) _i;
/* 022 */
/* 023 */     values = new Object[3];
/* 024 */
/* 025 */     boolean isNull1 = i.isNullAt(0);
/* 026 */     int value1 = isNull1 ? -1 : (i.getInt(0));
/* 027 */     if (isNull1) {
/* 028 */       values[0] = null;
/* 029 */     } else {
/* 030 */       values[0] = value1;
/* 031 */     }
/* 032 */
/* 033 */     boolean isNull2 = i.isNullAt(1);
/* 034 */     int value2 = isNull2 ? -1 : (i.getInt(1));
/* 035 */     if (isNull2) {
/* 036 */       values[1] = null;
/* 037 */     } else {
/* 038 */       values[1] = value2;
/* 039 */     }
/* 040 */
/* 041 */     boolean isNull4 = i.isNullAt(2);
/* 042 */     UTF8String value4 = isNull4 ? null : (i.getUTF8String(2));
/* 043 */
/* 044 */     boolean isNull3 = isNull4;
/* 045 */     final java.lang.String value3 = isNull3 ? null : (java.lang.String) value4.toString();
/* 046 */     isNull3 = value3 == null;
/* 047 */     if (isNull3) {
/* 048 */       values[2] = null;
/* 049 */     } else {
/* 050 */       values[2] = value3;
/* 051 */     }
/* 052 */
/* 053 */     final org.apache.spark.sql.Row value = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values, schema);
/* 054 */     if (false) {
/* 055 */       mutableRow.setNullAt(0);
/* 056 */     } else {
/* 057 */
/* 058 */       mutableRow.update(0, value);
/* 059 */     }
/* 060 */
/* 061 */     return mutableRow;
/* 062 */   }
/* 063 */ }

15399 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.server.Server@57ac5227
15407 [main] DEBUG org.spark_project.jetty.server.Server  - Graceful shutdown org.spark_project.jetty.server.Server@57ac5227 by 
15407 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping ServerConnector@655f7ea{HTTP/1.1}{0.0.0.0:4040}
15407 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.server.ServerConnector$ServerConnectorManager@6da00fb9
15407 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.io.SelectorManager$ManagedSelector@ed3068a keys=0 selected=0
15407 [main] DEBUG org.spark_project.jetty.io.SelectorManager$ManagedSelector  - Stopping org.spark_project.jetty.io.SelectorManager$ManagedSelector@ed3068a keys=0 selected=0
15408 [main] DEBUG org.spark_project.jetty.io.SelectorManager$ManagedSelector  - Queued change org.spark_project.jetty.io.SelectorManager$ManagedSelector$Stop@608b906d
15409 [SparkUI-48-selector-ServerConnectorManager@6da00fb9/0] DEBUG org.spark_project.jetty.io.SelectorManager$ManagedSelector  - Selector loop woken up from select, 0/0 selected
15409 [SparkUI-48-selector-ServerConnectorManager@6da00fb9/0] DEBUG org.spark_project.jetty.io.SelectorManager$ManagedSelector  - Running change org.spark_project.jetty.io.SelectorManager$ManagedSelector$Stop@608b906d
15411 [main] DEBUG org.spark_project.jetty.io.SelectorManager$ManagedSelector  - Stopped org.spark_project.jetty.io.SelectorManager$ManagedSelector@ed3068a keys=-1 selected=-1
15411 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.io.SelectorManager$ManagedSelector@ed3068a keys=-1 selected=-1
15411 [SparkUI-48-selector-ServerConnectorManager@6da00fb9/0] DEBUG org.spark_project.jetty.io.SelectorManager$ManagedSelector  - Stopped Thread[SparkUI-48-selector-ServerConnectorManager@6da00fb9/0,5,main] on org.spark_project.jetty.io.SelectorManager$ManagedSelector@ed3068a keys=-1 selected=-1
15411 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.io.SelectorManager$ManagedSelector@a8e6492 keys=0 selected=0
15412 [main] DEBUG org.spark_project.jetty.io.SelectorManager$ManagedSelector  - Stopping org.spark_project.jetty.io.SelectorManager$ManagedSelector@a8e6492 keys=0 selected=0
15412 [main] DEBUG org.spark_project.jetty.io.SelectorManager$ManagedSelector  - Queued change org.spark_project.jetty.io.SelectorManager$ManagedSelector$Stop@173cfb01
15413 [SparkUI-49-selector-ServerConnectorManager@6da00fb9/1] DEBUG org.spark_project.jetty.io.SelectorManager$ManagedSelector  - Selector loop woken up from select, 0/0 selected
15413 [SparkUI-49-selector-ServerConnectorManager@6da00fb9/1] DEBUG org.spark_project.jetty.io.SelectorManager$ManagedSelector  - Running change org.spark_project.jetty.io.SelectorManager$ManagedSelector$Stop@173cfb01
15413 [main] DEBUG org.spark_project.jetty.io.SelectorManager$ManagedSelector  - Stopped org.spark_project.jetty.io.SelectorManager$ManagedSelector@a8e6492 keys=-1 selected=-1
15413 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.io.SelectorManager$ManagedSelector@a8e6492 keys=-1 selected=-1
15413 [SparkUI-49-selector-ServerConnectorManager@6da00fb9/1] DEBUG org.spark_project.jetty.io.SelectorManager$ManagedSelector  - Stopped Thread[SparkUI-49-selector-ServerConnectorManager@6da00fb9/1,5,main] on org.spark_project.jetty.io.SelectorManager$ManagedSelector@a8e6492 keys=-1 selected=-1
15413 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.io.SelectorManager$ManagedSelector@1c7fd41f keys=0 selected=0
15414 [main] DEBUG org.spark_project.jetty.io.SelectorManager$ManagedSelector  - Stopping org.spark_project.jetty.io.SelectorManager$ManagedSelector@1c7fd41f keys=0 selected=0
15414 [main] DEBUG org.spark_project.jetty.io.SelectorManager$ManagedSelector  - Queued change org.spark_project.jetty.io.SelectorManager$ManagedSelector$Stop@7e1762e6
15414 [SparkUI-50-selector-ServerConnectorManager@6da00fb9/2] DEBUG org.spark_project.jetty.io.SelectorManager$ManagedSelector  - Selector loop woken up from select, 0/0 selected
15414 [SparkUI-50-selector-ServerConnectorManager@6da00fb9/2] DEBUG org.spark_project.jetty.io.SelectorManager$ManagedSelector  - Running change org.spark_project.jetty.io.SelectorManager$ManagedSelector$Stop@7e1762e6
15414 [main] DEBUG org.spark_project.jetty.io.SelectorManager$ManagedSelector  - Stopped org.spark_project.jetty.io.SelectorManager$ManagedSelector@1c7fd41f keys=-1 selected=-1
15414 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.io.SelectorManager$ManagedSelector@1c7fd41f keys=-1 selected=-1
15414 [SparkUI-50-selector-ServerConnectorManager@6da00fb9/2] DEBUG org.spark_project.jetty.io.SelectorManager$ManagedSelector  - Stopped Thread[SparkUI-50-selector-ServerConnectorManager@6da00fb9/2,5,main] on org.spark_project.jetty.io.SelectorManager$ManagedSelector@1c7fd41f keys=-1 selected=-1
15415 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.io.SelectorManager$ManagedSelector@3b77a04f keys=0 selected=0
15415 [main] DEBUG org.spark_project.jetty.io.SelectorManager$ManagedSelector  - Stopping org.spark_project.jetty.io.SelectorManager$ManagedSelector@3b77a04f keys=0 selected=0
15415 [main] DEBUG org.spark_project.jetty.io.SelectorManager$ManagedSelector  - Queued change org.spark_project.jetty.io.SelectorManager$ManagedSelector$Stop@5bccaedb
15415 [SparkUI-51-selector-ServerConnectorManager@6da00fb9/3] DEBUG org.spark_project.jetty.io.SelectorManager$ManagedSelector  - Selector loop woken up from select, 0/0 selected
15415 [SparkUI-51-selector-ServerConnectorManager@6da00fb9/3] DEBUG org.spark_project.jetty.io.SelectorManager$ManagedSelector  - Running change org.spark_project.jetty.io.SelectorManager$ManagedSelector$Stop@5bccaedb
15415 [main] DEBUG org.spark_project.jetty.io.SelectorManager$ManagedSelector  - Stopped org.spark_project.jetty.io.SelectorManager$ManagedSelector@3b77a04f keys=-1 selected=-1
15416 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.io.SelectorManager$ManagedSelector@3b77a04f keys=-1 selected=-1
15416 [SparkUI-51-selector-ServerConnectorManager@6da00fb9/3] DEBUG org.spark_project.jetty.io.SelectorManager$ManagedSelector  - Stopped Thread[SparkUI-51-selector-ServerConnectorManager@6da00fb9/3,5,main] on org.spark_project.jetty.io.SelectorManager$ManagedSelector@3b77a04f keys=-1 selected=-1
15416 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.server.ServerConnector$ServerConnectorManager@6da00fb9
15416 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping HttpConnectionFactory@698122b2{HTTP/1.1}
15416 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED HttpConnectionFactory@698122b2{HTTP/1.1}
15416 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.util.thread.ScheduledExecutorScheduler@549949be
15417 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.util.thread.ScheduledExecutorScheduler@549949be
15417 [main] INFO  org.spark_project.jetty.server.AbstractConnector  - Stopped ServerConnector@655f7ea{HTTP/1.1}{0.0.0.0:4040}
15417 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED ServerConnector@655f7ea{HTTP/1.1}{0.0.0.0:4040}
15417 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.server.Server@57ac5227
15417 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.server.handler.ContextHandlerCollection@13cf7d52[org.spark_project.jetty.servlets.gzip.GzipHandler@7d42c224, org.spark_project.jetty.servlets.gzip.GzipHandler@5e01a982, org.spark_project.jetty.servlets.gzip.GzipHandler@704f1591, org.spark_project.jetty.servlets.gzip.GzipHandler@349c1daf, org.spark_project.jetty.servlets.gzip.GzipHandler@aafcffa, org.spark_project.jetty.servlets.gzip.GzipHandler@5f7b97da, org.spark_project.jetty.servlets.gzip.GzipHandler@395b56bb, org.spark_project.jetty.servlets.gzip.GzipHandler@6c5945a7, org.spark_project.jetty.servlets.gzip.GzipHandler@7ac2e39b, org.spark_project.jetty.servlets.gzip.GzipHandler@52de51b6, org.spark_project.jetty.servlets.gzip.GzipHandler@485e36bc, org.spark_project.jetty.servlets.gzip.GzipHandler@17cdf2d0, org.spark_project.jetty.servlets.gzip.GzipHandler@5b529706, org.spark_project.jetty.servlets.gzip.GzipHandler@1af687fe, org.spark_project.jetty.servlets.gzip.GzipHandler@5a411614, org.spark_project.jetty.servlets.gzip.GzipHandler@47a5b70d, org.spark_project.jetty.servlets.gzip.GzipHandler@5286c33a, org.spark_project.jetty.servlets.gzip.GzipHandler@85e6769, org.spark_project.jetty.servlets.gzip.GzipHandler@70efb718, org.spark_project.jetty.servlets.gzip.GzipHandler@435fb7b5, org.spark_project.jetty.servlets.gzip.GzipHandler@5733f295, org.spark_project.jetty.servlets.gzip.GzipHandler@f478a81, org.spark_project.jetty.servlets.gzip.GzipHandler@4c4748bf, org.spark_project.jetty.servlets.gzip.GzipHandler@5a85c92, o.s.j.s.ServletContextHandler@6bc28a83{/metrics/json,null,SHUTDOWN}, o.s.j.s.ServletContextHandler@27f9e982{/SQL,null,SHUTDOWN}, o.s.j.s.ServletContextHandler@30c0ccff{/SQL/json,null,SHUTDOWN}, o.s.j.s.ServletContextHandler@7cd1ac19{/SQL/execution,null,SHUTDOWN}, o.s.j.s.ServletContextHandler@69c43e48{/SQL/execution/json,null,SHUTDOWN}, o.s.j.s.ServletContextHandler@c8b96ec{/static/sql,null,SHUTDOWN}]
15418 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.server.handler.ContextHandlerCollection@13cf7d52[org.spark_project.jetty.servlets.gzip.GzipHandler@7d42c224, org.spark_project.jetty.servlets.gzip.GzipHandler@5e01a982, org.spark_project.jetty.servlets.gzip.GzipHandler@704f1591, org.spark_project.jetty.servlets.gzip.GzipHandler@349c1daf, org.spark_project.jetty.servlets.gzip.GzipHandler@aafcffa, org.spark_project.jetty.servlets.gzip.GzipHandler@5f7b97da, org.spark_project.jetty.servlets.gzip.GzipHandler@395b56bb, org.spark_project.jetty.servlets.gzip.GzipHandler@6c5945a7, org.spark_project.jetty.servlets.gzip.GzipHandler@7ac2e39b, org.spark_project.jetty.servlets.gzip.GzipHandler@52de51b6, org.spark_project.jetty.servlets.gzip.GzipHandler@485e36bc, org.spark_project.jetty.servlets.gzip.GzipHandler@17cdf2d0, org.spark_project.jetty.servlets.gzip.GzipHandler@5b529706, org.spark_project.jetty.servlets.gzip.GzipHandler@1af687fe, org.spark_project.jetty.servlets.gzip.GzipHandler@5a411614, org.spark_project.jetty.servlets.gzip.GzipHandler@47a5b70d, org.spark_project.jetty.servlets.gzip.GzipHandler@5286c33a, org.spark_project.jetty.servlets.gzip.GzipHandler@85e6769, org.spark_project.jetty.servlets.gzip.GzipHandler@70efb718, org.spark_project.jetty.servlets.gzip.GzipHandler@435fb7b5, org.spark_project.jetty.servlets.gzip.GzipHandler@5733f295, org.spark_project.jetty.servlets.gzip.GzipHandler@f478a81, org.spark_project.jetty.servlets.gzip.GzipHandler@4c4748bf, org.spark_project.jetty.servlets.gzip.GzipHandler@5a85c92, o.s.j.s.ServletContextHandler@6bc28a83{/metrics/json,null,SHUTDOWN}, o.s.j.s.ServletContextHandler@27f9e982{/SQL,null,SHUTDOWN}, o.s.j.s.ServletContextHandler@30c0ccff{/SQL/json,null,SHUTDOWN}, o.s.j.s.ServletContextHandler@7cd1ac19{/SQL/execution,null,SHUTDOWN}, o.s.j.s.ServletContextHandler@69c43e48{/SQL/execution/json,null,SHUTDOWN}, o.s.j.s.ServletContextHandler@c8b96ec{/static/sql,null,SHUTDOWN}]
15418 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@5a85c92
15418 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@5a85c92
15418 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping o.s.j.s.ServletContextHandler@302a07d{/stages/stage/kill,null,SHUTDOWN}
15418 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping o.s.j.s.ServletContextHandler@302a07d{/stages/stage/kill,null,UNAVAILABLE}
15418 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlet.ServletHandler@5cdd09b1
15419 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlet.ServletHandler@5cdd09b1
15419 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.apache.spark.ui.JettyUtils$$anon$3-8c11eee@18f56dee==org.apache.spark.ui.JettyUtils$$anon$3,-1,true
15419 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.apache.spark.ui.JettyUtils$$anon$3-8c11eee@18f56dee==org.apache.spark.ui.JettyUtils$$anon$3,-1,true
15421 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlet.ServletHandler@5cdd09b1
15422 [main] INFO  org.spark_project.jetty.server.handler.ContextHandler  - Stopped o.s.j.s.ServletContextHandler@302a07d{/stages/stage/kill,null,UNAVAILABLE}
15422 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED o.s.j.s.ServletContextHandler@302a07d{/stages/stage/kill,null,UNAVAILABLE}
15422 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlets.gzip.GzipHandler@5a85c92
15422 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@4c4748bf
15422 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@4c4748bf
15422 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping o.s.j.s.ServletContextHandler@77a7cf58{/api,null,SHUTDOWN}
15422 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping o.s.j.s.ServletContextHandler@77a7cf58{/api,null,UNAVAILABLE}
15423 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlet.ServletHandler@3d97a632
15423 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlet.ServletHandler@3d97a632
15423 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-129b4fe2@3150754d==org.spark_project.jetty.servlet.ServletHandler$Default404Servlet,-1,false
15423 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlet.ServletHandler$Default404Servlet-129b4fe2@3150754d==org.spark_project.jetty.servlet.ServletHandler$Default404Servlet,-1,false
15423 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.glassfish.jersey.servlet.ServletContainer-2d36e77e@27135163==org.glassfish.jersey.servlet.ServletContainer,-1,false
15423 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.glassfish.jersey.servlet.ServletContainer-2d36e77e@27135163==org.glassfish.jersey.servlet.ServletContainer,-1,false
15424 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlet.ServletHandler@3d97a632
15424 [main] INFO  org.spark_project.jetty.server.handler.ContextHandler  - Stopped o.s.j.s.ServletContextHandler@77a7cf58{/api,null,UNAVAILABLE}
15424 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED o.s.j.s.ServletContextHandler@77a7cf58{/api,null,UNAVAILABLE}
15424 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlets.gzip.GzipHandler@4c4748bf
15424 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@f478a81
15424 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@f478a81
15425 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping o.s.j.s.ServletContextHandler@37052337{/,null,SHUTDOWN}
15425 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping o.s.j.s.ServletContextHandler@37052337{/,null,UNAVAILABLE}
15425 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlet.ServletHandler@2320fa6f
15425 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlet.ServletHandler@2320fa6f
15425 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.apache.spark.ui.JettyUtils$$anon$3-7a560583@3b0bb357==org.apache.spark.ui.JettyUtils$$anon$3,-1,true
15425 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.apache.spark.ui.JettyUtils$$anon$3-7a560583@3b0bb357==org.apache.spark.ui.JettyUtils$$anon$3,-1,true
15426 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlet.ServletHandler@2320fa6f
15426 [main] INFO  org.spark_project.jetty.server.handler.ContextHandler  - Stopped o.s.j.s.ServletContextHandler@37052337{/,null,UNAVAILABLE}
15426 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED o.s.j.s.ServletContextHandler@37052337{/,null,UNAVAILABLE}
15426 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlets.gzip.GzipHandler@f478a81
15426 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@5733f295
15426 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@5733f295
15426 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping o.s.j.s.ServletContextHandler@70ab2d48{/static,null,SHUTDOWN}
15427 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping o.s.j.s.ServletContextHandler@70ab2d48{/static,null,UNAVAILABLE}
15427 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlet.ServletHandler@5b068087
15427 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlet.ServletHandler@5b068087
15427 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlet.DefaultServlet-a43ce46@1d8ca315==org.spark_project.jetty.servlet.DefaultServlet,-1,true
15427 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlet.DefaultServlet-a43ce46@1d8ca315==org.spark_project.jetty.servlet.DefaultServlet,-1,true
15428 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlet.ServletHandler@5b068087
15428 [main] INFO  org.spark_project.jetty.server.handler.ContextHandler  - Stopped o.s.j.s.ServletContextHandler@70ab2d48{/static,null,UNAVAILABLE}
15428 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED o.s.j.s.ServletContextHandler@70ab2d48{/static,null,UNAVAILABLE}
15428 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlets.gzip.GzipHandler@5733f295
15428 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@435fb7b5
15428 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@435fb7b5
15428 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping o.s.j.s.ServletContextHandler@12bd8a64{/executors/threadDump/json,null,SHUTDOWN}
15429 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping o.s.j.s.ServletContextHandler@12bd8a64{/executors/threadDump/json,null,UNAVAILABLE}
15429 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlet.ServletHandler@61e94def
15429 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlet.ServletHandler@61e94def
15429 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.apache.spark.ui.JettyUtils$$anon$2-3300f4fd@1d1aaef9==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15429 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-3300f4fd@1d1aaef9==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15429 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlet.ServletHandler@61e94def
15430 [main] INFO  org.spark_project.jetty.server.handler.ContextHandler  - Stopped o.s.j.s.ServletContextHandler@12bd8a64{/executors/threadDump/json,null,UNAVAILABLE}
15430 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED o.s.j.s.ServletContextHandler@12bd8a64{/executors/threadDump/json,null,UNAVAILABLE}
15430 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlets.gzip.GzipHandler@435fb7b5
15430 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@70efb718
15430 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@70efb718
15430 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping o.s.j.s.ServletContextHandler@43b6123e{/executors/threadDump,null,SHUTDOWN}
15430 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping o.s.j.s.ServletContextHandler@43b6123e{/executors/threadDump,null,UNAVAILABLE}
15430 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlet.ServletHandler@260e86a1
15431 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlet.ServletHandler@260e86a1
15431 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.apache.spark.ui.JettyUtils$$anon$2-19648c40@951eeb9a==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15431 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-19648c40@951eeb9a==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15431 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlet.ServletHandler@260e86a1
15431 [main] INFO  org.spark_project.jetty.server.handler.ContextHandler  - Stopped o.s.j.s.ServletContextHandler@43b6123e{/executors/threadDump,null,UNAVAILABLE}
15431 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED o.s.j.s.ServletContextHandler@43b6123e{/executors/threadDump,null,UNAVAILABLE}
15431 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlets.gzip.GzipHandler@70efb718
15432 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@85e6769
15432 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@85e6769
15432 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping o.s.j.s.ServletContextHandler@71cf1b07{/executors/json,null,SHUTDOWN}
15432 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping o.s.j.s.ServletContextHandler@71cf1b07{/executors/json,null,UNAVAILABLE}
15432 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlet.ServletHandler@615091b8
15432 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlet.ServletHandler@615091b8
15432 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.apache.spark.ui.JettyUtils$$anon$2-4fce136b@68b38def==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15433 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-4fce136b@68b38def==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15433 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlet.ServletHandler@615091b8
15433 [main] INFO  org.spark_project.jetty.server.handler.ContextHandler  - Stopped o.s.j.s.ServletContextHandler@71cf1b07{/executors/json,null,UNAVAILABLE}
15433 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED o.s.j.s.ServletContextHandler@71cf1b07{/executors/json,null,UNAVAILABLE}
15433 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlets.gzip.GzipHandler@85e6769
15433 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@5286c33a
15433 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@5286c33a
15434 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping o.s.j.s.ServletContextHandler@2f17e30d{/executors,null,SHUTDOWN}
15434 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping o.s.j.s.ServletContextHandler@2f17e30d{/executors,null,UNAVAILABLE}
15434 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlet.ServletHandler@3e821657
15434 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlet.ServletHandler@3e821657
15434 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.apache.spark.ui.JettyUtils$$anon$2-1d131e1b@6f3165a7==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15434 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-1d131e1b@6f3165a7==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15434 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlet.ServletHandler@3e821657
15435 [main] INFO  org.spark_project.jetty.server.handler.ContextHandler  - Stopped o.s.j.s.ServletContextHandler@2f17e30d{/executors,null,UNAVAILABLE}
15435 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED o.s.j.s.ServletContextHandler@2f17e30d{/executors,null,UNAVAILABLE}
15435 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlets.gzip.GzipHandler@5286c33a
15435 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@47a5b70d
15435 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@47a5b70d
15435 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping o.s.j.s.ServletContextHandler@6622fc65{/environment/json,null,SHUTDOWN}
15435 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping o.s.j.s.ServletContextHandler@6622fc65{/environment/json,null,UNAVAILABLE}
15435 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlet.ServletHandler@299321e2
15436 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlet.ServletHandler@299321e2
15436 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.apache.spark.ui.JettyUtils$$anon$2-23fb172e@1407b823==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15436 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-23fb172e@1407b823==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15436 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlet.ServletHandler@299321e2
15436 [main] INFO  org.spark_project.jetty.server.handler.ContextHandler  - Stopped o.s.j.s.ServletContextHandler@6622fc65{/environment/json,null,UNAVAILABLE}
15436 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED o.s.j.s.ServletContextHandler@6622fc65{/environment/json,null,UNAVAILABLE}
15436 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlets.gzip.GzipHandler@47a5b70d
15437 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@5a411614
15437 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@5a411614
15437 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping o.s.j.s.ServletContextHandler@45815ffc{/environment,null,SHUTDOWN}
15437 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping o.s.j.s.ServletContextHandler@45815ffc{/environment,null,UNAVAILABLE}
15437 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlet.ServletHandler@732f29af
15437 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlet.ServletHandler@732f29af
15437 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.apache.spark.ui.JettyUtils$$anon$2-d3957fe@4acc713e==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15437 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-d3957fe@4acc713e==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15438 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlet.ServletHandler@732f29af
15438 [main] INFO  org.spark_project.jetty.server.handler.ContextHandler  - Stopped o.s.j.s.ServletContextHandler@45815ffc{/environment,null,UNAVAILABLE}
15438 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED o.s.j.s.ServletContextHandler@45815ffc{/environment,null,UNAVAILABLE}
15438 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlets.gzip.GzipHandler@5a411614
15438 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@1af687fe
15439 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@1af687fe
15439 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping o.s.j.s.ServletContextHandler@4b45dcb8{/storage/rdd/json,null,SHUTDOWN}
15439 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping o.s.j.s.ServletContextHandler@4b45dcb8{/storage/rdd/json,null,UNAVAILABLE}
15439 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlet.ServletHandler@7216fb24
15439 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlet.ServletHandler@7216fb24
15439 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.apache.spark.ui.JettyUtils$$anon$2-2072acb2@22935598==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15439 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-2072acb2@22935598==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15440 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlet.ServletHandler@7216fb24
15440 [main] INFO  org.spark_project.jetty.server.handler.ContextHandler  - Stopped o.s.j.s.ServletContextHandler@4b45dcb8{/storage/rdd/json,null,UNAVAILABLE}
15440 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED o.s.j.s.ServletContextHandler@4b45dcb8{/storage/rdd/json,null,UNAVAILABLE}
15440 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlets.gzip.GzipHandler@1af687fe
15440 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@5b529706
15440 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@5b529706
15440 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping o.s.j.s.ServletContextHandler@674bd420{/storage/rdd,null,SHUTDOWN}
15440 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping o.s.j.s.ServletContextHandler@674bd420{/storage/rdd,null,UNAVAILABLE}
15441 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlet.ServletHandler@2b0f373b
15441 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlet.ServletHandler@2b0f373b
15441 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.apache.spark.ui.JettyUtils$$anon$2-2ceb80a1@fd811583==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15441 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-2ceb80a1@fd811583==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15441 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlet.ServletHandler@2b0f373b
15441 [main] INFO  org.spark_project.jetty.server.handler.ContextHandler  - Stopped o.s.j.s.ServletContextHandler@674bd420{/storage/rdd,null,UNAVAILABLE}
15442 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED o.s.j.s.ServletContextHandler@674bd420{/storage/rdd,null,UNAVAILABLE}
15442 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlets.gzip.GzipHandler@5b529706
15442 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@17cdf2d0
15442 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@17cdf2d0
15442 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping o.s.j.s.ServletContextHandler@2dd80673{/storage/json,null,SHUTDOWN}
15442 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping o.s.j.s.ServletContextHandler@2dd80673{/storage/json,null,UNAVAILABLE}
15442 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlet.ServletHandler@4af0df05
15442 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlet.ServletHandler@4af0df05
15442 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.apache.spark.ui.JettyUtils$$anon$2-57ea113a@1d812539==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15443 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-57ea113a@1d812539==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15443 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlet.ServletHandler@4af0df05
15443 [main] INFO  org.spark_project.jetty.server.handler.ContextHandler  - Stopped o.s.j.s.ServletContextHandler@2dd80673{/storage/json,null,UNAVAILABLE}
15443 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED o.s.j.s.ServletContextHandler@2dd80673{/storage/json,null,UNAVAILABLE}
15443 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlets.gzip.GzipHandler@17cdf2d0
15443 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@485e36bc
15443 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@485e36bc
15444 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping o.s.j.s.ServletContextHandler@48d5f34e{/storage,null,SHUTDOWN}
15444 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping o.s.j.s.ServletContextHandler@48d5f34e{/storage,null,UNAVAILABLE}
15444 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlet.ServletHandler@fc258b1
15444 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlet.ServletHandler@fc258b1
15444 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.apache.spark.ui.JettyUtils$$anon$2-6ff65192@3affeb02==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15444 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-6ff65192@3affeb02==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15444 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlet.ServletHandler@fc258b1
15445 [main] INFO  org.spark_project.jetty.server.handler.ContextHandler  - Stopped o.s.j.s.ServletContextHandler@48d5f34e{/storage,null,UNAVAILABLE}
15445 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED o.s.j.s.ServletContextHandler@48d5f34e{/storage,null,UNAVAILABLE}
15445 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlets.gzip.GzipHandler@485e36bc
15445 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@52de51b6
15445 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@52de51b6
15445 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping o.s.j.s.ServletContextHandler@5911e990{/stages/pool/json,null,SHUTDOWN}
15445 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping o.s.j.s.ServletContextHandler@5911e990{/stages/pool/json,null,UNAVAILABLE}
15445 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlet.ServletHandler@31000e60
15445 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlet.ServletHandler@31000e60
15446 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.apache.spark.ui.JettyUtils$$anon$2-1d470d0@14467239==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15446 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-1d470d0@14467239==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15446 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlet.ServletHandler@31000e60
15446 [main] INFO  org.spark_project.jetty.server.handler.ContextHandler  - Stopped o.s.j.s.ServletContextHandler@5911e990{/stages/pool/json,null,UNAVAILABLE}
15446 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED o.s.j.s.ServletContextHandler@5911e990{/stages/pool/json,null,UNAVAILABLE}
15446 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlets.gzip.GzipHandler@52de51b6
15446 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@7ac2e39b
15447 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@7ac2e39b
15447 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping o.s.j.s.ServletContextHandler@51972dc7{/stages/pool,null,SHUTDOWN}
15447 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping o.s.j.s.ServletContextHandler@51972dc7{/stages/pool,null,UNAVAILABLE}
15447 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlet.ServletHandler@3700ec9c
15447 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlet.ServletHandler@3700ec9c
15447 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.apache.spark.ui.JettyUtils$$anon$2-2002348@f0360eca==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15447 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-2002348@f0360eca==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15448 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlet.ServletHandler@3700ec9c
15448 [main] INFO  org.spark_project.jetty.server.handler.ContextHandler  - Stopped o.s.j.s.ServletContextHandler@51972dc7{/stages/pool,null,UNAVAILABLE}
15448 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED o.s.j.s.ServletContextHandler@51972dc7{/stages/pool,null,UNAVAILABLE}
15448 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlets.gzip.GzipHandler@7ac2e39b
15448 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@6c5945a7
15448 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@6c5945a7
15448 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping o.s.j.s.ServletContextHandler@39a8312f{/stages/stage/json,null,SHUTDOWN}
15449 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping o.s.j.s.ServletContextHandler@39a8312f{/stages/stage/json,null,UNAVAILABLE}
15449 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlet.ServletHandler@5f6722d3
15449 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlet.ServletHandler@5f6722d3
15449 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.apache.spark.ui.JettyUtils$$anon$2-2c532cd8@a9002ca1==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15449 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-2c532cd8@a9002ca1==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15450 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlet.ServletHandler@5f6722d3
15450 [main] INFO  org.spark_project.jetty.server.handler.ContextHandler  - Stopped o.s.j.s.ServletContextHandler@39a8312f{/stages/stage/json,null,UNAVAILABLE}
15450 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED o.s.j.s.ServletContextHandler@39a8312f{/stages/stage/json,null,UNAVAILABLE}
15450 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlets.gzip.GzipHandler@6c5945a7
15450 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@395b56bb
15450 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@395b56bb
15450 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping o.s.j.s.ServletContextHandler@72c28d64{/stages/stage,null,SHUTDOWN}
15451 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping o.s.j.s.ServletContextHandler@72c28d64{/stages/stage,null,UNAVAILABLE}
15451 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlet.ServletHandler@6492fab5
15451 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlet.ServletHandler@6492fab5
15451 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.apache.spark.ui.JettyUtils$$anon$2-2c5529ab@a91bbdc6==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15451 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-2c5529ab@a91bbdc6==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15451 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlet.ServletHandler@6492fab5
15451 [main] INFO  org.spark_project.jetty.server.handler.ContextHandler  - Stopped o.s.j.s.ServletContextHandler@72c28d64{/stages/stage,null,UNAVAILABLE}
15452 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED o.s.j.s.ServletContextHandler@72c28d64{/stages/stage,null,UNAVAILABLE}
15452 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlets.gzip.GzipHandler@395b56bb
15452 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@5f7b97da
15452 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@5f7b97da
15452 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping o.s.j.s.ServletContextHandler@2e8e8225{/stages/json,null,SHUTDOWN}
15452 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping o.s.j.s.ServletContextHandler@2e8e8225{/stages/json,null,UNAVAILABLE}
15452 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlet.ServletHandler@6ebf0f36
15452 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlet.ServletHandler@6ebf0f36
15453 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.apache.spark.ui.JettyUtils$$anon$2-18920cc@c97566a3==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15453 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-18920cc@c97566a3==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15453 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlet.ServletHandler@6ebf0f36
15453 [main] INFO  org.spark_project.jetty.server.handler.ContextHandler  - Stopped o.s.j.s.ServletContextHandler@2e8e8225{/stages/json,null,UNAVAILABLE}
15453 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED o.s.j.s.ServletContextHandler@2e8e8225{/stages/json,null,UNAVAILABLE}
15453 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlets.gzip.GzipHandler@5f7b97da
15453 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@aafcffa
15454 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@aafcffa
15454 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping o.s.j.s.ServletContextHandler@178213b{/stages,null,SHUTDOWN}
15454 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping o.s.j.s.ServletContextHandler@178213b{/stages,null,UNAVAILABLE}
15454 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlet.ServletHandler@7103cb56
15454 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlet.ServletHandler@7103cb56
15454 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.apache.spark.ui.JettyUtils$$anon$2-1b765a2c@fce0bfa==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15454 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-1b765a2c@fce0bfa==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15454 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlet.ServletHandler@7103cb56
15455 [main] INFO  org.spark_project.jetty.server.handler.ContextHandler  - Stopped o.s.j.s.ServletContextHandler@178213b{/stages,null,UNAVAILABLE}
15455 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED o.s.j.s.ServletContextHandler@178213b{/stages,null,UNAVAILABLE}
15455 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlets.gzip.GzipHandler@aafcffa
15455 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@349c1daf
15455 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@349c1daf
15455 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping o.s.j.s.ServletContextHandler@fade1fc{/jobs/job/json,null,SHUTDOWN}
15455 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping o.s.j.s.ServletContextHandler@fade1fc{/jobs/job/json,null,UNAVAILABLE}
15455 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlet.ServletHandler@67c2e933
15456 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlet.ServletHandler@67c2e933
15456 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.apache.spark.ui.JettyUtils$$anon$2-41dd05a@5eaad3b2==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15456 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-41dd05a@5eaad3b2==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15456 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlet.ServletHandler@67c2e933
15456 [main] INFO  org.spark_project.jetty.server.handler.ContextHandler  - Stopped o.s.j.s.ServletContextHandler@fade1fc{/jobs/job/json,null,UNAVAILABLE}
15456 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED o.s.j.s.ServletContextHandler@fade1fc{/jobs/job/json,null,UNAVAILABLE}
15456 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlets.gzip.GzipHandler@349c1daf
15456 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@704f1591
15457 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@704f1591
15457 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping o.s.j.s.ServletContextHandler@59f63e24{/jobs/job,null,SHUTDOWN}
15457 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping o.s.j.s.ServletContextHandler@59f63e24{/jobs/job,null,UNAVAILABLE}
15457 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlet.ServletHandler@61f05988
15457 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlet.ServletHandler@61f05988
15457 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.apache.spark.ui.JettyUtils$$anon$2-7ca33c24@fb7bb69d==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15457 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-7ca33c24@fb7bb69d==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15457 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlet.ServletHandler@61f05988
15458 [main] INFO  org.spark_project.jetty.server.handler.ContextHandler  - Stopped o.s.j.s.ServletContextHandler@59f63e24{/jobs/job,null,UNAVAILABLE}
15458 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED o.s.j.s.ServletContextHandler@59f63e24{/jobs/job,null,UNAVAILABLE}
15458 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlets.gzip.GzipHandler@704f1591
15458 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@5e01a982
15458 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@5e01a982
15459 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping o.s.j.s.ServletContextHandler@3016fd5e{/jobs/json,null,SHUTDOWN}
15459 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping o.s.j.s.ServletContextHandler@3016fd5e{/jobs/json,null,UNAVAILABLE}
15459 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlet.ServletHandler@35d08e6c
15459 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlet.ServletHandler@35d08e6c
15459 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.apache.spark.ui.JettyUtils$$anon$2-53d102a2@458e8c2b==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15459 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-53d102a2@458e8c2b==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15460 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlet.ServletHandler@35d08e6c
15460 [main] INFO  org.spark_project.jetty.server.handler.ContextHandler  - Stopped o.s.j.s.ServletContextHandler@3016fd5e{/jobs/json,null,UNAVAILABLE}
15460 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED o.s.j.s.ServletContextHandler@3016fd5e{/jobs/json,null,UNAVAILABLE}
15460 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlets.gzip.GzipHandler@5e01a982
15460 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@7d42c224
15460 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlets.gzip.GzipHandler@7d42c224
15460 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping o.s.j.s.ServletContextHandler@c5dc4a2{/jobs,null,SHUTDOWN}
15461 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping o.s.j.s.ServletContextHandler@c5dc4a2{/jobs,null,UNAVAILABLE}
15461 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.servlet.ServletHandler@4a194c39
15461 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.servlet.ServletHandler@4a194c39
15461 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.apache.spark.ui.JettyUtils$$anon$2-31d0e481@c09be9f==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15461 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-31d0e481@c09be9f==org.apache.spark.ui.JettyUtils$$anon$2,-1,true
15461 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlet.ServletHandler@4a194c39
15462 [main] INFO  org.spark_project.jetty.server.handler.ContextHandler  - Stopped o.s.j.s.ServletContextHandler@c5dc4a2{/jobs,null,UNAVAILABLE}
15462 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED o.s.j.s.ServletContextHandler@c5dc4a2{/jobs,null,UNAVAILABLE}
15462 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.servlets.gzip.GzipHandler@7d42c224
15462 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.server.handler.ContextHandlerCollection@13cf7d52[org.spark_project.jetty.servlets.gzip.GzipHandler@7d42c224, org.spark_project.jetty.servlets.gzip.GzipHandler@5e01a982, org.spark_project.jetty.servlets.gzip.GzipHandler@704f1591, org.spark_project.jetty.servlets.gzip.GzipHandler@349c1daf, org.spark_project.jetty.servlets.gzip.GzipHandler@aafcffa, org.spark_project.jetty.servlets.gzip.GzipHandler@5f7b97da, org.spark_project.jetty.servlets.gzip.GzipHandler@395b56bb, org.spark_project.jetty.servlets.gzip.GzipHandler@6c5945a7, org.spark_project.jetty.servlets.gzip.GzipHandler@7ac2e39b, org.spark_project.jetty.servlets.gzip.GzipHandler@52de51b6, org.spark_project.jetty.servlets.gzip.GzipHandler@485e36bc, org.spark_project.jetty.servlets.gzip.GzipHandler@17cdf2d0, org.spark_project.jetty.servlets.gzip.GzipHandler@5b529706, org.spark_project.jetty.servlets.gzip.GzipHandler@1af687fe, org.spark_project.jetty.servlets.gzip.GzipHandler@5a411614, org.spark_project.jetty.servlets.gzip.GzipHandler@47a5b70d, org.spark_project.jetty.servlets.gzip.GzipHandler@5286c33a, org.spark_project.jetty.servlets.gzip.GzipHandler@85e6769, org.spark_project.jetty.servlets.gzip.GzipHandler@70efb718, org.spark_project.jetty.servlets.gzip.GzipHandler@435fb7b5, org.spark_project.jetty.servlets.gzip.GzipHandler@5733f295, org.spark_project.jetty.servlets.gzip.GzipHandler@f478a81, org.spark_project.jetty.servlets.gzip.GzipHandler@4c4748bf, org.spark_project.jetty.servlets.gzip.GzipHandler@5a85c92, o.s.j.s.ServletContextHandler@6bc28a83{/metrics/json,null,SHUTDOWN}, o.s.j.s.ServletContextHandler@27f9e982{/SQL,null,SHUTDOWN}, o.s.j.s.ServletContextHandler@30c0ccff{/SQL/json,null,SHUTDOWN}, o.s.j.s.ServletContextHandler@7cd1ac19{/SQL/execution,null,SHUTDOWN}, o.s.j.s.ServletContextHandler@69c43e48{/SQL/execution/json,null,SHUTDOWN}, o.s.j.s.ServletContextHandler@c8b96ec{/static/sql,null,SHUTDOWN}]
15462 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping org.spark_project.jetty.server.handler.ErrorHandler@3e10dc6
15462 [main] DEBUG org.spark_project.jetty.server.handler.AbstractHandler  - stopping org.spark_project.jetty.server.handler.ErrorHandler@3e10dc6
15462 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.server.handler.ErrorHandler@3e10dc6
15463 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - stopping SparkUI{STARTED,8<=8<=200,i=8,q=0}
15464 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED SparkUI{STOPPED,8<=8<=200,i=0,q=0}
15464 [main] DEBUG org.spark_project.jetty.util.component.AbstractLifeCycle  - STOPPED org.spark_project.jetty.server.Server@57ac5227
15465 [main] INFO  org.apache.spark.internal.Logging$class  - Stopped Spark web UI at http://192.168.1.4:4040
15479 [dispatcher-event-loop-4] INFO  org.apache.spark.internal.Logging$class  - MapOutputTrackerMasterEndpoint stopped!
15563 [main] INFO  org.apache.spark.internal.Logging$class  - MemoryStore cleared
15563 [main] INFO  org.apache.spark.internal.Logging$class  - BlockManager stopped
15564 [main] INFO  org.apache.spark.internal.Logging$class  - BlockManagerMaster stopped
15566 [dispatcher-event-loop-7] INFO  org.apache.spark.internal.Logging$class  - OutputCommitCoordinator stopped!
15570 [main] INFO  org.apache.spark.internal.Logging$class  - Successfully stopped SparkContext
15572 [Thread-1] INFO  org.apache.spark.internal.Logging$class  - Shutdown hook called
15573 [Thread-1] INFO  org.apache.spark.internal.Logging$class  - Deleting directory /private/var/folders/6g/bp1435t101l9k5gdkbxw87p40000gn/T/spark-b6a0b06f-3bb5-4dec-858e-eb37c7760b85
