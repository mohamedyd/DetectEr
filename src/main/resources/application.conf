home.dir {
  blackoak = "/Users/visenger/research/datasets/BlackOak/Archive"
  hosp = "/Users/visenger/research/datasets/clean-and-dirty-data/HOSP-10K"
  salaries = "/Users/visenger/research/datasets/clean-and-dirty-data/SALARIES/salaries_small"
  flights = "/Users/visenger/research/datasets/clean-and-dirty-data/FLIGHTS"
}

#todo: extract common paths as $home.dir.dataset
data.BlackOak {
  clean-data-path = "/Users/visenger/research/datasets/BlackOak/Archive/groundDB.csv"
  dirty-data-path = "/Users/visenger/research/datasets/BlackOak/Archive/inputDB.csv"
}

# INPUT DATA:

data.hosp {
  clean.1k = "/Users/visenger/research/datasets/clean-and-dirty-data/HOSP-1K/hospital_1k_with_rowid.csv"
  dirty.1k = "/Users/visenger/research/datasets/clean-and-dirty-data/HOSP-1K/dirty/dirty_hosp_1k_with_rowid.csv"
  gold.1k = "/Users/visenger/research/datasets/clean-and-dirty-data/HOSP-1K/gold_1k"

  clean.10k = "/Users/visenger/research/datasets/clean-and-dirty-data/HOSP-10K/hospital_10k_with_rowid.csv"
  dirty.10k = "/Users/visenger/research/datasets/clean-and-dirty-data/HOSP-10K/dirty/dirty_hosp_10k_with_rowid.csv"
  gold.10k = "/Users/visenger/research/datasets/clean-and-dirty-data/HOSP-10K/gold_10k"

  clean.100k = "/Users/visenger/research/datasets/clean-and-dirty-data/HOSP-100K/hospital_100k_with_rowid.csv"
  dirty.100k = "/Users/visenger/research/datasets/clean-and-dirty-data/HOSP-100K/dirty/dirty_hosp_100k_with_rowid.csv"
  gold.100k = "/Users/visenger/research/datasets/clean-and-dirty-data/HOSP-100K/gold_100k"

  fds.rules.folder = "/Users/visenger/research/datasets/clean-and-dirty-data/HOSP-10K/rules-vio"
  fds.rules.file = "fds.txt"
}

data.salaries {
  clean = "/Users/visenger/research/datasets/clean-and-dirty-data/SALARIES/salaries_small/salaries-1_with_id.csv"
  dirty = "/Users/visenger/research/datasets/clean-and-dirty-data/SALARIES/salaries_small/dirty/dirty_salaries-1_with_id.csv"
  gold = "/Users/visenger/research/datasets/clean-and-dirty-data/SALARIES/salaries_small/gold_small/"

  fds.rules.folder = "/Users/visenger/research/datasets/clean-and-dirty-data/SALARIES/salaries_small"
  fds.rules.file = "fds.txt"
}

data.flights {

  clean = "/Users/visenger/research/datasets/clean-and-dirty-data/FLIGHTS/ground-truth/flights-ground-truth.csv"
  dirty = "/Users/visenger/research/datasets/clean-and-dirty-data/FLIGHTS/dirty-data/flights-dirty.csv"
  gold = "/Users/visenger/research/datasets/clean-and-dirty-data/FLIGHTS/gold/gold-flights.txt"
  fds.rules.folder = ""
  fds.rules.file = ""

}

#DATASETS METADATA
metadata.blackoak {
  path = "/Users/visenger/research/datasets/BlackOak/Archive/inputDB-metadata.json"
}
metadata.hosp {
  path = "/Users/visenger/research/datasets/clean-and-dirty-data/HOSP-10K/metadata/hosp-metadata.json"
}
metadata.salaries {
  path = "/Users/visenger/research/datasets/clean-and-dirty-data/SALARIES/salaries_small/metadata/salaries-metadata.json"
}
metadata.flights {
  path = "/Users/visenger/research/datasets/clean-and-dirty-data/FLIGHTS/metadata/flights-metadata.json"
}

#Nadeef requires special header and naming convention;
data.salaries.nadeef {
  dirty = "/Users/visenger/research/datasets/clean-and-dirty-data/SALARIES/salaries_small/dirty/dirty_salaries_with_id_nadeef_version.csv"
}

# INITIAL TOOLS RUN:
# format: [tool-name].[data-set]{ same structure}

dboost.BlackOak {
  result.dir = "/Users/visenger/research/datasets/BlackOak/Archive/outputDBoost-newHighliting.csv"
  result.hist = "/Users/visenger/research/datasets/BlackOak/Archive/hist-0.9-0.01-outputDBoost.txt"
  result.gaus = "/Users/visenger/research/datasets/BlackOak/Archive/gaussian1.5-output.txt"

  //cells detected:
  detect.hits = "/Users/visenger/research/datasets/BlackOak/Archive/dboost-detect-result/dboost-detect-result.txt"
  detect.gaus = "/Users/visenger/research/datasets/BlackOak/Archive/dboost-gaus-detect-result/dboost-gaus-detect-result.txt"
}

dboost.hosp {
  detect.gauss = "/Users/visenger/research/datasets/clean-and-dirty-data/HOSP-10K/outliers/gauss/dtct_hosp_10k_with_rowid.txt"
  detect.hist = "/Users/visenger/research/datasets/clean-and-dirty-data/HOSP-10K/outliers/hist/dtct_hosp_10k_with_rowid.txt"

  //cells detected:
  result.gauss = "/Users/visenger/research/datasets/clean-and-dirty-data/HOSP-10K/outliers/gauss/outliers-gauss-result"
  result.hist = "/Users/visenger/research/datasets/clean-and-dirty-data/HOSP-10K/outliers/hist/outliers-hist-result"
}

dboost.salaries {
  detect.gauss = "/Users/visenger/research/datasets/clean-and-dirty-data/SALARIES/salaries_small/outliers/gauss/dtct_salaries-1_with_rowid.txt"
  detect.hist = "/Users/visenger/research/datasets/clean-and-dirty-data/SALARIES/salaries_small/outliers/hist/dtct_salaries-1_with_rowid.txt"

  //cells detected:
  result.gauss = "/Users/visenger/research/datasets/clean-and-dirty-data/SALARIES/salaries_small/outliers/gauss/outliers-gauss-result"
  result.hist = "/Users/visenger/research/datasets/clean-and-dirty-data/SALARIES/salaries_small/outliers/hist/outliers-hist-result"
}

dboost.flights {
  detect.gauss = ${home.dir.flights}"/outliers/gauss/dtct_flights-dirty.txt"
  detect.hist = ${home.dir.flights}"/outliers/hist/dtct_flights-dirty.txt"
  //cells detected:
  result.gauss = ${home.dir.flights}"/outliers/gauss/outliers-gauss-result"
  result.hist = ${home.dir.flights}"/outliers/hist/outliers-hist-result"
}


trifacta.hosp {
  vio = "/Users/visenger/research/datasets/clean-and-dirty-data/HOSP-10K/pattern-vio/trifacta_result_dirty_hosp_10k_with_rowid.csv"
  result.folder = "/Users/visenger/research/datasets/clean-and-dirty-data/HOSP-10K/pattern-vio/trifacta-result"

}

trifacta.salaries {
  vio = "/Users/visenger/research/datasets/clean-and-dirty-data/SALARIES/salaries_small/pattern-vio/trifacta_res_dirty_salaries-1_with_id.csv"
  result.folder = "/Users/visenger/research/datasets/clean-and-dirty-data/SALARIES/salaries_small/pattern-vio/trifacta-result"
}

trifacta.flights {
  vio = "/Users/visenger/trifacta/flights_dirty/2017-04-24_11-54-44/flights-dirty.csv"
  result.folder = "/Users/visenger/research/datasets/clean-and-dirty-data/FLIGHTS/trifacta/trifacta-result"
}

nadeef.rules.vio {
  result.folder = "/Users/visenger/research/datasets/clean-and-dirty-data/SALARIES/salaries_small/rules-vio/nadeef-result"
}

nadeef.dedup {
  result.folder = "/Users/visenger/research/datasets/clean-and-dirty-data/SALARIES/salaries_small/dedup"
}


deduplication {
  # rule names shoud correspond to the nadeef clean path rules for udf (deduplication)
  rule.for.dirty.data = "dedupHosp"
  rule.for.gold.data = "dedupBlackOakGold"
}

# SECOND CONVERSION -> CELLS RESULT
# format: result.[dataset].[size]{ same structure for each}

result.flights {
  gold = ${home.dir.flights}"/gold/gold-flights.txt"

  deduplication = ${home.dir.flights}"/"
  outlier.gauss = ${home.dir.flights}"/outliers/gauss/outliers-gauss-result/outliers-gauss-flights.txt"
  outlier.hist = ${home.dir.flights}"/outliers/hist/outliers-hist-result/outliers-hist-flights.txt"
  rules.vio = ${home.dir.flights}""
  pattern.vio = ${home.dir.flights}"/trifacta/trifacta-result/trifacta-flights.txt"

  full.result.folder = ${home.dir.flights}"/full"
  full.result.file = ${home.dir.flights}"/full/flights-full-result.csv"
}

result.hosp.10k {
  gold = ${home.dir.hosp}"/gold_10k/gold-hosp-10k.txt"

  deduplication = ${home.dir.hosp}"/dedup/dedup-hosp-10k.csv"
  outlier.gauss = ${home.dir.hosp}"/outliers/gauss/outliers-gauss-result/outliers-gauss-hosp-10k.txt"
  outlier.hist = ${home.dir.hosp}"/outliers/hist/outliers-hist-result/outliers-hist-hosp-10k.txt"
  rules.vio = ${home.dir.hosp}"/rules-vio/nadeef-result/rules-vio-hosp-10k.txt"
  pattern.vio = ${home.dir.hosp}"/pattern-vio/trifacta-result/trifacta-hosp-10k.txt"

  full.result.folder = ${home.dir.hosp}"/full"
  full.result.file = ${home.dir.hosp}"/full/full-result.csv"
}


# format: result.[dataset].[size]{ same structure for each}
result.salaries {
  gold = ${home.dir.salaries}"/gold_small/gold-salaries-small.txt"

  deduplication = ${home.dir.salaries}"/dedup/dedup-salaries.csv"
  outlier.gauss = ${home.dir.salaries}"/outliers/gauss/outliers-gauss-result/outliers-gauss-salaries-small.txt"
  outlier.hist = ${home.dir.salaries}"/outliers/hist/outliers-hist-result/outliers-hist-salaries-small.txt"
  rules.vio = ${home.dir.salaries}"/rules-vio/nadeef-result/rules-vio-salaries.txt"
  pattern.vio = ${home.dir.salaries}"/pattern-vio/trifacta-result/trifacta-salaries-small.txt"

  full.result.folder = ${home.dir.salaries}"/full"
  full.result.file = ${home.dir.salaries}"/full/full-result.csv"
}

# this output is blackOak output -> should be result.blackoak{}
output {

  blackoak.external.goldstandard.ground.truth.folder = ${home.dir.blackoak}"/external-gold-standard-ground-truth"
  blackoak.external.goldstandard.ground.truth.file = ${home.dir.blackoak}"/external-gold-standard-ground-truth/external-gold-standard-ground-truth.csv"


  blackouak.goldstandard = "/Users/visenger/research/datasets/BlackOak/Archive/gold-standard"
  blackouak.gold.file = "/Users/visenger/research/datasets/BlackOak/Archive/gold-standard/gold-standard.txt"

  # this contains an additional column with labels: 1/0 -> error/non-error
  blackoak.goldstandard.ground.truth.folder = "/Users/visenger/research/datasets/BlackOak/Archive/gold-standard-ground-truth"
  blackoak.goldstandard.ground.truth.file = "/Users/visenger/research/datasets/BlackOak/Archive/gold-standard-ground-truth/gold-standard-ground-truth.csv"

  dboost.gaus.result.folder = "/Users/visenger/research/datasets/BlackOak/Archive/dboost-gaus-detect-result"
  dboost.gaus.result.file = "/Users/visenger/research/datasets/BlackOak/Archive/dboost-gaus-detect-result/dboost-gaus-detect-result.txt"
  dboost.result.folder = "/Users/visenger/research/datasets/BlackOak/Archive/dboost-detect-result"
  dboost.result.file = "/Users/visenger/research/datasets/BlackOak/Archive/dboost-detect-result/dboost-detect-result.txt"

  trifacta.result.folder = "/Users/visenger/research/datasets/BlackOak/Archive/trifacta-detect-result"
  trifacta.result.file = "/Users/visenger/research/datasets/BlackOak/Archive/trifacta-detect-result/trifacta-detect-result.csv"

  nadeef.detect.result.folder = "/Users/visenger/research/datasets/BlackOak/Archive/nadeef-detect-result"
  nadeef.detect.result.file = "/Users/visenger/research/datasets/BlackOak/Archive/nadeef-detect-result/nadeef-detect-result.txt"

  nadeef.deduplication.result.folder = "/Users/visenger/research/datasets/BlackOak/Archive/nadeef-deduplication-result"
  nadeef.deduplication.result.file = "/Users/visenger/research/datasets/BlackOak/Archive/nadeef-deduplication-result/nadeef-deduplication-result.csv"

  full.result.folder = "/Users/visenger/research/datasets/BlackOak/Archive/full-result"
  full.result.file = "/Users/visenger/research/datasets/BlackOak/Archive/full-result/full-result.csv"
}


model {

  full.result.folder = "/Users/visenger/research/datasets/BlackOak/Archive/full-result-model"
  full.result.file = "/Users/visenger/research/datasets/BlackOak/Archive/full-result-model/full-result-model.txt"

  matrix.folder = "/Users/visenger/research/datasets/BlackOak/Archive/matrix"
  matrix.file = "/Users/visenger/research/datasets/BlackOak/Archive/matrix/matrix.csv"
  libsvm.folder = "/Users/visenger/research/datasets/BlackOak/Archive/libsvm"
  prediction.folder = "/Users/visenger/research/datasets/BlackOak/Archive/prediction"
}

# MODEL DATA
# format: model.[data-set].[size] { same structure}
model.hosp.10k {
  libsvm.folder = "/Users/visenger/research/datasets/clean-and-dirty-data/HOSP-10K/libsvm"
  libsvm.file = "/Users/visenger/research/datasets/clean-and-dirty-data/HOSP-10K/libsvm/libsvm-hosp-10k.txt"
}

model.salaries {
  libsvm.folder = "/Users/visenger/research/datasets/clean-and-dirty-data/SALARIES/salaries_small/libsvm"
  libsvm.file = "/Users/visenger/research/datasets/clean-and-dirty-data/SALARIES/salaries_small/libsvm/libsvm-salaries.txt"
}

dc.rules {
  path = "/Users/visenger/research/datasets/BlackOak/Archive/"
  file = "fds-modified.txt"
  noisy = ""

}

# EXPERIMENTS
multiarmed.bandit {
  header = "dataset,banditalg,param,expectations"
  result.csv = "/Users/visenger/research/datasets/multiarmed-bandit/experiments.csv"
}


###########################################

spark.config {
  local.ip = "spark.local.ip"
  local.ip.value = "192.168.253.138"

  driver.memory = "spark.driver.memory"
  driver.memory.value = "10g"

  executor.memory = "spark.executor.memory"
  executor.memory.value = "8g"
}

db.postgresql {
  url = "jdbc:postgresql://localhost:5432/unittest"
  user = "tester"
  password = "tester"
  driver = "org.postgresql.Driver"
}

tools {
  available = 5 //8
}


test {
  # todo: just add files
  clean.data = ""
  dirty.data = ""
  ground.truth = ""
}

common.base {
  field = "first/value"
}

common.element {
  field = ${common.base.field}"/second/value"
}